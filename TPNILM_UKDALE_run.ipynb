{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivan\\PycharmProjects\\TPNILM\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saPKpECRVlln"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "from collections import defaultdict, OrderedDict\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, matthews_corrcoef\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPxd9q-PhAXB"
   },
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2t-SBZDmjvHJ",
    "outputId": "c5ec6875-b48c-47d3-dfc3-0b7beb6346cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2835M  100 2835M    0     0  35.5M      0  0:01:19  0:01:19 --:--:-- 53.9M0:00:02 53.7M\n"
     ]
    }
   ],
   "source": [
    "!curl https://data.ukedc.rl.ac.uk/browse/edc/efficiency/residential/EnergyConsumption/Domestic/UK-DALE-2015/UK-DALE-disaggregated/ukdale.h5.tgz -o ukdale.h5.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "jR0T2s5QNkqv",
    "outputId": "7d50ab1c-fd17-49ee-982b-5199a97f7143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ukdale.h5\r\n"
     ]
    }
   ],
   "source": [
    "!tar xvfz ukdale.h5.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srIUiAAXPRrk"
   },
   "outputs": [],
   "source": [
    "!rm ukdale.h5.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "Lf-TVlKgPEVs",
    "outputId": "9536941e-9cb6-4e6d-91ae-2e01c4dce53b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3267076\r\n",
      "-rw-r--r-- 1 root root     370852 Oct 16 08:04 TPNet_UKDALE_run.ipynb\r\n",
      "-rw-r--r-- 1 root root        850 Oct 16 07:52 house_1_labels.dat\r\n",
      "-rw-r--r-- 1 root root        228 Oct 16 07:52 house_2_labels.dat\r\n",
      "-rw-r--r-- 1 root root         60 Oct 16 07:52 house_3_labels.dat\r\n",
      "-rw-r--r-- 1 root root        111 Oct 16 07:52 house_4_labels.dat\r\n",
      "-rw-r--r-- 1 root root        376 Oct 16 07:52 house_5_labels.dat\r\n",
      "-rw-r--r-- 1 root root          0 Oct 16 07:50 onstart.log\r\n",
      "-rw-r--r-- 1 root root         75 Oct 16 07:50 onstart.sh\r\n",
      "-rw-rw-r-- 1 1000 1000 3345086829 Jan  5  2015 ukdale.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9g6B69jPJkz"
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('ukdale.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4y3vM6zQz5DC"
   },
   "outputs": [],
   "source": [
    "def resample_meter(store=None, building=1, meter=1, period='1min', cutoff=1000.):\n",
    "    key = '/building{}/elec/meter{}'.format(building,meter)\n",
    "    m = store[key]\n",
    "    v = m.values.flatten()\n",
    "    t = m.index\n",
    "    s = pd.Series(v, index=t).clip(0.,cutoff)\n",
    "    s[s<10.] = 0.\n",
    "    return s.resample('1s').ffill(limit=300).fillna(0.).resample(period).mean().tz_convert('UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_a-pqdClkRmO"
   },
   "outputs": [],
   "source": [
    "def get_series(datastore, house, label, cutoff):\n",
    "    filename = './house_%1d_labels.dat' %house\n",
    "    print(filename)\n",
    "    labels = pd.read_csv(filename, delimiter=' ', header=None, index_col=0).to_dict()[1]\n",
    "    \n",
    "    for i in labels:\n",
    "        if labels[i] == label:\n",
    "            print(i, labels[i])\n",
    "            s = resample_meter(store, house, i, '1min', cutoff)\n",
    "            #s = resample_meter(store, house, i, '6s', cutoff)\n",
    "    \n",
    "    s.index.name = 'datetime'\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "6lRVLSIzk6B5",
    "outputId": "bc980e76-1dca-4841-cddd-f10d6ee69d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_1_labels.dat\n",
      "1 aggregate\n",
      "./house_1_labels.dat\n",
      "10 kettle\n",
      "./house_1_labels.dat\n",
      "12 fridge\n",
      "./house_1_labels.dat\n",
      "5 washing_machine\n",
      "./house_1_labels.dat\n",
      "13 microwave\n",
      "./house_1_labels.dat\n",
      "6 dishwasher\n"
     ]
    }
   ],
   "source": [
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_1 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_1.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_1_train = ds_1[pd.datetime(2013,4,12):pd.datetime(2014,12,15)]\n",
    "ds_1_valid = ds_1[pd.datetime(2014,12,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "5_PW33ZLpwsK",
    "outputId": "776d64c0-fcfa-4637-f983-cf7207bd6ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_2_labels.dat\n",
      "1 aggregate\n",
      "./house_2_labels.dat\n",
      "8 kettle\n",
      "./house_2_labels.dat\n",
      "14 fridge\n",
      "./house_2_labels.dat\n",
      "12 washing_machine\n",
      "./house_2_labels.dat\n",
      "15 microwave\n",
      "./house_2_labels.dat\n",
      "13 dish_washer\n"
     ]
    }
   ],
   "source": [
    "house = 2\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dish_washer', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_2 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_2.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_2_train = ds_2[pd.datetime(2013,5,22):pd.datetime(2013,10,3,6,16)]\n",
    "ds_2_valid = ds_2[pd.datetime(2013,10,3,6,16):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WPRtFZqclByE",
    "outputId": "234de859-ec02-433f-8d75-86dbfe448a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_3_labels.dat\n",
      "1 aggregate\n",
      "./house_3_labels.dat\n",
      "2 kettle\n"
     ]
    }
   ],
   "source": [
    "house = 3\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = 0.*m\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_3 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_3.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_3_train = ds_3[pd.datetime(2013,2,27):pd.datetime(2013,4,1,6,15)]\n",
    "ds_3_valid = ds_3[pd.datetime(2013,4,1,6,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "q0xA9KgDqt1q",
    "outputId": "e5eb5263-7e1c-4fbf-c46c-3a2c3e55858a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_4_labels.dat\n",
      "1 aggregate\n",
      "./house_4_labels.dat\n",
      "3 kettle_radio\n",
      "./house_4_labels.dat\n",
      "5 freezer\n"
     ]
    }
   ],
   "source": [
    "house = 4\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle_radio', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_4 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_4.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_4_train = ds_4[pd.datetime(2013,3,9):pd.datetime(2013,9,24,6,15)]\n",
    "ds_4_valid = ds_4[pd.datetime(2013,9,24,6,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "-G2EBJcWrM5V",
    "outputId": "088066b8-e349-475e-fdf0-0bd14abcc57c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_5_labels.dat\n",
      "1 aggregate\n",
      "./house_5_labels.dat\n",
      "18 kettle\n",
      "./house_5_labels.dat\n",
      "19 fridge_freezer\n",
      "./house_5_labels.dat\n",
      "24 washer_dryer\n",
      "./house_5_labels.dat\n",
      "23 microwave\n",
      "./house_5_labels.dat\n",
      "22 dishwasher\n"
     ]
    }
   ],
   "source": [
    "house = 5\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge_freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washer_dryer', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_5 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_5.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_5_train = ds_5[pd.datetime(2014,6,29):pd.datetime(2014,9,1)]\n",
    "ds_5_valid = ds_5[pd.datetime(2014,9,1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69C1JT0kdJ_W"
   },
   "outputs": [],
   "source": [
    "ds_1_train.reset_index().to_feather('./UKDALE_1_train.feather')\n",
    "ds_2_train.reset_index().to_feather('./UKDALE_2_train.feather')\n",
    "ds_3_train.reset_index().to_feather('./UKDALE_3_train.feather')\n",
    "ds_4_train.reset_index().to_feather('./UKDALE_4_train.feather')\n",
    "ds_5_train.reset_index().to_feather('./UKDALE_5_train.feather')\n",
    "\n",
    "ds_1_valid.reset_index().to_feather('./UKDALE_1_valid.feather')\n",
    "ds_2_valid.reset_index().to_feather('./UKDALE_2_valid.feather')\n",
    "ds_3_valid.reset_index().to_feather('./UKDALE_3_valid.feather')\n",
    "ds_4_valid.reset_index().to_feather('./UKDALE_4_valid.feather')\n",
    "ds_5_valid.reset_index().to_feather('./UKDALE_5_valid.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvY0BjXDdKXR"
   },
   "source": [
    "# Read the feather dataframe resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5vzLJT4LwVl"
   },
   "outputs": [],
   "source": [
    "def get_status(app, threshold, min_off, min_on):\n",
    "    condition = app > threshold\n",
    "    # Find the indicies of changes in \"condition\"\n",
    "    d = np.diff(condition)\n",
    "    idx, = d.nonzero() \n",
    "\n",
    "    # We need to start things after the change in \"condition\". Therefore, \n",
    "    # we'll shift the index by 1 to the right.\n",
    "    idx += 1\n",
    "\n",
    "    if condition[0]:\n",
    "        # If the start of condition is True prepend a 0\n",
    "        idx = np.r_[0, idx]\n",
    "\n",
    "    if condition[-1]:\n",
    "        # If the end of condition is True, append the length of the array\n",
    "        idx = np.r_[idx, condition.size] # Edit\n",
    "\n",
    "    # Reshape the result into two columns\n",
    "    idx.shape = (-1,2)\n",
    "    on_events = idx[:,0].copy()\n",
    "    off_events = idx[:,1].copy()\n",
    "    assert len(on_events) == len(off_events)\n",
    "\n",
    "    if len(on_events) > 0:\n",
    "        off_duration = on_events[1:] - off_events[:-1]\n",
    "        off_duration = np.insert(off_duration, 0, 1000.)\n",
    "        on_events = on_events[off_duration > min_off]\n",
    "        off_events = off_events[np.roll(off_duration, -1) > min_off]\n",
    "        assert len(on_events) == len(off_events)\n",
    "\n",
    "        on_duration = off_events - on_events\n",
    "        on_events = on_events[on_duration > min_on]\n",
    "        off_events = off_events[on_duration > min_on]\n",
    "\n",
    "    s = app.copy()\n",
    "    #s.iloc[:] = 0.\n",
    "    s[:] = 0.\n",
    "\n",
    "    for on, off in zip(on_events, off_events):\n",
    "        #s.iloc[on:off] = 1.\n",
    "        s[on:off] = 1.\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iR9d91y1ktT"
   },
   "outputs": [],
   "source": [
    "class Power(data.Dataset):\n",
    "    def __init__(self, meter=None, appliance=None, status=None, \n",
    "                 length=256, border=680, max_power=1., train=False):\n",
    "        self.length = length\n",
    "        self.border = border\n",
    "        self.max_power = max_power\n",
    "        self.train = train\n",
    "\n",
    "        self.meter = meter.copy()/self.max_power\n",
    "        self.appliance = appliance.copy()/self.max_power\n",
    "        self.status = status.copy()\n",
    "\n",
    "        self.epochs = (len(self.meter) - 2*self.border) // self.length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        i = index * self.length + self.border\n",
    "        if self.train:\n",
    "            i = np.random.randint(self.border, len(self.meter) - self.length - self.border)\n",
    "\n",
    "        x = self.meter.iloc[i-self.border:i+self.length+self.border].values.astype('float32')\n",
    "        y = self.appliance.iloc[i:i+self.length].values.astype('float32')\n",
    "        s = self.status.iloc[i:i+self.length].values.astype('float32')\n",
    "        x -= x.mean()\n",
    "        \n",
    "        return x, y, s\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mirkSD9a1qTQ",
    "outputId": "df7353f4-93f8-4583-da76-3d999bb2d248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 480])\n",
      "327619\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #return self.bn(F.relu(self.conv(x)))\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        #self.upsample = nn.Upsample( scale_factor=kernel_size, mode='linear', align_corners=True)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        #return self.upsample(x)\n",
    "        #return self.drop(self.upsample(x))\n",
    "        return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='linear', align_corners=True))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "class PTPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(PTPNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1**k, features * 2**k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2**k, features * 4**k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4**k, features * 8**k, kernel_size=3, padding=0)\n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features*8**k, features*2**k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features*8**k, features*2**k, kernel_size=10)\n",
    "        self.tpool3 = TemporalPooling(features*8**k, features*2**k, kernel_size=20)\n",
    "        self.tpool4 = TemporalPooling(features*8**k, features*2**k, kernel_size=30)\n",
    "\n",
    "        self.decoder = Decoder(2*features * 8**k, features * 1**k, kernel_size=p**3, stride=p**3)\n",
    "\n",
    "        self.activation = nn.Conv1d(features * 1**k, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        tp1 = self.tpool1(enc4)\n",
    "        tp2 = self.tpool2(enc4)\n",
    "        tp3 = self.tpool3(enc4)\n",
    "        tp4 = self.tpool4(enc4)\n",
    "\n",
    "        dec = self.decoder(torch.cat([enc4, tp1, tp2, tp3, tp4], dim=1))\n",
    "\n",
    "        act = self.activation(dec)\n",
    "        return act\n",
    "\n",
    "x = torch.randn(32,1,60*8+2*16)\n",
    "model = PTPNet(1,3,32)\n",
    "print(model(x).shape)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMsGSOln1wS3"
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, n_epochs, filename):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the test loss as the model trains\n",
    "    test_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    # to track the average test loss per epoch as the model trains\n",
    "    avg_test_losses = [] \n",
    "    \n",
    "    min_loss = np.inf\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    #patience = 10\n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target_power, target_status) in enumerate(train_loader, 1):\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target_power, target_status in valid_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        ##################    \n",
    "        # test the model #\n",
    "        ##################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target_power, target_status in test_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # record validation loss\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        test_loss = np.average(test_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'test_loss: {test_loss:.5f} ')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        #early_stopping(valid_loss, model)\n",
    "        #if (early_stopping.early_stop and (epoch > 80)):\n",
    "        #    break\n",
    "        \n",
    "        if valid_loss < min_loss:\n",
    "            print(f'Validation loss decreased ({min_loss:.6f} --> {valid_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            min_loss = valid_loss\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    return  model, avg_train_losses, avg_valid_losses, avg_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pP9W8d_K12mm"
   },
   "outputs": [],
   "source": [
    "def evaluate_activation(model, loader, a):\n",
    "    x_true = []\n",
    "    s_true = []\n",
    "    p_true = []\n",
    "    s_hat = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, p, s in loader:\n",
    "            x = x.unsqueeze(1).cuda()\n",
    "            p = p.permute(0,2,1)[:,a,:]\n",
    "            s = s.permute(0,2,1)[:,a,:]\n",
    "            \n",
    "            sh = model(x)\n",
    "            sh = torch.sigmoid(sh[:,a,:])\n",
    "            \n",
    "            s_hat.append(sh.contiguous().view(-1).detach().cpu().numpy())\n",
    "            \n",
    "            x_true.append(x[:,:,BORDER:-BORDER].contiguous().view(-1).detach().cpu().numpy())\n",
    "            s_true.append(s.contiguous().view(-1).detach().cpu().numpy())\n",
    "            p_true.append(p.contiguous().view(-1).detach().cpu().numpy())\n",
    "    x_true = np.hstack(x_true)\n",
    "    s_true = np.hstack(s_true)\n",
    "    p_true = np.hstack(p_true)\n",
    "    s_hat = np.hstack(s_hat)\n",
    "\n",
    "    return x_true, p_true, s_true, s_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeLT8jHC1-mA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSypXakc14qb"
   },
   "outputs": [],
   "source": [
    "APPLIANCE = ['fridge', 'dish_washer', 'washing_machine']\n",
    "THRESHOLD = [50., 10., 20.]\n",
    "MIN_ON = [1., 30., 30.]\n",
    "MIN_OFF = [1., 30., 3.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 16\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mO2wSprQfV6x"
   },
   "outputs": [],
   "source": [
    "ds_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "for i in range(5):\n",
    "    ds = pd.read_feather('./UKDALE_%d_train.feather' %(i+1))\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    ds_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(ds_meter[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "sGfWAsgWbVtt",
    "outputId": "2c141854-5a34-469f-e034-c5f535f83c52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fridge             3525\n",
       "dish_washer          98\n",
       "washing_machine      54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ds_status[1].diff()==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "Tq5YJh7lzqvS",
    "outputId": "494fd4da-e7ad-485e-9da1-4ffee3d198a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fridge</th>\n",
       "      <th>dish_washer</th>\n",
       "      <th>washing_machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>193337.00000</td>\n",
       "      <td>193337.000000</td>\n",
       "      <td>193337.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.37792</td>\n",
       "      <td>0.028918</td>\n",
       "      <td>0.011317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.48503</td>\n",
       "      <td>0.167543</td>\n",
       "      <td>0.105773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fridge    dish_washer  washing_machine\n",
       "count  193337.00000  193337.000000    193337.000000\n",
       "mean        0.37792       0.028918         0.011317\n",
       "std         0.48503       0.167543         0.105773\n",
       "min         0.00000       0.000000         0.000000\n",
       "25%         0.00000       0.000000         0.000000\n",
       "50%         0.00000       0.000000         0.000000\n",
       "75%         1.00000       0.000000         0.000000\n",
       "max         1.00000       1.000000         1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_status[1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLZ7PmzG7jHS"
   },
   "outputs": [],
   "source": [
    "ds_house_train = [Power(ds_meter[i][:int(0.8*ds_len[i])], \n",
    "                        ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                        ds_status[i][:int(0.8*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, True) for i in range(5+0)]\n",
    "\n",
    "ds_house_valid = [Power(ds_meter[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        ds_appliance[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])],\n",
    "                        ds_status[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_test  = [Power(ds_meter[i][int(0.9*ds_len[i]):], \n",
    "                        ds_appliance[i][int(0.9*ds_len[i]):],\n",
    "                        ds_status[i][int(0.9*ds_len[i]):], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_total  = [Power(ds_meter[i], ds_appliance[i], ds_status[i], \n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_train_seen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                ds_house_train[1], \n",
    "                                                #ds_house_train[2], \n",
    "                                                #ds_house_train[3],\n",
    "                                                ds_house_train[4]\n",
    "                                                ])\n",
    "ds_valid_seen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                #ds_house_valid[1], \n",
    "                                                #ds_house_valid[2], \n",
    "                                                #ds_house_valid[3], \n",
    "                                                #ds_house_valid[4]\n",
    "                                                ])\n",
    "\n",
    "dl_train_seen = DataLoader(dataset = ds_train_seen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_seen = DataLoader(dataset = ds_valid_seen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_seen = DataLoader(dataset = ds_house_test[0], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "ds_train_unseen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                  #ds_house_train[1], \n",
    "                                                  #ds_house_train[2], \n",
    "                                                  #ds_house_train[3], \n",
    "                                                  ds_house_train[4]\n",
    "                                                  ])\n",
    "ds_valid_unseen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                  #ds_house_valid[1], \n",
    "                                                  #ds_house_valid[2], \n",
    "                                                  #ds_house_valid[3], \n",
    "                                                  ds_house_valid[4]\n",
    "                                                  ])\n",
    "dl_train_unseen = DataLoader(dataset = ds_train_unseen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_unseen = DataLoader(dataset = ds_valid_unseen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_unseen = DataLoader(dataset = ds_house_total[1], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_test[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_valid[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QC-IwqzoL2D5"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(dl_house_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "rB6NlwrYYE3D",
    "outputId": "1a9d8d38-b2b9-45cd-f1fd-0b0604f464a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 1.5)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAHWCAYAAADKJUDHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdeXzddZ3v8ffnLOmSpG3SfWUppUBZ\nx4ICKnChwCBYUWRgOlccVGQUFR3vgDiKIjvijDqCVAdwHIURRUBkUbbClbV4oVBaKBRKW6BNaVq6\npck5v+/94yw5SdPmpL9PzjltX8/HI7Y5W34nMy159/P5fj4WQhAAAAAAYOeQqPYFAAAAAAD8EPIA\nAAAAYCdCyAMAAACAnQghDwAAAAB2IoQ8AAAAANiJEPIAAAAAYCfiEvLM7EYzW2lmL27l/qPNbK2Z\nPZf/+HbJfSea2ctm9qqZXehxPQAAAACwqzKPPXlm9mFJ6yX9Vwhh/x7uP1rS10MIJ3e7PSnpFUkz\nJC2T9IykM0MIL8W+KAAAAADYBblU8kIIj0pavR1PPUzSqyGExSGEdkm3SprpcU0AAAAAsCuq5Jm8\nw83seTO718ym5W8bL2lpyWOW5W8DAAAAAGyHVIW+zl8l7RZCWG9mJ0m6Q9KUvryAmZ0j6RxJqq+v\nf98+++zjf5UAAAAAsAN49tlnV4UQRvZ0X0VCXgjhvZLf32Nm15nZCEnLJU0seeiE/G09vcZsSbMl\nafr06WHu3Ln9eMUAAAAAULvMbMnW7qtIu6aZjTEzy//+sPzXfVe5QStTzGwPM6uTdIakuypxTQAA\nAACwM3Kp5JnZLZKOljTCzJZJulhSWpJCCD+VdJqkfzKzjKRNks4IubGeGTM7T9L9kpKSbgwhzPe4\nJgAAAADYFbmsUKg02jUBAAAA7MrM7NkQwvSe7qvkdE0AAAAAQD8j5AEAAADAToSQBwAAAAA7EUIe\nAAAAAOxECHkAAAAAsBMh5AEAAADAToSQBwAAAAA7EUIeAAAAAOxECHmAk3fffVdXXHGFNmzYUO1L\nAQAAwC6MkAc4eeihh3TRRRdp1qxZymaz1b4cAAAA7KIIeYCTQrC78847dfXVV1f5agAAALCrIuQB\nTqIokiSZmZ577rkqXw0AAAB2VYQ8wEkh5KVSKYUQqnw1AAAA2FUR8gAnhZCXTCYJeQAAAKgaQh7g\nhJAHAACAWkDIA5wQ8gAAAFALCHmAk9IzeYXfAwAAAJVGyAOcUMkDAABALSDkAU4IeQAAAKgFhDzA\nCSEPAAAAtYCQBzgh5AEAAKAWEPIAJwxeAQAAQC0g5AFOqOQBAACgFhDyACeFYEfIAwAAQDUR8gAn\nVPIAAABQCwh5gJNCyEskEpzJAwAAQNUQ8gAnpYNXqOQBAACgWgh5gBPaNQEAAFALCHmAE0IeAAAA\nagEhD3BCyAMAAEAtIOQBTkpDHoNXAAAAUC2EPMAJlTwAAADUAkIe4CSKIiUSCZkZIQ8AAABVQ8gD\nnBDyAAAAUAsIeYATQh4AAABqASEPcFIa8hi8AgAAgGoh5AFOCiEvkUhQyQMAAEDVEPIAJ7RrAgAA\noBYQ8gAnhDwAAADUAkIe4ISQBwAAgFpAyAOcMHgFAAAAtYCQBzhh8AoAAABqASEPcEK7JgAAAGoB\nIQ9wEkIg5AEAAKDqCHmAkyiKZGaEPAAAAFQVIQ9wwuAVAAAA1AKXkGdmN5rZSjN7cSv3zzKzeWb2\ngpk9bmYHldz3Rv7258xsrsf1ANXA4BUAAADUAq9K3s2STtzG/a9LOiqEcICk70ma3e3+Y0IIB4cQ\npjtdD1BxDF4BAABALUh5vEgI4VEz230b9z9e8umTkiZ4fF2glhDyAAAAUAuqcSbvM5LuLfk8SPqT\nmT1rZuds7Ulmdo6ZzTWzuS0tLf1+kUBfEfIAAABQC1wqeeUys2OUC3kfLLn5gyGE5WY2StKfzWxh\nCOHR7s8NIcxWvs1z+vTp/ASNmsPgFQAAANSCilXyzOxAST+XNDOE8G7h9hDC8vyvKyX9XtJhlbom\nwBODVwAAAFALKhLyzGySpNsl/e8Qwislt9ebWWPh95KOl9TjhE6g1tGuCQAAgFrg0q5pZrdIOlrS\nCDNbJuliSWlJCiH8VNK3JQ2XdJ2ZSVImP0lztKTf529LSfp1COE+j2sCKo2QBwAAgFrgNV3zzF7u\n/6ykz/Zw+2JJB235DGDHQ8gDAABALajGdE1gp1R6Jo/BKwAAAKgWQh7ghEoeAAAAagEhD3BCyAMA\nAEAtIOQBTgh5AAAAqAWEPMAJIQ8AAAC1gJAHOGHwCgAAAGoBIQ9wQiUPAAAAtYCQBzgh5AEAAKAW\nEPIAJyEEmRkhDwAAAFVFyAOclFbyOJMHAACAaiHkAU5KB69QyQMAAEC1EPIAJ5zJAwAAQC0g5AFO\nCHkAAACoBYQ8wAkhDwAAALWAkAc4YfAKAAAAagEhD3DC4BUAAADUAkIe4IR2TQAAANQCQh7ghJAH\nAACAWkDIA5wQ8gAAAFALCHmAEwavAAAAoBYQ8gAnDF4BAABALSDkAU5o1wQAAEAtIOQBTgh5AAAA\nqAWEPMAJIQ8AAAC1gJAHOGHwCgAAAGoBIQ9wwuAVAAAA1AJCHuCEdk0AAADUAkIe4ISQBwAAgFpA\nyAOchBBkZoQ8AAAAVBUhD3BSWsmTRNADAABAVRDyACelg1ckQh4AAACqg5AHOKGSBwAAgFpAyAOc\nEPIAAABQCwh5gBNCHgAAAGoBIQ9w0j3kRVFU5SsCAADAroiQBzhh8AoAAABqASEPcEK7JgAAAGoB\nIQ9wQsgDAABALSDkAU4IeQAAAKgFhDzASX8PXlmyZImeeOIJ19cEAADAzidV7QsAdhaeg1dCCPr4\nxz+uBQsWqLGxUWPHjtV9992nKIrU0tKipqYmr8sGAADAToZKHuAghKAQglu7ZktLi+644w41Njaq\nublZCxcu1LHHHqtsNqs5c+Z4XTYAAAB2QlTyAAeFQNeXkNfWkdXba9u0x4j6Le576aWXJEmXX365\nZsyYIUlqb29XU1OTHnroIX3sYx/zvHwAAADsRKjkAQ4K5+/KCXlRFPTYohad9KPH9L+ufUTPLV3T\n5f5sFPTEvJeVGDREGxsn6r4X35Ek1dXV6YMfOkoPPfRQP74TAAAA7Oio5AEOSkNe4UxeT4NXXnrr\nPX3uv+Zq+ZpNmtA0SMPr6/Sdu+br9n86QomE6Zk3Vuvbd87XgrcnaOKXf62v3PGaJOlrM/bWS2+9\np1cO/oI2j12sO59+RTMP21uS9OyS1br58SXa1J7Vm6s3KJMN+vDeI/WPR+6u3YZvWSX0e89BZiqG\nWgAAANQGl0qemd1oZivN7MWt3G9m9iMze9XM5pnZ35Tcd5aZLcp/nOVxPUCllVvJe/bNVi1fs0mX\nnbq//vzVo3TBifvouaVr9MMHF+k3zyzVGbOf1HubOjR8ycMa8uqf9J9nTdcpB43TD/78iv700js6\ndo/BslSdvv77V/TsklZJ0vfuXqCHFqzQstaNmtRcr92GD9avn35TM/7tUf3yySX98n47spHO/NmT\n+uRPn9B7bR2SpDUb23XPC2/r9VUbWB8BAABQRV6VvJsl/Yek/9rK/X8raUr+4/2Srpf0fjNrlnSx\npOmSgqRnzeyuEEKr03UBFVFuyMtmc487af+xGlSX1Cf+ZoL+9NIK/fDBRZKkIyYP1+xPTddeu/29\nPvKRj+jYfUfrQ1NGakLTIB2+53AdOblZe//oa2o/+is6++andfVpB+m5pWv04foVuu6zp6uhoUGS\ntOK9Nn3+l8/qhw8s0v/+wG7u7/fHD72qp15frWTCdMYNT2ryqAY9uGCFNrZnJUnN9XWa2DRIK9dt\n1uGTh+vyUw/QwHTS/ToAAACwJZeQF0J41Mx238ZDZkr6r5D7qfdJMxtmZmMlHS3pzyGE1ZJkZn+W\ndKKkWzyuC6iUskJeR5vqNr6jMXpXqQ1vS5mUEpJmzxyjJw+s0/zla/UPHxinTe8sVGrjSh22zwRp\n7XLVSbrg8EZJ7dL6d/T7G67UJz51jupPu1jf/e8VGpNt18M//IqO+eW/6le//rX2njJFoyV9aNRm\n3dfSIq1d7vpel7Zu1O8efkr/eMBoHbX3KF37p5e1fJPpk1OG6pSDJunN1Ru18O11emftGu0zJqk5\nf52nf353qX7wdwdpQLKHoGcmNY7N/QoAAIDYzKutKh/y7g4h7N/DfXdLujKE8H/znz8o6QLlQt7A\nEMKl+du/JWlTCOH72/pa06dPD3PnznW5bsDDunXrNGTIEH3/+9/X4MGD9YUvfEFvv/22xowZ0/mg\n2UdLb/2/ql1jTfvwv0j/65vVvgoAAIAdhpk9G0KY3tN9O8zgFTM7R9I5kjRp0qQqXw3QVU+DV7b4\nB5R17+itIQfrh+8eqss+tr9SiZ4rV3MenaNf/vK/ddVVV2p48/Ctfs0777pL9/3lr9pn0ih95Ytf\n0EXf/KYmTpyofzr3XEnSPS++rb+8+q4u+9gW/+4Sy8J31ummx99QetGDGprs0P/5+v/Z5uODpJ/O\neU2rN7TrX06YqnSy21HgP31LWv+O6zUCAADsyioV8pZLmljy+YT8bcuVq+aV3v5ITy8QQpgtabaU\nq+T1x0UC26usds0Q6d3Bu+t/ssfoivedJG0l5P3mxqf1m1cHavax50uJrc9G+ujffEr2hz/oyCOP\nlIYP17zUbbr7qTf1Tz/LzS+av2qhbn15sS5730kO77DT0pdW6H8em6u3H71D7Ste06xrZ2jcuHFb\nfbxJOmjYKv39z57SY483asjAtG4++1ANrsv/9fPIlVLYchIpAAAAtk+l9uTdJelT+SmbH5C0NoTw\ntqT7JR1vZk1m1iTp+PxtwA6l3JAXyWQmJbYS8CTp+eef14EHHlisCG6NmemjH/2ohg/PVfumTp2q\nRYsWFa8llUgoGwX3SZeZ/PCYYUOHSJLuuOOOXp9zxOQR+rvpE7U5E+npN1ZrccuGkjeSkJjGCQAA\n4MZrhcItkp6QNNXMlpnZZ8zsXDM7N/+QeyQtlvSqpJ9J+oIk5QeufE/SM/mPSwpDWIAdSXkhLyir\nhNLbCG9RFGnevHk66KCD+nwNU6dOVVtbm958801JKraDZiLfAPXKq7ndfV/58pe0zz776He/+50k\nacmSJfra176mFStW9Pi8q047UNeenntfLes3d95ByAMAAHDlNV3zzF7uD5K+uJX7bpR0o8d1ANXS\nU8jbYhl6iBQFKbmNKt6SJUu0bt267Q55kvTKK69o9913Vyp/9i0bBXluL3hv3frc15uylz7xiU/o\nyiuv1KOPPqovfelLmjdvnu677z49+OCDGjt2rCRp1apVWrFihaZNm6aRDQMkSS3rSkOe0a4JAADg\nqFLtmsBOrazBKyFSFGyrA1ekXKumpFgh7+WXX5bUWcnryPoGqPZMbhfegLq0/vEf/1FNTU066qij\n9OKLL+p73/uelixZosmTJ+uUU07R3nvvrZEjR2r//ffXww8/rJGNhDwAAID+RsgDHJTfrmlKJbcd\n8sxM++/f94mYo0eP1pAhQzpDXv7rZLK+rZAdhZCXTmny5MlatGiRLr74Yt10003613/9Vz399NP6\nh3/4By1YsED77ruvrrjiCo0fP17f/OY3NSCVUOPAVLeQlyDkAQAAONphVigAtayskKegbDAl85W+\ntWvXatmyZdpvv/2Kz3n++ec1ZcoU1dfX9/kazExTp04tCXm5r+N9Ji9XGUyqLp3762PYsGH6zne+\nU7x/2rRpmj17dpfnNDU16dxzz9WsWbOUGXuKVrxXshrCEsotWgAAAIAHKnmAg3Kna2YjaXPbRh19\n9NEaPny49t9/f+211176yU9+ouXLl+upp57arlbNgqlTp2rBggWSSgev+FbJCpW8Qsgrx9lnn619\n991Xt9xyi1rfXqKlLWs776SSBwAA4IqQBzgod/BK69q1al3VonfeeUcXXnihfvrTn2rcuHE677zz\nNHHiRLW0tGjWrFnbfR2HHnqoli9frjfeeKMz5Dm3axZWKAyqS5f9nHQ6reeee06/+93vlN3Qqnc3\ndJTcy5k8AAAAT7RrAg7KHbzSkZVCFGnOnDkaPXq0JOmcc87Rbbfdpr/85S8677zzNGXKlO2+juOO\nO06S9OCDD6r5b06Q1F/tmlJdXd/++qirq1Nzc7OyG1rVuinbeQeVPAAAAFeEPMBBXwavKGSVTHbu\nNDAznX766Tr99NNjX8e+++6rsWPH6oEHHtCUMEJSqlh581J4vQHp8it5BQ0NDcqub1VbVtrYntHg\nuhR78gAAAJzRrgk4KPdMXhRMIZvpEvI8mZmOPfZY/elPf9L11/2HpP6r5A0cUNfn5zY0NCi7oVWS\ntGpde+5GQh4AAIArQh7goOzBK5JClO23kCdJxx57rFavXq0N696T1A9n8vKhcUAfBq8UNDY2FkNe\ny/q23I0m2jUBAAAcEfIAB+UOXomCSVHUryFvxowZSqVSxRDmPV0zG0UK2YzS29uuWQh5hV15nMkD\nAABwRcgDHJQ1eEW5M3kh6r92TUkaP368Fi5cqFM+8hFJ/dGuGRSirFKpvlfy6uvrew557MkDAABw\nQ8gDHPTarpn/fTYyqZ/bNSVp8uTJxT12Hc6DV7JRkKLsdlXyUqmUBoQOKQQqeQAAAP2EkAc46D3k\n5e6PZAr93K5ZUJfKfY2OjH/I295KniQ11A9WXWhXy/p8yGNPHgAAgCtCHuCgEPLMrJeQl5CiTLGl\nsz+lU7mvsbmjo5dH9k0mX8nb3vfQ0NAgizJqz+S/P1TyAAAAXBHyAAeFQLfVwSv5+zOhclWrQiVv\nc3vG9XWzUYj1HhobGxWibGcIZoUCAACAK0Ie4KDXwSsl7ZpWoZCXLoY830pe4Uze9mpoaFCIIkWB\nSh4AAEB/IOQBDvpyJq/ilbwO50peUKz30NDQoCjKqjj004xKHgAAgCNCHuCg3JCXDYmKVfIK0zXb\nM/7tmnHeQ2NjYz7kUckDAADoD4Q8wEGvIS+/By6SySq0E66wDN39TJ5HJS+b7SzemYk9eQAAAH4I\neYCDnkJe18Er+UqeKlfJG1Cs5G3/+bmeREGx3kMh5FHJAwAA6B+EPMBBuYNXgkymyg5eae/wDXnZ\noFjvobGxUVE20xny2JMHAADgipAHOOj9TF7u97lKXmVaEwfW9c+ZvFwlb/vfQ2G6ZiabD3ZU8gAA\nAFwR8gAH5Ya8SImKVfIGpNOS/Ct5cc8VNjQ0KIRIHYWpn+zJAwAAcEXIAxyUPV1TiYoNXilO18z2\nw5m8mO2aCkEdmdKQRyUPAADACyEPcFDu4JVKTtcstGt2eA9ekcVu11QIJZU8zuQBAAB4IuQBDsod\nvBIpoUSlVijU5ds1+yHkJSxmyFPUeVaQdk0AAABXhDzAQbntmpElK/aHLp1OK2Qz6sj4VslyE0K3\nX2Njo0L3dk325AEAALgh5AEO+rIMvVKVvHQ6rRBl1ZH1D3mJGGfycu2akTKFCiPtmgAAAK4IeYCD\ncit5IWarY1+k02kpynauKnCSa9fc/ucXzuRligNhCHkAAACeCHmAg3IHr4QKDl4pVvL6oV0zzl8c\nhemanZU8pmsCAAB4IuQBDsoevBLiVcH6IpVKSVFGHZFzyLOEQyWv+zJ0zuQBAAB4IeQBDvqyDD1Z\n4Uqed7tmkCkZo+V08ODBCqXtmlTyAAAAXBHyAAflhrwgVayS13kmzzlUWkIJ2/43kUgklEyYssVK\nHmfyAAAAPBHyAAeFkGdmvZzJi9fq2BepVEohm1Um8g15wRJKxnwPqURCmai0XZOQBwAA4IWQBzgo\nVO16X4ZeuTN56XRaCtnOMOUl5pk8SUokrPP7w548AAAAV4Q8wEFf9uTFrYKVq7AMvT8qeanYSTV0\nzlph8AoAAIArQh7goC978ioV8lKplEKUlfPcFSmRVDLm3xym3Pei+BntmgAAAG4IeYCDckNexds1\no6yyzpU8WULJGINXckJngyaDVwAAAFwR8gAHfVmGXtF2zSgjz+GaIQRZIqlkzKSaq+QVPqFdEwAA\nwBMhD3DQ+zL0kj15FSrl5ZahR66VvML5vrhn8kxBKrRrMl0TAADAFSEPcFD+mTxVrJJnZrl2Tcci\nWSEw+lbyaNcEAADwRMgDHJS7DL2SlTwpVzHzPJJXrOQ5JNWu7ZqEPAAAAC+EPMBBn6ZrVjLkhUjZ\n4Pf1OjJZST7tmqG0XZM9eQAAAG4IeYCDXgevlOzJS1Uu48kUybNG1l4IeTF3KHT5FlDJAwAAcEXI\nAxz0PnilSpU8BUWOlbzN7RlJPiGv6548KnkAAABeXEKemZ1oZi+b2atmdmEP9/+bmT2X/3jFzNaU\n3Jctue8uj+sBKq0ve/Litjr2RULBtZK3ub1dkpRKxAx5FjiTBwAA0E9ScV/AzJKSfiJphqRlkp4x\ns7tCCC8VHhNC+GrJ478k6ZCSl9gUQjg47nUA1VR+yEvEroL1RcK5ktfW3iFJSqfiVvJMnSsUqOQB\nAAB48vhp8zBJr4YQFocQ2iXdKmnmNh5/pqRbHL4uUDPKna4ZZBUNeaagSI7tmh25ds107HbNboNX\nqOQBAAC48fhpc7ykpSWfL8vftgUz203SHpIeKrl5oJnNNbMnzexjDtcDVFyvg1cKlbxQ2XbNpIKC\n+X29dq+QV3pJhDwAAABXsds1++gMSb8NIWRLbtsthLDczPaU9JCZvRBCeK37E83sHEnnSNKkSZMq\nc7VAmfoyeKWiZ/JMyjrOV9qcb9d0Gbxipe2ahDwAAAAvHj/9LZc0seTzCfnbenKGurVqhhCW539d\nLOkRdT2vV/q42SGE6SGE6SNHjox7zYCrPg1eqeSZPCtpi3RQaNesS8X796Hct4g9eQAAAP3B46fN\nZyRNMbM9zKxOuSC3xZRMM9tHUpOkJ0puazKzAfnfj5B0pKSXuj8XqHW9hjxV50xe0kzBHCt5hXbN\n2INXOv+Xdk0AAABfsds1QwgZMztP0v2SkpJuDCHMN7NLJM0NIRQC3xmSbg1df/LdV9INZhYpFziv\nLJ3KCewoyh28kpuuWcEzeRZcQ157e+FMXjLW63Sp3RHyAAAAXLmcyQsh3CPpnm63fbvb59/p4XmP\nSzrA4xqAaiqEPDPb5uCVICkdc8dcXyTMJEsoioISDmcB2zP5ds10vJBnppLpK/lfQ+g2kQUAAADb\no3I/bQI7sRBCMdxta/BKpffkFb5UJvI587a5IzczKZ2KGfJK/leFSiO78gAAAFwQ8gAHURQVw922\n2zUt9vqBvkjlc1TWKeR1FCp5cUOelU7XLIQ8WjYBAAA8EPIAB72HvMKePCkVMyD1RTLfotkR+QSo\n9nwlz3e6ZqFdk5AHAADggZAHOCg35IUQKRlzaElfFEJeJutTyWvP5ENeOl7IS0id4Y6QBwAA4IqQ\nBzjoKeT1NHglmw0VDXmFxesZp0peRzHkxW3XNG1xJo9deQAAAC4IeYCD0pDX4+CVwp68ClfyCkNe\n/Cp5uTN5A+JW8kqna3ImDwAAwBUhD3BQbrtmNqp0JU/Fr+uhI5N7H3FDXq6Ql784Qh4AAIArQh7g\noNzpmiGqTiWvI+vUrpn1OpNX+j3iTB4AAIAnQh7goOxKXqhsJa+wrsGtkpcPi3FDXqFTMwpiTx4A\nAIAzQh7goPfBK1Wq5CUKlTyvds1cJW9AXTrW6yRK9/fRrgkAAOCKkAc46HXwSpWmaxYqeV7TNTP5\nSt7AmCGvGIRDoJIHAADgjJAHOKjVPXnpVCHk+bZrukzXVD7XsScPAADAFSEPcFD+dM1IqVS8gNQX\nxcErGa/BK/mQ51rJy4c89uQBAAC4IOQBDno9k5cPMFG2spW8ulTuaxX228VV2Lc3cEBdrNdJFAev\ncCYPAADAGyEPcFBuJS+q8J68dD7kbW7vcHm9jFe7Zv7XrtM1CXkAAAAeCHmAg3IHr0QVPpNXVwx5\nPpW8bBQUoqzSaZ92TfbkAQAA+CPkAQ7KruRls9Wp5HX4hLyObCRF2djnChPsyQMAAOg3hDzAQe8h\nL38mr8LtmgPyIa/NqV0zGwWFbCZ2JS/R4woFKnkAAAAeCHmAgyiKiuFuW8vQK96umT871+5UyctE\nwWUNRCJByAMAAOgvhDzAQa22a3ZO18y6vF42ClKULb7X7ZUofo/EnjwAAABnhDzAQQihvMErUXUq\neV5n8jIhuISxHlcoAAAAwAU/XQEOeq3k5ffkZSt9Js+5XTOKJEXxq4Kd7ZqiXRMAAMAZIQ9wUKvt\nmul07mt1ZH0CVNatklc4txho1wQAAHBGyAMc9BTyug5eifK/VLpdMzcFsyPjtSdPMseQl8vBhDwA\nAABPhDzAQa0uQy+0a3Z4DV4J6r8zeezJAwAAcEHIAxyUuycvm63smby6ulwlL+PUrhkFp0oeKxQA\nAAD6DSEPcFD2mbxK78lL5Sp5GbdKXnBt12TwCgAAgD9CHuCg3EpeqPR0zbrCmTyfkBcFkyl+W2Wi\n9Nwig1cAAABcEfIAB6Uhr/S2onyAyVZ48Eoh5GUix3ZNj5CXb9fMRFHJnjzO5AEAAHgg5AEOuoe8\nRCLR4568KFS2kpd2bteMZK7tmplMlnZNAAAAZ4Q8wEH3kGdmPZ7JU5XaNV0Hr1j81ylU8rLZiOma\nAAAAzgh5gINyQl4UTKHSg1fq6hSirFvIC/L5S6NQyctGkdiTBwAA4IuQBzgoK+TJpAqfyUulUlKI\n/M7kyWfwSjKZ+15lsqWDV6jkAQAAeCDkAQ56CnldB6+EXMgLUS54VUg6nVaIIr9KnlO7ZrK0kseZ\nPAAAAFeEPMBBr4NXQqSgRMXbNQuVvKxTJS/IfNo1ezyTR8gDAADwQMgDHPTWrhmiKNfkWOGQl0wm\n8yHPpxUyyGmFQpdKHmfyAAAAPBHyAAe9hbwoRIqUqPiZvGQyqRBFirxCnpkSHu2aPVXy2JMHAADg\ngpAHOOi1kpcfvFLpds1iJc8tP5kcMl7xe5XhTB4AAIA7Qh7gIIoiWclEku6DV0JUnemayWRS8qzk\nKeHbrsmZPAAAAHeEPMBBb4NXQn4fXDUqeSFEyjqtJ3Bv12RPHgAAgDtCHuAghFBWu2Z1Bq9k5bRB\nQZJPyEskelqhEP91AQAAQJlTcMEAACAASURBVMgDXGSz2V6ma2arF/KiSJFjJc/jTF4y/72iXRMA\nAMAfIQ9wkM1mu4S3LSt5QZESCtWarulVJbOEbyUvywoFAAAAb4Q8wEFPIa908EpuGXru19KKX39L\nJBJSyLpV8rzaNYuVPPbkAQAAuCPkAQ66h7yeBq8EJZSo8MEzM5NCcKzk9cPgFfbkAQAAuCLkAQ56\nb9fMDV4xjwNtfRX82jWDJYrrD+LorOQFzuQBAAA4cwl5Znaimb1sZq+a2YU93P9pM2sxs+fyH58t\nue8sM1uU/zjL43qASivvTJ5V519VQvBr13Q6k1cIeRkGrwAAALhLxX0BM0tK+omkGZKWSXrGzO4K\nIbzU7aH/E0I4r9tzmyVdLGm6cr1az+af2xr3uoBKymQySqU6/zj1NF0zyLosTK8cz8Er5lLJKwxe\nidiTBwAA4M6jsHCYpFdDCItDCO2SbpU0s8znniDpzyGE1flg92dJJzpcE1BR5Q1e8TnP1lfmeibP\nt5LXtV2TM3kAAAAePELeeElLSz5flr+tu0+Y2Twz+62ZTezjc2Vm55jZXDOb29LS4nDZgJ9eB6+E\nSFGoTshTCPLbhe4V8npahk4lDwAAwEOljgj9QdLuIYQDlavW/aKvLxBCmB1CmB5CmD5y5Ej3CwTi\nKHdPXlUqeYr8imRmxVbLOJLJ3F89UZeQRyUPAADAg0fIWy5pYsnnE/K3FYUQ3g0hbM5/+nNJ7yv3\nucCOoNeQF+X25FUn5Pm0a4YQZJZQ0n26JmfyAAAAPHmEvGckTTGzPcysTtIZku4qfYCZjS359KOS\nFuR/f7+k482sycyaJB2fvw3YoZRzJq9alTzlJ3vGlc0nRY/3kOhpGTp78gAAAFzEnq4ZQsiY2XnK\nhbOkpBtDCPPN7BJJc0MId0n6spl9VFJG0mpJn84/d7WZfU+5oChJl4QQVse9JqDSytmTlxu8UvmU\nZwounZDZ/IskHVJeKmGSgiL25AEAALiLHfIkKYRwj6R7ut327ZLff0PSN7by3Bsl3ehxHUC1ZLPZ\nLisUug9eKVTTPFod+8oUXGpkhcKkR8jLncnLsgwdAACgH1RlNzOws8lkMtus5BVXKFShX9Mkl3bN\nTD7leVQjC2fyosCePAAAAG+EPCCmwtm73gavRNXak+dUyctkspKcKnn5kJfJMl0TAADAGyEPiCmb\nzYefXpehJ6pUyQsKIf7Xbc9kJHm2a4ozeQAAAP2AkAfEtLWQt+WevB37TF6HYyUvVQh5gZAHAADg\njZAHxNRTyNty8Eo19+RJweNMXj7keZ7JY08eAACAP0IeEFM5lbzCnjyPKlhf5Sp5Hu2ajmfyiu2a\nJWfyAAAA4IKfroCYyg15QVaVkJeQTyWvI38mL5WI/9dGsZJHuyYAAIA7Qh4QU6YQfkr25HUfvFI4\nk1eVZegmBYev2y9n8mjXBAAAcEfIA2LaVdo1iyEv6VfJY7omAACAP0IeEFNfBq8kHVod+yphXu2a\n/XAmLwR1LkNnTx4AAIAHQh4QU61X8hK5C4r9OoXpmj5n8nLfqyyVPAAAAHeEPCCm8kJeqNrgFfOq\n5GX92jXZkwcAANB/CHlATFsLeV0Hr0SKQpWma5pcKnkdxUqe5woFQh4AAIA3Qh4QUzmVPCuuUKjC\nmTxJwWEXXcaxklccvBJKpmuKM3kAAAAeCHlATIWQV7pCoafBK1G19uSZJNfBK37tmtnSZehU8gAA\nAFwQ8oCYCnvytnkmT6F6lTxTZ5CKIZPNhbCURyUv/72KgkpCHpU8AAAAD4Q8IKY+TddM7rhn8ooh\nz2O6ZjKpEGU5kwcAANAPCHlATOUMXslN1/QJSH2VMJOsW/vodiicyfOo5CUSCSmErmfyqOQBAAC4\nIOQBMZW7DD23J69K7ZrK76SLoXgmzzvkSZKMSh4AAIATQh4QU1ntmgqKZC5VsL5K5Ctl2diVPL8z\nebkQHHWGPEsQ8gAAAJwQ8oCYyj2TF2RVOZNXyGRRzAzVGfKSvTyyd4VKXjHXEfIAAADcEPKAmPqy\nJ6+uZM1CpfhV8rzP5JVW8kzsyQMAAPBByANiKqxQKN2T19PglUjW5TGVknQ6k1eo5KW9KnkKtGsC\nAAD0A0IeEFNZg1fyZ/LSqfgBqa8S+ckrkVPIS6W8zuQFFS+JkAcAAOCGkAfEVP6ZvERVQl4y4dOu\nmY28z+R1H7xCuyYAAIAHQh4QU7ln8qIg1aXTFb++wpm8uJW8YshzWANRHLzSZYUCIQ8AAMADIQ+I\nqayQpyi3DL0aZ/KcKnmZbO75aad2TeWDryTaNQEAABwR8oCYthbyug5ekaJQpcErhZDndibPp10z\nlC5DN5ahAwAAeCHkATGVNXglX7WqZsiLuyev0K7pNl2z++AVVigAAAC4IOQBMRVCXvcVClsOXqlS\nyHPbk+dXyUsmk1vuyaOSBwAA4IKQB8RU2JPX25m86rVr5v6Yx23XLFbynNo1c4NX8jdwJg8AAMAN\nIQ+IqbwVCkGRqtuumclf5/bK5EOiX7smg1cAAAD6AyEPiKmcwSuFSl66CisUkslcyOvIxAt52axv\nJS+UrlBgTx4AAIAbQh4QU3mDV4KCQlXbNWOHvEIlz+E9mFnXwSviTB4AAIAXQh4QU/nL0KtzJi+V\nKFTyMrFex/NMnplJ+d2BuRuo5AEAAHgh5AExlXUmT6GKKxR8K3mppNNfG1325HEmDwAAwAshD4ip\n3DN5oVohL+ncrpl2eg9dpmua2JMHAADgg5AHxFRYobCtPXlWxemaqXwlL+50zcKevTqHdk1J+XOK\neezJAwAAcEPIA2IqZ/BKbrqmduzpmo6DV3ICKxQAAAD6ASEPiKmswStS1c7kdVby4oWozpDnU8kz\nlqEDAAD0C0IeEFPZ0zVVpemaTmfyoigoRFnH99B98Apn8gAAADwQ8oCYyhu8EqSoOnvyvEJeNgQp\nRF3eZzwlZ/LYkwcAAOCGkAfEVAh5iUTnH6ct2zUjZVWlds1CyIs7eCUKCpFjyKNdEwAAoF8Q8oCY\nstnsFsFny8Er1TuTV7i2uGfyosi3kmellTxCHgAAgBtCHhBTNrvlObWeKnmhStM1U4ncdE2XFQoh\nOLZrqmvIAwAAgAuXn6zM7EQze9nMXjWzC3u4/2tm9pKZzTOzB81st5L7smb2XP7jLo/rASopk8ls\nEXy6h7xEfl1ANds1Y1fyghRCJDPzuKxcJa/YrikqeQAAAE5i/8RpZklJP5E0Q9IySc+Y2V0hhJdK\nHvb/JE0PIWw0s3+SdLWkv8vftymEcHDc6wCqpad2zZ4Gr1Qv5OXbNTPxQ54ixyAWgoLygZF2TQAA\nADcelbzDJL0aQlgcQmiXdKukmaUPCCE8HELYmP/0SUkTHL4uUBO2FvK6V/JClUJeulDJi2KuUMhP\n1/TStZJHyAMAAPDiEfLGS1pa8vmy/G1b8xlJ95Z8PtDM5prZk2b2MYfrASqq18Er+V+z1arkpbwG\nr8g9iHUdvMKePAAAAA8V/YnTzP5B0nRJR5XcvFsIYbmZ7SnpITN7IYTwWg/PPUfSOZI0adKkilwv\nUI5eK3n5YBRU3T15cUNeYfCKF2NPHgAAQL/wqOQtlzSx5PMJ+du6MLPjJH1T0kdDCJsLt4cQlud/\nXSzpEUmH9PRFQgizQwjTQwjTR44c6XDZgI+yQ15UrXZNp0peyE0J9cQKBQAAAH8eIe8ZSVPMbA8z\nq5N0hqQuUzLN7BBJNygX8FaW3N5kZgPyvx8h6UhJpQNbgJrX6+CVQrumqrRCId+umXUIef5n8hi8\nAgAA4C12WSGEkDGz8yTdLykp6cYQwnwzu0TS3BDCXZKukdQg6bb8+PU3QwgflbSvpBvMLFIucF7Z\nbSonUPN63ZOXDy/Vmq5ZrOTFnIwZ+rNd0xIqqesBAAAgBpefOEMI90i6p9tt3y75/XFbed7jkg7w\nuAagWnrak9d18Eo+XEVVOpPnNXhF8h2OEkrbNY3BKwAAAE5clqEDu7Jyz+RVa7pmOpX7Y56N4oUo\n7zN5ZoEzeQAAAP2AkAfEVG7Ii1Stds3c1/QYvOLbrqmSZehM1wQAAPBCyANi6nXwSr5eFVWpXTNd\nGLwS+0xe7hydF+vyCXvyAAAAvBDygJjKXqEQqjNdszPkxQtRQb7n5rpU8tiTBwAA4IaQB8TUU8jr\nOnglX8kLQYlE5f/IeVXyIvlW8tR9uiYhDwAAwAUhD4ip3BUKQab8CpGKShena8as5PVLuyZ78gAA\nALwR8oCYelqh0DXkhdJfKi6VSimEKP50TUnm2a5pYk8eAABAPyDkATH1OnilUKGqQhVPUu7aoij+\n4BWZcyUvSEzXBAAAcEfIA2Lq/Uxe5+CVakgmk1KIlI15Af3RrsmZPAAAAH+EPCCmsqdrVrGSFyKP\ndk3vSl7pnjxWKAAAAHgh5AEx9Rry8sEoqIrtmiFS5LBCwfMd5DJvabsmIQ8AAMADIQ+IqdxKXnUi\nXr6S5zB4JUgyc56uaSWf0a4JAADggpAHxLS1FQrdB69Us11TUTb+mTyZ73RNdW/XJOQBAAB4IOQB\nMZU9eGVnaNd0fAtd2zUJeQAAAF4IeUBM5e7JU6jy4JW4RTgz178wTCXVTfbkAQAAuCHkATH1diYv\nFCpUiepW8uKfyTPXM3mJLQavUMkDAADwQMgDYuot5GWz2cKtFb6ynGK7pseZPMf3QLsmAABA/yDk\nATFtLeQVBq8UfrUq78mL4mYoMyWcp2t2adck5AEAALgg5AEx9TZ4JdTI4BWP6Zqef2EktqjkOb44\nAADALoyQB8TUe7tmoZJXnT9uuUpeVjGP5EmW8J2uKanzBTmTBwAA4IWQB8S0tT15hZAXRdnCjZW+\nNEm5qmLuTF681wky19kxxuAVAACAfkHIA2La2gqFwlm8kA951TqTl7+I2INXcmfyfC5HyrdrciYP\nAADAHSEPiKnXds1CCa1K7ZqSZCG4tGv67smzriGPQ3kAAAAuCHlATL0NXomKlbwq/nFzCXmmhGM1\nMlfJy39PqOQBAAC4IeQBMW2tklcQqrxCISeKXScLluiHM3nKhWHO5AEAALgh5AExbSvkhRA69+Ql\ndoJ2TdczebkXi4Ko5AEAADgi5AExbSvkRVFUsgy9mn/caq9ds/BSUQj5kMeZPAAAAA+EPCCmra1Q\nkHKVvFBcoVDdSl7sCGUJeRYjCy9FyAMAAPBFyANiCCFsdfBK4f5CJS/h2evYRxazkhdCkFlCSc/B\nK4lCEJZYhg4AAOCHkAfEUAhw2z6Tl5+umeha7ausoKDtD2iFgOg7XbNwJo/BKwAAAJ4IeUAM2Wwu\nwG075OUSUlUreTEHrxR2/SUd/8boPJMn9uQBAAA4IuQBMfQW8roMXkl0fUwlWexKXj6o9lslj+ma\nAAAAXgh5QAzlVPJCyD0mUc3BK4o3eKWzkue8DF35bEe7JgAAgBtCHhDD1kJe6eCVUBy8suNW8rLF\nSp7XFZVUO6nkAQAAuCLkATGUdyYvH/I8D7T1kSneibdsNj9gxrWS1z3kcSYPAADAAyEPiKEQ8ra1\nJy/KPyaRrN50TbN4lbxM1n8NRKL74BUqeQAAAC4IeUAMmUxG0rYHr2QyHZI6WzirITe7svyAtuTd\nDXpsUYvaM7ng1ZHJVywd30OiJAgXa41U8wAAAGKr5uIuYIdXTrtmNpsPglUMeRYiba4bokMu+ZMG\npJJKJU0h5Fol9xrVoItP2U97jWosPv5bd87Xo6+0qLm+Tr/5/OFKq/AePJeh537tiPLtmlIu5DlO\n8AQAANgVUckDYihn8Eqh2lfNds365U+reeVfdcpB4/ThvUfosN2bdfjk4TpyrxGat2ytTvrh/9VL\nb71XfPySdzdoyqgGrd7QrqdfX632YiXPL4A1WK7C+ea7GztDHrvyAAAAYqOSB8RQ1uCVwpm8Kk7X\nHNj2ruqXPqpLZn5ri/uWrt6oD139sB5/bZX2GzdE2SjorTWbdPYH99Cy1k16deV6HTx2oCQp5ViN\nbEq0SVlp0cp1OtxK9ylU7/sEAACwM6CSB8RQVrtm4dxeqnrhJZlMFq+1u4nNg9VcX6fXWtZLkla8\n16aObNCk5sGaPKpei1auU0fGv12zPpFR1LZer6xY19miyfAVAACA2Ah5QAy9hbwoipQpnsmrzZAn\nSZNH1uu1lRsk5Sp7kjSxabD2Gtmg11auLxm84hfyksmE2lct0cvvrCs5k0fIAwAAiIuQB8RQzgqF\nYiUvna7sxZXoPeQ1FCt5y1o3SZImNA3SlNGNemttm97b1J57Hcd2zWQyqY5Vb2rRivUKKhm8AgAA\ngFgIeUAM5QxeKZzJq+1KXoPe3dCu1g3tWtqaq+SNbxqkySMbJEmvtuRu852umVDHqje1ZlOH1rfn\nK3hU8gAAAGIj5AEx9LYnL4SgbJR7TKpGz+RJ0uRR9ZKkxavWa1nrJo0eMkADUkntNSoX8l4rhjzH\nPXmJhDpalkiSWtbnKoWEPAAAgPhcfmIzsxPN7GUze9XMLuzh/gFm9j/5+58ys91L7vtG/vaXzewE\nj+sBKqUv0zVTVVyhUE4lT5JeW7lBS1dv1MSmwZKk3YYPVjppWry6TZKUSvqGvPZVb0qSVhLyAAAA\n3MT+ic3MkpJ+IulvJe0n6Uwz26/bwz4jqTWEsJekf5N0Vf65+0k6Q9I0SSdKui7/esAOoZzBK8V2\nzVTthrwJTYNVl0rotZZcJW9C0yBJUjqZ0O7D6/Xaqtw5Pe92zWjjGg2vT2tRvlLInjwAAID4PP5Z\n/jBJr4YQFocQ2iXdKmlmt8fMlPSL/O9/K+lYy/0UPFPSrSGEzSGE1yW9mn89YIewzZCXTOuXc1fo\ntUIVLFW7g1eSCdOeI+r14ltr9fbaTZrYPLh434f3HqnVmwrVSN9KniR9+gMTtCg/2bP74JVN7VnN\nfWO15rzSolXrN7t9bQAAgJ2ZR2lhvKSlJZ8vk/T+rT0mhJAxs7WShudvf7Lbc8c7XNNOZdOGdVpw\n3d9X+zLQg2T7Zt31lUPU+PQ1+uvzPy7evs/GDbr7nw+Tzb1AE6xFSmw5gbOi19lLyJOkw/Zo1n89\nkTsjV6jkSdLXj5+qB19crjfWtCvZDyHvzPeN1a+eGSC1S/P+40xlrE6SFIWg1RvalYlywW+uScMb\nBihpftVEAACw49s86gAdftbl1b6MmlK9nzr7yMzOkXSOJE2aNKnKV1NZUZRV86Y3qn0ZNSMUqj1m\nqvaP+1EUaVh9h+oym5WIOgPQ0CirkYl2jR3SoM0dGT327ngNHjaiatdZTsj71sn7adq4IfrD82+r\nuaNFP/vZ/Zo+fboOOeQQXfjhETrrh3dr3P77uF1TIeQlTZpx/Ef02r0PakjbW10eM2FgUg0DUkqY\naf3mDm1s2/Z7AAAAu54V7zVX+xJqjkfIWy5pYsnnE/K39fSYZWaWkjRU0rtlPleSFEKYLWm2JE2f\nPn2XOrhT3zhM9d9+odqXURNefPFFHXLIIcWpls3NzZowYYKGDRum9vZ2jRw5UpMmTdKECRMkSQce\neKBOOumkfrueBx54QDNmzNBjjz2mD37wg8Xbb7rpJp199tl644039Prrr+uYY47Rw6fU99t19KY0\n5P385z/X/fffr9NOO02TJ0/W1KlT1djYqHQyob87dJL+7tBJOvroozVnzhxJ0sMPP6xxzc1a+Ztv\nq/n037pdUyHkRVGk/aYfLU3/6zYfP3ib9wIAgF3V7tW+gBrkEfKekTTFzPZQLqCdIal7b+Fdks6S\n9ISk0yQ9FEIIZnaXpF+b2Q8kjZM0RdLTDteEnVAIQV/60pc0ZMgQXXvttXr77be1bNkyLVu2TGvX\nrlVDQ4OWLFmiRx99VGvXrpUkDRo0SCtXrlRDQ0O/XFNvKxTOPPNMDRs2TFJttGvOnz9fX/ziFxVC\n0G9/mwtskyZN0l//+lcNHz5cUu77PG/ePJ144om67777NG/ePH3oQx8qvo6X0pAHAAAAP7F/6syf\nsTtP0v2SkpJuDCHMN7NLJM0NIdwl6T8l/dLMXpW0WrkgqPzjfiPpJUkZSV8MIdCPhR7dfvvteuSR\nR3T99dfr05/+9DYfu379ej311FM67rjjdOedd2rWrFn9ck29LUN/4oknirdVM+Q1NjZqyZIlOuaY\nYzRkyBC98MILeuONN7Ro0SJ95jOf0Wc/+1ndfvvtMjO99dZbam1t1cknn6y//OUvWrRokY444ghJ\n/RPyemsjBQAAQN+4/NQZQrhH0j3dbvt2ye/bJH1yK8+9TNJlHteBndvdd9+tUaNG6XOf+1yvj21o\naNAxxxyjiRMn6pZbbnENeevXr9evfvUrNTY26sknc3ODtlbJK1XNkHfppZeqqalJv/jFL/SjH/1I\nY8aM0ZgxY/SBD3xAK1eu1Ne//nX98Y9/1Mknn6wXXsi1Bh9wwAGaMmWKFi1aVAxiCedl6BKVPAAA\nAG9+P7EB/WzhwoWaNm1a2dWkRCKhM844Q/fff79WrVrlcg0dHR067bTTdO6552rWrFn68Y9/rIkT\nJ24xDKgQ8qZOnVq8rZohr6mpSZdeeqmWLl2qU089tct9X/7ylzVkyBDdeeedklQMefvvv/8WIc+z\nkldXl5uiuXHjxl4e6a+trU3f+MY39O///u+aN29e5zAfAACAncAOM10Tu7YQghYuXKgzzzyzT8+b\nNWuWrrnmGs2ePVsXXXRR7Os4//zzdf/99+v666/XUUcdpfr6ek2cOHGLyt3w4cOVTCY1e/ZsHX30\n0QohKJ2u3p68bUmn05oxY4buvfdehRD0wgsvaNy4cWpubtaUKVN02223af369ZJ8g+qUKVMkSS+/\n/LL22msvt9ctx/nnn68bbrih+Pnee++t97///dp9993V0NCg1tZWtbW1qbGxUUcccYSOPPJI1dfX\na/78+Uqn09pnH78powAAAN4IedghrFixQmvWrOnzD9cHHXSQZs6cqSuuuEJnn322xowZs93XcP/9\n9+u6667TV7/6VZ177rnbfOwJJ5ygN998U+PGjdMBBxygefPmVbWS15u//du/1e9+9zu98MILeuGF\nF3TAAQdIygWxKIr0y1/+UmamAw880O1rTps2TZI0f/58feQjH3F73d785je/0Q033KALLrhA5513\nnv74xz/qjjvu0EMPPaTly3PDfdPptAYOHKgNGzYU20kHDBigzZtzC9mnT5+uO++8U+PGjavYdQMA\nAJSrdn/qBEosXLhQkrTvvvv2+bnXXHONpk2bppkzZ2rffffV0KFDNXDgQLW1tRU/Nm/eLDNTIpHo\n8pFMJlVXV6cBAwbotttu07777qvLL+992WYikSgGgCOPPLLmQ96JJ54oKXfuccGCBTr22GMldVbb\nbr31Vh166KEaNWqU29dsamrSuHHjNH/+fLfXLMePfvQj7bfffrr00kuVSqX0+c9/Xp///Ocl5c4H\nbty4UfX19TIzbdq0SXPmzNHzzz+vlStX6sADD9SaNWv0ta99TT/5yU902WUcJwZqXQhBbW1tGjRo\nULUvJbaVK1dq0KBBamxsrPalAKhxtftTJ1CiEPK2p01uypQpuvrqq/WDH/xA77zzjtauXVv8D/7A\ngQM1cODA4vmwKIqUzWYVQlA2m1U2m1V7e7s2b96shoYG3XzzzRo4cGCfvv7MmTN1yy23aMSI6i1D\n78348eN10EEH6eKLL1Ymk+lSyZNyqyL6o9o2bdq0ioa81atX64knntBFF13UY+hOJBJd1m0MGjRI\nJ554YjEEFzzwwAO66aab9N3vfreq4T2E0OOQn1KrV68u7pD0PFMJ7Ch+/vOf64ILLtArr7xS038P\nl+P444/XxIkT9Yc//KHalwKgxhHysENYuHCh6uvri0vO++r888/X+eef73xV5TnhhBO0evXqXn8Y\nr7bvf//7uv3229XU1KSPfexjknJnC4cNG6Y1a9b0W8i74YYbFEWR6+TOrbn//vsVRVHs9/LZz35W\nd999t+69916dcsopTlfXNxs3btT06dP1yU9+Ut/97nd7fMw999yjU089Ve3t7dpnn330xBNPFPc2\nAruKJ598Uq2trbrxxhv1L//yL9W+nO3W3t6uF198UfPmzdPixYu15557VvuSANQwpmtih7BgwQLt\ns88+NR+UtmZHuO7jjjtO1113nS677DINHTpUUu669957b40ZM0aHHHKI+9fcf//9tWnTJr3++uvu\nr92TP/7xjxoxYoQOPfTQWK9z0kknacyYMbrqqqvU3t7udHV9c9NNN2nBggW65JJLdO+993a5L5vN\n6uabb9bHP/5xTZs2Tddcc40WLVqkr371q1W5VqCaXn75ZUnS9ddfv0Pv5Vy8eHGx02T27NnVvhwA\nNY5K3k5m1apV6ujo0NixY6t9KW7Wr1+vl156SUcddVS1L2WXdMUVV6itra1fKm2lw1cmT54c67XW\nr1+vf/7nf9Zjjz2m448/XgcddJCGDBmiNWvW6JlnntFbb72lOXPmaObMmbHbFtPptK6++mp96lOf\n0hlnnKGjjz5amzZtUjqdVl1dndLptAYMGKCmpia1tLRo0aJFymQyam1tVWtrq0aPHq36+nplMpni\nRxRFiqJII0eO1GGHHaZTTz11q/84kMlkdO211+qwww5TW1ubZs6cqQMOOEDNzc3avHmzXnvtNb31\n1lv6wAc+oLvvvlvDhw9Xa2urLr/8cj344IOKokh77rmnLr30Un34wx+O9b3w8Oijj+rjH/+41q1b\np0GDBmnYsGEyM0VRpBCCQggaP368Jk+erNbWVq1YsUIrV65US0uLxo0bp1NOOUU/+MEPaqId9Re/\n+IVuueUWZTIZdXR0aOjQodp9992VSCQ0YsQIHXDAARo7dqzq6uoUQlBzc3Px/OeYMWOK/8BSbSEE\nXXvttbrzzju1cuVKjR49WolEQnV1dRoxYoRGjBihoUOHasCAARo4cKAGDBighoYGHXvssXrkkUd0\n1VVX6b//+7/75R+HZObkxwAAHhtJREFU+uqVV17RuHHj9MYbb+iss87S7rvvrj322EMjRoxQQ0OD\nGhoa1NTUVOxcqIX/P+pJ4djCHnvsoZ/97GfavHmzBg0aVPwYPXq0JkyYoEQioaFDh6qpqUltbW3F\n/5txjg/YtdiOuB9q+vTpYe7cudW+jJqyceNGHX/88Xr88ccl5VoEDznkEI0bN07jxo3T2LFjNXLk\nSDU0NKixsVGDBw/eIapL//mf/6nPfe5zCiHoqquu2qFbbbCl9957T0OHDtU3v/lNXXrppdv9Ok89\n9ZRmzZqlxYsX60Mf+pCefPLJLhW2IUOGaLfddlNra6tuvvnm4mCZuK6++mpdcMEFvT6uEPyGDBmi\n5uZmvfPOO2pra1MqlSp+JJNJmZlWrlypjo4OXXLJJfrWt74lSXr22Wf1zjvvaMiQIRo4cKBuvvlm\nXXfddfr973+v973vffrxj3+s559/XuvXr1c6ndbYsWN12mmn6dRTTy2G882bN+uiiy7SqlWrZGaa\nM2eOWlpa9Oc//1mHH364y/dje5188sl66qmndPbZZ6utrU2tra0ys+KHJL3++utasmSJhg8frtGj\nR2vUqFEaMWKE5s+fr3vvvVe///3vi23G1ZLNZjV+/HiZmfbcc0+lUim1trbqzTffVAhB7733Xq+v\nccEFF+jKK6+swNVu2xNPPKEjjjhCBx98sKZMmaKWlhZFUaT29na9++67WrVqldasWbPNHZOf+MQn\n9Nvf/raCV72l1tZWNTc36/LLL9ddd92ll156SRs2bNhmRS+VSimdTmv48OHavHmzRo4cqcsuu0wz\nZ86s6n83r7rqKl144YW699579ZnPfEbr1q3Tpk2blMlkynr+XnvtpQkTJmjq1Km67rrrKtIiX47n\nn39ep59+ujo6OrTbbrvp4IMP1vDhw9XQ0KChQ4dqypQpmjhxooYMGaIhQ4bUbAjvbuHChZo7d65G\njhypUaNGFT9qdZ1SuV588UU1NjZqt912q/alQJKZPRtCmN7jfYS8ncMDDzygGTNm6LzzzlNTU5N+\n9atfacmSJVv9D1kikdCwYcOKHyNGjNAJJ5ygT33qUzV1MP2YY47R0qVLde211+qEE07o89AT1L73\nv//9evr/t3fvwVXVd7/HP99kJ9nkRgIEpBCLTBHQelS8FFt1nsdaa1tadUatnYOlaovaYtFT66PS\nGXuOtcVqW+j0eDpyUQRbfaoVEHHEC1ZbWhUFC1otlJtATEISkp37vnzPH9l7N0FAkr0hYfF+zazZ\n67YX3wU/9lqfdX39dZ1yyikKh8Pq6OhQIpFQcXGxOjs71djYqMbGRo0ePVqnnHKKhg8froqKChUW\nFmrPnj1at26dVq5cqdGjR2vx4sU677zz1N7erqqqKkUiEZWUlOj4448/bDsGH3zwgcLhsIqKihSN\nRtNdKqyUlpZq7Nixh7xDFYvF9O1vf1uLFi3Sddddp1Ao1OOdflLX/9+pU6fqoYce6vOOWlVVlc4/\n/3y1tLRo+/bt/bbjsW3bNo0dO1azZs3S3Xff3evvx2IxjRs3TqNGjdKf//znw1DhoXv11Vd1/vnn\n6/e//72uuuqqj0yPRCJ69913VVtbq2g0KjNTXV2dJCkcDmvJkiV6+eWXtXv3bpWXlx/p8nu44447\ndP/996u2tvaA93G6u2KxmDo6OtTe3q7a2lotXbpUpaWl2r59u37xi19o8+bNOuGEE45w9f/22muv\nafLkyVq2bJm+9rWvSepqMx988IEaGhrU3NysSCSihoYG1dXVqb6+XrFYTO3t7aqvr1c4HNYrr7yi\nd999V4sXL9bUqVP7bV2uvfZaPfvss6qqquoxPhaLqa2tTbt379bu3bvl7mpsbFRDQ4MKCgoUjUa1\na9curVu3Tlu3btVbb72lF154IWsHuzIRi8U0efJk7dixQxdddJH++c9/asOGDWpvbz/gd4qLi1Va\nWqrBgwertLRU48eP129+85sBc6bS3XXrrbdq7ty5+90HKy8vTx+oGj58uEaMGKGKigqVlZVp8ODB\n6fU69dRTB9T+mNR1sO3EE09ULBbTkCFDlJOToxNOOEEnnniiSkpKNH78eI0dOzb9m9DW1qbOzk6N\nHj1ao0eP1qBBgzRx4sQBc4BB6vr3WrhwoR5//HF98pOf1IQJEzRx4kRNmDBBZWVlKigoUFFRUX+X\neUCEvGPA3XffrbvuuksNDQ3py33i8bj27Nmjqqoq7d69W3V1dYpEImpubk7vOO/du1d79+7V9u3b\ntXHjRp133nl65ZVX+nltukQiEQ0ZMkQ/+MEPBsSRbRwezc3NWrBggVasWKFQKKSCggLl5OQoEoko\nPz9fZWVlKikp0datW/Xee++ptrZWbW1t6e+PHz9eF110ke6+++4Bc6lbpqLRqG666SYtXrxYra2t\nmjlzpr7xjW+oqalJkUhEZ5xxRlaOoi5btkyXXnqpnnnmGX35y1/OQuW9d8cdd+jnP/+5tm3bpsrK\nyj4t49e//rVmzpypNWvW9OtZyZtvvlm//e1vVVtb26cdznXr1mnSpEmaO3euvv/97x+GCg/dySef\nrJEjR+qFF17o0/d37dqlMWPG6Hvf+57mzJmT5eoO3SOPPKJp06bpvffe0/jx4/u0jFgsphNPPFET\nJ07UM888k+UKD93nPvc55efna/Xq1X1eRnt7u0aOHKmvfOUrWrJkSRar65s5c+bolltu0eOPP64r\nr7wyPb6zs1MtLS2qr6/X+++/n34ydlNTU3r/pampSQ0NDXrxxRd133336dZbb+3HNfm31Fnwb37z\nm/rhD3+oxsZG1dTUqKamJn2peff+6upqNTQ0fGQ5p59+ut58880BddXVzJkz9cADD+inP/2ptm7d\nqkQioU2bNmnr1q1qampKH7Q6mIcffljTpk07AtUemiuvvFJ/+MMfNG7cODU0NGjPnj0fmWfw4MG6\n7bbbdOedd/ZDhQd3sJCXvt/haOrOOOMMR08XX3yxf/rTn85oGXfddZdL8g8//DBLVWVm6dKlLslX\nr17d36VggGlpafHa2lrv7Ozs71IOq5aWFt+xY8dhW35HR4cPHTrUr7rqqsP2ZxzMnj17vKyszC+7\n7LKMlhOJRHzo0KF+zjnneCwWy1J1vZNIJLyystK/+tWvZrScs88+20866SRPJBJZqqz3Nm3a5JJ8\n7ty5GS1n6tSpXlRU5LW1tVmqrPdmzZrlubm53tHRkdFybr75Zi8oKPBIJJKlynpvyJAhfv3112e8\nnBtvvNHD4bDv3bs3C1X1XTQa9ZEjR/qFF16YUXu/4IIL/BOf+IS3t7dnsbq++/GPf+xm5nV1dYf8\nnc7OTq+rq/MtW7b4+vXr0/tja9asOYyV9k5DQ4MXFRX51VdffcB5qqqq/I033vANGzb45s2bfdeu\nXV5dXe2vv/66L1++3MeMGeNf/OIXj2DVB1dVVeWSfObMmR6Px93dvba21l999VWfP3++z50712fP\nnu0zZszwP/7xj/1c7f5JWusHyEv9Htj60hHyeorH415WVubf+c53MlrOunXrXJIvXLgwS5Vl5oYb\nbvDi4uKMN84ADuy73/2uh8Nhf+mll3zVqlW+du1aX7Jkic+bN8/feOMNX79+va9atcoXLFjgzz//\nfPogUCQS8W3btvmWLVu8ubk5vbxEIuHxeNyj0ah3dHSkN5z7M2PGDM/JyfGNGzdmvB6PPPKIS/Kf\n/exnvnPnTq+pqfHGxkZvb28/6A5kIpHwWCyWcaiaP3++S/KHH344o+U89NBDLsnnzJmT0XIyMWvW\nLJfkW7ZsyWg577zzjpuZz5o1K0uV9d7ll1/u48aNy3g5q1evdkn+5JNPZqGq3qutrXVJ/stf/jLj\nZb3++usuye+9994sVNZ3Tz/9tEvyp556KqPlPPfccy7J582bl6XKMvPZz37Wzz777IyWEYlEvKSk\nxKdOnZqlqjJ37733uiRfv359n5dx2223eSgU6lUAPpwWL17skvzNN9/s71L67GAhj8s1A+Af//iH\nTjrpJC1cuFDXXHNNn5fj7qqsrNRnPvMZPfnkk1mssPdaW1s1YcIETZo0SUuXLu3XWoAgS92z1Bsl\nJSWKRCI9xuXk5CiRSOx3/tQ9i0VFRSosLEz3/+Uvf9H06dP1wAMP9Ln+FHfXlClTtHLlyv1OTz3x\nND8/X7m5uero6FBnZ6c6Ojrk7gqFQukHUxUXFysUCqmlpUWhUEiJREJNTU3Ky8tTYWFhuhs0aJCK\niopUUlKixx57TJ///Oe1YsUK5efn93k9EomErrjiCi1dulQ33XSTysrK1NjYqFgspry8POXl5SkW\ni6mlpUWtra09upaWFrW3t8vdVVxcrHA4rLq6OpWVlamiokKRSEQ5OTkqKCjo8VTM1CXS7q7t27dr\nxYoVmjJlSlZeuH3FFVdo1apVuv3221VaWqqioiK5u+LxuOLxuKLRqDo7O3t8uruGDh2avs+vuLg4\nXV9FRUX6SbVmpvLycg0fPjz98KJUl5OTo9zcXM2YMUPjx4/PeF2i0ahGjBihc889VzfddFP63tvU\nv0s8HtfevXvV0NCgaDSqcDiscDiseDze4/aI7p9tbW0aNWqUysrKFI1GVVxcLHdXS0tL+qEjJSUl\nSiQS2r17txYuXKiVK1fqS1/6Ukbrkvq/8uyzz2r27Nk67bTTVFBQoLy8vB4PPDIzFRQUpJ/eWVhY\nqHA4rPz8fIVCoYwvI7zsssu0Zs0a7dy5M6N7gt1dkydP1ubNm/X222/3+X262bB3714NGzZMd9xx\nR5/uMe5uxowZmjdvnnbs2KERI0ZkqcK+6ezs1NixYzVhwoQ+X8ItSWvXrtVZZ52lBQsW6Nprr81i\nhX0zbdo0rVy5UtXV1QPqPsHe4J68I+CBBx5QS0tLeiNcVFSUvoG2ublZ9fX1XanaLL0zsWfPnvQD\nJYqKitINLBKJKBqNavjw4TKz9M5IaicltfFI9S9fvly33XZb+l1ymbj++uv1u9/9Ti+//LLy8vKU\nm5urUCik+vp67d69O11/6lHx3btoNKq2tjaZWXqHr7GxUZJ6POY5HA7LzNTe3p6+xyi1k5Wbm6vO\nzk7Nnj1ba9as0bJly/rtZdPAseLpp59O/y7V1tbqU5/6lAoLC/X3v/9dkjR06FCNHj1a27Zt04YN\nG7R582aNGjVKxx13nNxdNTU1am5uTu9cp34DzEzRaDQdQFKfqf78/Hw9+uijWXu4QHNzs5YtW6bW\n1lZ1dHT0CHLd++PxeDrcpH6/2tvbFYlE0vctx2IxFRUVKR6Py8xUWlqqWCz2kWDV3Nys6upqnXXW\nWXrsscdUWFiY8Xq0trbqkksu0Z/+9CdFo1GVlJQoFAqlg0X3sJkKzqmu+z2tbW1tGjZsmOrr61VX\nV5e+T7C9vT39YITU301qXyAvL0833HCDfvSjHykUyvwtSxs3btSFF16o6urqQ/6OmaXrKSwsVGtr\na0Y13HnnnbrnnnsyWoYkfetb39KiRYv69N38/HwNHjy4x8M1wuGwPvjgg/RTcSORiMxMxcXF6Xvn\nm5ublZOTo1AopKFDh2r9+vUaPnx4xuvS1tamyy67TM8991yflzFo0KD0A1BycnK0d+/e9P+X7k/G\ndXeNGTNGZWVlqq+vV05OjvLy8vTXv/5Vt9xyi+67776M12fTpk2aNGmSxo8fr69//esKhUKKx+OK\nxWL7/dx3XF5eXnq/KrUfc/rpp6uioiK9H5TaR+n+ue+4l156Sddcc41eeeUVnXfeeRmt0/vvv69T\nTz1V55xzjp577rmMDh5lasmSJbr66qszPsjg7ho7dqxGjBihFStW9OuDZdy7Xs1z/vnn67HHHuu3\nOjJFyDsCKisrtXPnzn7784cNG5aVIxHPPPOMpkyZkqWq+i6183f55Zf3dykA0G8SicRRe4Q5xd3V\n0dGRDtBmlj7jlnq9SOrAYW5urtw9/WTI4uLi9Blid1ddXZ1CoZDKysrS89XW1qZ33PftvOsWDw0a\nNCjj9WhqatJbb72VPtCZl5enUCikWCyWPqtYVlam/Pz8dIg2M5WVlfX5ydCpg8OHQzwe19tvv622\ntjZ1dHSkz6J27zo6OtTW1qbW1la1tbWpvb1dnZ2d6uzsVGtra/pBKIlEQuXl5el/v1SXWod//etf\nikQiGjZsmNxdnZ2dys3N1fz587P2KP4nnnhC06ZNO+hBge7BrHs4Sz1Rtb29XYMGDVI8HldHR0ef\n6igpKVFdXV1Wnlj86KOPaurUqaqsrEzXnGp3qYM+qX+P1MHy1Drl5OT0+L+QCrWJRCJ9IK77Qbnc\n3Nz0SYRwOJw+KB8Oh7Vu3ToNHjxYGzduzLg9zps3TzfeeKOKioo0adIkFRYWqqWlRRUVFTr++OPT\nXXl5eY82lPo8UH9vx1VXV2v69OmaP3++rrvuuozWqT8R8o6A1A9k6j9cS0tL+hKN4uJiDR06NH10\nMhqNqrm5Ob1B2LVrl9ra2tINMXXUtra2Vu6ePtqc+hFKbTxSnbtr4sSJmjRpUsbr4e5avXq1IpFI\n+gch9ULfysrK9JH5fX9cOjo6lJ+fn96QpX5ESktL0y/6TXWpmsPhcPrSqNSPbDweV35+viorK3Xc\nccdlvD4AAODYkLrcNRaLfeQsW28OlkSjUb333ntqampSLBbrcdbv4/pPPvlknXvuuVlbpwcffFCr\nVq1SYWFhep8sdfYxdSVC6lLb7rUkEomPhNnU30NqfzORSCiRSMjdewTd7l3qNQg/+clPsvYu0nfe\neUf333+/Nm/erLa2NhUWFqqmpkY7duzo8fTswy0UCmnLli19frLzQEDIAwAAADBgpc7Wb9++XU1N\nTT0u+z1Yf1+nl5eXa8yYMUds/Q6Hg4W8zC+4BwAAAIAMmJmGDRs24F4Cf7Q6ui/0BwAAAAD0QMgD\nAAAAgAAh5AEAAABAgBDyAAAAACBACHkAAAAAECCEPAAAAAAIEEIeAAAAAAQIIQ8AAAAAAoSQBwAA\nAAABQsgDAAAAgAAh5AEAAABAgBDyAAAAACBACHkAAAAAECCEPAAAAAAIEEIeAAAAAAQIIQ8AAAAA\nAoSQBwAAAAABQsgDAAAAgAAh5AEAAABAgBDyAAAAACBACHkAAAAAECCEPAAAAAAIEEIeAAAAAAQI\nIQ8AAAAAAoSQBwAAAAABQsgDAAAAgAAh5AEAAABAgGQU8sxsiJk9b2abkp/l+5nnNDP7q5m9Y2Z/\nN7Ovd5v2sJltNbP1ye60TOoBAAAAgGNdpmfybpf0oruPk/RicnhfrZK+6e4nS7pY0hwzK+s2/Yfu\nflqyW59hPQAAAABwTMs05F0iaVGyf5GkS/edwd3/6e6bkv27JdVIqsjwzwUAAAAA7EemIW+Eu1cl\n+z+UNOJgM5vZ2ZLyJf2r2+h7kpdx/srMCg7y3elmttbM1tbW1mZYNgAAAAAE08eGPDN7wcw27qe7\npPt87u6S/CDLGSlpsaRr3D2RHH2HpAmSzpI0RNJ/Hej77v6gu5/p7mdWVHAiEAAAAAD2J/RxM7j7\nhQeaZmbVZjbS3auSIa7mAPOVSnpG0ix3/1u3ZafOAnaY2UOSbu1V9QAAAACAHjK9XHO5pGnJ/mmS\nlu07g5nlS3pK0iPu/sQ+00YmP01d9/NtzLAeAAAAADimZRryZkv6gpltknRhclhmdqaZzU/Oc6Wk\n8yV9az+vSnjUzDZI2iBpmKSfZFgPAAAAABzTrOtWuqPLmWee6WvXru3vMgAAAACgX5jZm+5+5v6m\nZXomDwAAAAAwgBDyAAAAACBACHkAAAAAECCEPAAAAAAIEEIeAAAAAAQIIQ8AAAAAAoSQBwAAAAAB\nQsgDAAAAgAAh5AEAAABAgBDyAAAAACBACHkAAAAAECCEPAAAAAAIEEIeAAAAAAQIIQ8AAAAAAoSQ\nBwAAAAABQsgDAAAAgAAh5AEAAABAgBDyAAAAACBACHkAAAAAECCEPAAAAAAIEEIeAAAAAAQIIQ8A\nAAAAAoSQBwAAAAABQsgDAAAAgAAh5AEAAABAgBDyAAAAACBACHkAAAAAECCEPAAAAAAIEEIeAAAA\nAAQIIQ8AAAAAAoSQBwAAAAABQsgDAAAAgAAh5AEAAABAgBDyAAAAACBACHkAAAAAECCEPAAAAAAI\nEEIeAAAAAAQIIQ8AAAAAAoSQBwAAAAABQsgDAAAAgAAh5AEAAABAgBDyAAAAACBACHkAAAAAECCE\nPAAAAAAIEEIeAAAAAARIRiHPzIaY2fNmtin5WX6A+eJmtj7ZLe82/gQze83MNpvZ42aWn0k9AAAA\nAHCsy/RM3u2SXnT3cZJeTA7vT5u7n5bsvtZt/L2SfuXun5LUIOm6DOsBAAAAgGNapiHvEkmLkv2L\nJF16qF80M5N0gaQn+vJ9AAAAAMBHZRryRrh7VbL/Q0kjDjBf2MzWmtnfzCwV5IZK2uvuseTwTkmj\nMqwHAAAAAI5poY+bwcxekHTcfibN6j7g7m5mfoDFfNLdd5nZWEkvmdkGSY29KdTMpkuanhxsNrP3\ne/P9gBgmaU9/F4FAoU0h22hTyCbaE7KNNoVs68829ckDTfjYkOfuFx5omplVm9lId68ys5GSag6w\njF3Jzy1m9rKk0yU9KanMzELJs3mjJe06SB0PSnrw4+oNMjNb6+5n9ncdCA7aFLKNNoVsoj0h22hT\nyLaB2qYyvVxzuaRpyf5pkpbtO4OZlZtZQbJ/mKTPSXrX3V3SakmXH+z7AAAAAIBDl2nImy3pC2a2\nSdKFyWGZ2ZlmNj85z0RJa83sbXWFutnu/m5y2n9J+l9mtlld9+gtyLAeAAAAADimfezlmgfj7nWS\nPr+f8WslfTvZv0bSKQf4/hZJZ2dSwzHmmL5cFYcFbQrZRptCNtGekG20KWTbgGxT1nXVJAAAAAAg\nCDK9XBMAAAAAMIAQ8o4CZnaxmb1vZpvN7Pb+rgdHBzNbaGY1Zrax27ghZva8mW1KfpYnx5uZ/TrZ\nxv5uZpP6r3IMVGZWaWarzexdM3vHzGYmx9Ou0CdmFjaz183s7WSb+t/J8SeY2WvJtvO4meUnxxck\nhzcnp4/pz/oxMJlZrpmtM7MVyWHaEzJiZtvMbIOZrTeztclxA3rbR8gb4MwsV9L/lfQlSSdJ+oaZ\nndS/VeEo8bCki/cZd7ukF919nKQXk8NSV/sal+ymS/p/R6hGHF1ikn7g7idJmizpe8nfI9oV+qpD\n0gXufqqk0yRdbGaTJd0r6Vfu/ilJDZKuS85/naSG5PhfJecD9jVT0j+6DdOekA3/6e6ndXtdwoDe\n9hHyBr6zJW129y3u3inpMUmX9HNNOAq4+yuS6vcZfYmkRcn+RZIu7Tb+Ee/yN3W9w3LkkakURwt3\nr3L3t5L9EXXtRI0S7Qp9lGwbzcnBvGTnki6Q9ERy/L5tKtXWnpD0eTOzI1QujgJmNlrSVyTNTw6b\naE84PAb0to+QN/CNkvRBt+GdyXFAX4xw96pk/4eSRiT7aWfoleRlTadLek20K2QgeWndekk1kp6X\n9C9Je909lpyle7tJt6nk9EZ1vYIJSJkj6TZJieTwUNGekDmXtMrM3jSz6clxA3rbl9ErFAAcvdzd\nzYzH66LXzKxY0pOSbnb3pu4HvmlX6C13j0s6zczKJD0laUI/l4SjlJlNkVTj7m+a2X/0dz0IlHPd\nfZeZDZf0vJm9133iQNz2cSZv4NslqbLb8OjkOKAvqlOXDCQ/a5LjaWc4JGaWp66A96i7/zE5mnaF\njLn7XkmrJZ2jrsubUgeiu7ebdJtKTh8sqe4Il4qB63OSvmZm29R1e8sFkuaK9oQMufuu5GeNug5G\nna0Bvu0j5A18b0gal3wyVL6kqyQt7+eacPRaLmlasn+apGXdxn8z+USoyZIau12CAEhK39uyQNI/\n3P2X3SbRrtAnZlaRPIMnMxsk6QvqutdztaTLk7Pt26ZSbe1ySS85L/xFkrvf4e6j3X2MuvaXXnL3\n/ynaEzJgZkVmVpLql3SRpI0a4Ns+XoZ+FDCzL6vrGvNcSQvd/Z5+LglHATP7vaT/kDRMUrWkuyQt\nlfTfko6XtF3Sle5en9x5/426nsbZKukad1/bH3Vj4DKzcyW9KmmD/n2/y53qui+PdoVeM7P/oa4H\nFuSq68Dzf7v7/zGzseo6EzNE0jpJU929w8zCkhar637QeklXufuW/qkeA1nycs1b3X0K7QmZSLaf\np5KDIUm/c/d7zGyoBvC2j5AHAAAAAAHC5ZoAAAAAECCEPAAAAAAIEEIeAAAAAAQIIQ8AAAAAAoSQ\nBwAAAAABQsgDAAAAgAAh5AEAAABAgBDyAAAAACBA/j/DG3k57YcuBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "#x, y, s = dataiter.next()\n",
    "a = 1\n",
    "for i in range(100):\n",
    "    x, y, s = dataiter.next()\n",
    "    if y[0,:,a].sum() > 0:\n",
    "        break\n",
    "    if s[0,:,a].sum() > 0:\n",
    "        break\n",
    "plt.plot(np.arange(-BORDER, SEQ_LEN + BORDER), x[0,:].detach().numpy(), 'k-')\n",
    "plt.plot(y[0,:,a].detach().numpy())\n",
    "plt.plot(s[0,:,a].detach().numpy())\n",
    "plt.ylim([-0.5,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zaxR7GIAnffI",
    "outputId": "5951ba49-d959-4cc3-d8af-edf6aea88a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/300] train_loss: 0.58662 valid_loss: 0.50422 test_loss: 0.50825 \n",
      "Validation loss decreased (inf --> 0.504216).  Saving model ...\n",
      "[  2/300] train_loss: 0.41906 valid_loss: 0.38410 test_loss: 0.39097 \n",
      "Validation loss decreased (0.504216 --> 0.384097).  Saving model ...\n",
      "[  3/300] train_loss: 0.33519 valid_loss: 0.32905 test_loss: 0.33933 \n",
      "Validation loss decreased (0.384097 --> 0.329049).  Saving model ...\n",
      "[  4/300] train_loss: 0.28827 valid_loss: 0.29103 test_loss: 0.30620 \n",
      "Validation loss decreased (0.329049 --> 0.291033).  Saving model ...\n",
      "[  5/300] train_loss: 0.25685 valid_loss: 0.25705 test_loss: 0.27648 \n",
      "Validation loss decreased (0.291033 --> 0.257051).  Saving model ...\n",
      "[  6/300] train_loss: 0.23341 valid_loss: 0.23138 test_loss: 0.24990 \n",
      "Validation loss decreased (0.257051 --> 0.231382).  Saving model ...\n",
      "[  7/300] train_loss: 0.20863 valid_loss: 0.21102 test_loss: 0.22715 \n",
      "Validation loss decreased (0.231382 --> 0.211021).  Saving model ...\n",
      "[  8/300] train_loss: 0.19058 valid_loss: 0.19807 test_loss: 0.21330 \n",
      "Validation loss decreased (0.211021 --> 0.198066).  Saving model ...\n",
      "[  9/300] train_loss: 0.18391 valid_loss: 0.18420 test_loss: 0.20043 \n",
      "Validation loss decreased (0.198066 --> 0.184200).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17234 valid_loss: 0.17930 test_loss: 0.19197 \n",
      "Validation loss decreased (0.184200 --> 0.179297).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16542 valid_loss: 0.17029 test_loss: 0.18321 \n",
      "Validation loss decreased (0.179297 --> 0.170289).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15733 valid_loss: 0.16452 test_loss: 0.17885 \n",
      "Validation loss decreased (0.170289 --> 0.164522).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15105 valid_loss: 0.15863 test_loss: 0.17275 \n",
      "Validation loss decreased (0.164522 --> 0.158635).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14810 valid_loss: 0.15509 test_loss: 0.16830 \n",
      "Validation loss decreased (0.158635 --> 0.155093).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14683 valid_loss: 0.14944 test_loss: 0.16396 \n",
      "Validation loss decreased (0.155093 --> 0.149435).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14086 valid_loss: 0.14902 test_loss: 0.16289 \n",
      "Validation loss decreased (0.149435 --> 0.149016).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13785 valid_loss: 0.14539 test_loss: 0.16029 \n",
      "Validation loss decreased (0.149016 --> 0.145385).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13558 valid_loss: 0.14430 test_loss: 0.15787 \n",
      "Validation loss decreased (0.145385 --> 0.144301).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13611 valid_loss: 0.14092 test_loss: 0.15535 \n",
      "Validation loss decreased (0.144301 --> 0.140922).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13583 valid_loss: 0.13740 test_loss: 0.15306 \n",
      "Validation loss decreased (0.140922 --> 0.137395).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13086 valid_loss: 0.13756 test_loss: 0.15222 \n",
      "[ 22/300] train_loss: 0.12549 valid_loss: 0.13634 test_loss: 0.15202 \n",
      "Validation loss decreased (0.137395 --> 0.136339).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12839 valid_loss: 0.13646 test_loss: 0.15032 \n",
      "[ 24/300] train_loss: 0.12490 valid_loss: 0.13644 test_loss: 0.15115 \n",
      "[ 25/300] train_loss: 0.12432 valid_loss: 0.13315 test_loss: 0.14808 \n",
      "Validation loss decreased (0.136339 --> 0.133151).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12384 valid_loss: 0.13099 test_loss: 0.14589 \n",
      "Validation loss decreased (0.133151 --> 0.130989).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11972 valid_loss: 0.12924 test_loss: 0.14673 \n",
      "Validation loss decreased (0.130989 --> 0.129241).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11986 valid_loss: 0.12983 test_loss: 0.14466 \n",
      "[ 29/300] train_loss: 0.12058 valid_loss: 0.12981 test_loss: 0.14366 \n",
      "[ 30/300] train_loss: 0.12081 valid_loss: 0.12737 test_loss: 0.14338 \n",
      "Validation loss decreased (0.129241 --> 0.127365).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11942 valid_loss: 0.12455 test_loss: 0.14273 \n",
      "Validation loss decreased (0.127365 --> 0.124552).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11659 valid_loss: 0.12384 test_loss: 0.14028 \n",
      "Validation loss decreased (0.124552 --> 0.123836).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11621 valid_loss: 0.12211 test_loss: 0.13894 \n",
      "Validation loss decreased (0.123836 --> 0.122112).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11478 valid_loss: 0.12213 test_loss: 0.14038 \n",
      "[ 35/300] train_loss: 0.11830 valid_loss: 0.12207 test_loss: 0.13874 \n",
      "Validation loss decreased (0.122112 --> 0.122073).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11535 valid_loss: 0.11891 test_loss: 0.13583 \n",
      "Validation loss decreased (0.122073 --> 0.118912).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11460 valid_loss: 0.12581 test_loss: 0.13885 \n",
      "[ 38/300] train_loss: 0.11211 valid_loss: 0.11998 test_loss: 0.13479 \n",
      "[ 39/300] train_loss: 0.11022 valid_loss: 0.11726 test_loss: 0.13462 \n",
      "Validation loss decreased (0.118912 --> 0.117260).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11194 valid_loss: 0.11790 test_loss: 0.13424 \n",
      "[ 41/300] train_loss: 0.11204 valid_loss: 0.11685 test_loss: 0.13232 \n",
      "Validation loss decreased (0.117260 --> 0.116854).  Saving model ...\n",
      "[ 42/300] train_loss: 0.11070 valid_loss: 0.12068 test_loss: 0.13570 \n",
      "[ 43/300] train_loss: 0.10671 valid_loss: 0.11572 test_loss: 0.13186 \n",
      "Validation loss decreased (0.116854 --> 0.115722).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10896 valid_loss: 0.11633 test_loss: 0.13333 \n",
      "[ 45/300] train_loss: 0.10934 valid_loss: 0.11785 test_loss: 0.13408 \n",
      "[ 46/300] train_loss: 0.10750 valid_loss: 0.11723 test_loss: 0.13257 \n",
      "[ 47/300] train_loss: 0.10533 valid_loss: 0.11551 test_loss: 0.13162 \n",
      "Validation loss decreased (0.115722 --> 0.115506).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10577 valid_loss: 0.11567 test_loss: 0.12856 \n",
      "[ 49/300] train_loss: 0.10680 valid_loss: 0.11194 test_loss: 0.12883 \n",
      "Validation loss decreased (0.115506 --> 0.111939).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10278 valid_loss: 0.11177 test_loss: 0.12752 \n",
      "Validation loss decreased (0.111939 --> 0.111769).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10245 valid_loss: 0.11360 test_loss: 0.12757 \n",
      "[ 52/300] train_loss: 0.10403 valid_loss: 0.11134 test_loss: 0.12915 \n",
      "Validation loss decreased (0.111769 --> 0.111344).  Saving model ...\n",
      "[ 53/300] train_loss: 0.10167 valid_loss: 0.11440 test_loss: 0.12835 \n",
      "[ 54/300] train_loss: 0.10308 valid_loss: 0.11039 test_loss: 0.12709 \n",
      "Validation loss decreased (0.111344 --> 0.110394).  Saving model ...\n",
      "[ 55/300] train_loss: 0.10452 valid_loss: 0.10826 test_loss: 0.12541 \n",
      "Validation loss decreased (0.110394 --> 0.108261).  Saving model ...\n",
      "[ 56/300] train_loss: 0.10087 valid_loss: 0.10915 test_loss: 0.12469 \n",
      "[ 57/300] train_loss: 0.09940 valid_loss: 0.10927 test_loss: 0.12560 \n",
      "[ 58/300] train_loss: 0.09935 valid_loss: 0.11373 test_loss: 0.12598 \n",
      "[ 59/300] train_loss: 0.10074 valid_loss: 0.10822 test_loss: 0.12285 \n",
      "Validation loss decreased (0.108261 --> 0.108224).  Saving model ...\n",
      "[ 60/300] train_loss: 0.09980 valid_loss: 0.10942 test_loss: 0.12090 \n",
      "[ 61/300] train_loss: 0.10095 valid_loss: 0.10729 test_loss: 0.12274 \n",
      "Validation loss decreased (0.108224 --> 0.107294).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09743 valid_loss: 0.10689 test_loss: 0.12252 \n",
      "Validation loss decreased (0.107294 --> 0.106891).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09640 valid_loss: 0.10856 test_loss: 0.12199 \n",
      "[ 64/300] train_loss: 0.09777 valid_loss: 0.11047 test_loss: 0.12242 \n",
      "[ 65/300] train_loss: 0.09521 valid_loss: 0.11070 test_loss: 0.12496 \n",
      "[ 66/300] train_loss: 0.09745 valid_loss: 0.10826 test_loss: 0.12105 \n",
      "[ 67/300] train_loss: 0.09808 valid_loss: 0.11880 test_loss: 0.12417 \n",
      "[ 68/300] train_loss: 0.09804 valid_loss: 0.10832 test_loss: 0.11923 \n",
      "[ 69/300] train_loss: 0.09781 valid_loss: 0.10575 test_loss: 0.11925 \n",
      "Validation loss decreased (0.106891 --> 0.105747).  Saving model ...\n",
      "[ 70/300] train_loss: 0.09467 valid_loss: 0.10725 test_loss: 0.12081 \n",
      "[ 71/300] train_loss: 0.09793 valid_loss: 0.10544 test_loss: 0.11859 \n",
      "Validation loss decreased (0.105747 --> 0.105442).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09726 valid_loss: 0.11064 test_loss: 0.11919 \n",
      "[ 73/300] train_loss: 0.09349 valid_loss: 0.10851 test_loss: 0.11967 \n",
      "[ 74/300] train_loss: 0.09569 valid_loss: 0.10548 test_loss: 0.11949 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 75/300] train_loss: 0.09591 valid_loss: 0.10404 test_loss: 0.11781 \n",
      "Validation loss decreased (0.105442 --> 0.104037).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09437 valid_loss: 0.10141 test_loss: 0.11759 \n",
      "Validation loss decreased (0.104037 --> 0.101406).  Saving model ...\n",
      "[ 77/300] train_loss: 0.09354 valid_loss: 0.10586 test_loss: 0.11602 \n",
      "[ 78/300] train_loss: 0.09317 valid_loss: 0.10706 test_loss: 0.11748 \n",
      "[ 79/300] train_loss: 0.09740 valid_loss: 0.10561 test_loss: 0.11741 \n",
      "[ 80/300] train_loss: 0.09200 valid_loss: 0.10185 test_loss: 0.11456 \n",
      "[ 81/300] train_loss: 0.09441 valid_loss: 0.10078 test_loss: 0.11485 \n",
      "Validation loss decreased (0.101406 --> 0.100780).  Saving model ...\n",
      "[ 82/300] train_loss: 0.09104 valid_loss: 0.10387 test_loss: 0.11503 \n",
      "[ 83/300] train_loss: 0.09570 valid_loss: 0.10198 test_loss: 0.11408 \n",
      "[ 84/300] train_loss: 0.09064 valid_loss: 0.10259 test_loss: 0.11530 \n",
      "[ 85/300] train_loss: 0.09054 valid_loss: 0.09941 test_loss: 0.11243 \n",
      "Validation loss decreased (0.100780 --> 0.099411).  Saving model ...\n",
      "[ 86/300] train_loss: 0.08877 valid_loss: 0.10234 test_loss: 0.11520 \n",
      "[ 87/300] train_loss: 0.09200 valid_loss: 0.10038 test_loss: 0.11189 \n",
      "[ 88/300] train_loss: 0.08955 valid_loss: 0.10099 test_loss: 0.11243 \n",
      "[ 89/300] train_loss: 0.08958 valid_loss: 0.09948 test_loss: 0.11221 \n",
      "[ 90/300] train_loss: 0.09013 valid_loss: 0.09955 test_loss: 0.11152 \n",
      "[ 91/300] train_loss: 0.09109 valid_loss: 0.09994 test_loss: 0.11261 \n",
      "[ 92/300] train_loss: 0.08774 valid_loss: 0.10191 test_loss: 0.11019 \n",
      "[ 93/300] train_loss: 0.09089 valid_loss: 0.10106 test_loss: 0.11326 \n",
      "[ 94/300] train_loss: 0.08976 valid_loss: 0.09811 test_loss: 0.10964 \n",
      "Validation loss decreased (0.099411 --> 0.098105).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08983 valid_loss: 0.10355 test_loss: 0.11075 \n",
      "[ 96/300] train_loss: 0.08866 valid_loss: 0.10088 test_loss: 0.11161 \n",
      "[ 97/300] train_loss: 0.08842 valid_loss: 0.09934 test_loss: 0.11122 \n",
      "[ 98/300] train_loss: 0.08764 valid_loss: 0.09753 test_loss: 0.11049 \n",
      "Validation loss decreased (0.098105 --> 0.097526).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08666 valid_loss: 0.09747 test_loss: 0.10896 \n",
      "Validation loss decreased (0.097526 --> 0.097472).  Saving model ...\n",
      "[100/300] train_loss: 0.08666 valid_loss: 0.09813 test_loss: 0.10861 \n",
      "[101/300] train_loss: 0.08824 valid_loss: 0.09596 test_loss: 0.10938 \n",
      "Validation loss decreased (0.097472 --> 0.095961).  Saving model ...\n",
      "[102/300] train_loss: 0.08831 valid_loss: 0.09726 test_loss: 0.10934 \n",
      "[103/300] train_loss: 0.08561 valid_loss: 0.09802 test_loss: 0.10833 \n",
      "[104/300] train_loss: 0.08733 valid_loss: 0.09832 test_loss: 0.10863 \n",
      "[105/300] train_loss: 0.08729 valid_loss: 0.09735 test_loss: 0.10937 \n",
      "[106/300] train_loss: 0.08689 valid_loss: 0.09513 test_loss: 0.10863 \n",
      "Validation loss decreased (0.095961 --> 0.095134).  Saving model ...\n",
      "[107/300] train_loss: 0.08641 valid_loss: 0.09535 test_loss: 0.10904 \n",
      "[108/300] train_loss: 0.08679 valid_loss: 0.09564 test_loss: 0.10739 \n",
      "[109/300] train_loss: 0.08576 valid_loss: 0.09487 test_loss: 0.10653 \n",
      "Validation loss decreased (0.095134 --> 0.094872).  Saving model ...\n",
      "[110/300] train_loss: 0.08554 valid_loss: 0.09663 test_loss: 0.10897 \n",
      "[111/300] train_loss: 0.08435 valid_loss: 0.09653 test_loss: 0.10748 \n",
      "[112/300] train_loss: 0.08442 valid_loss: 0.09955 test_loss: 0.10647 \n",
      "[113/300] train_loss: 0.08550 valid_loss: 0.09516 test_loss: 0.10677 \n",
      "[114/300] train_loss: 0.08748 valid_loss: 0.09670 test_loss: 0.10703 \n",
      "[115/300] train_loss: 0.08324 valid_loss: 0.09525 test_loss: 0.10553 \n",
      "[116/300] train_loss: 0.08434 valid_loss: 0.09232 test_loss: 0.10605 \n",
      "Validation loss decreased (0.094872 --> 0.092319).  Saving model ...\n",
      "[117/300] train_loss: 0.08518 valid_loss: 0.09328 test_loss: 0.10553 \n",
      "[118/300] train_loss: 0.08457 valid_loss: 0.09414 test_loss: 0.10486 \n",
      "[119/300] train_loss: 0.08512 valid_loss: 0.09454 test_loss: 0.10577 \n",
      "[120/300] train_loss: 0.08112 valid_loss: 0.09155 test_loss: 0.10462 \n",
      "Validation loss decreased (0.092319 --> 0.091549).  Saving model ...\n",
      "[121/300] train_loss: 0.08450 valid_loss: 0.09373 test_loss: 0.10576 \n",
      "[122/300] train_loss: 0.07909 valid_loss: 0.09558 test_loss: 0.10663 \n",
      "[123/300] train_loss: 0.08612 valid_loss: 0.09330 test_loss: 0.10615 \n",
      "[124/300] train_loss: 0.08296 valid_loss: 0.09375 test_loss: 0.10620 \n",
      "[125/300] train_loss: 0.08271 valid_loss: 0.09273 test_loss: 0.10568 \n",
      "[126/300] train_loss: 0.08451 valid_loss: 0.09344 test_loss: 0.10493 \n",
      "[127/300] train_loss: 0.08524 valid_loss: 0.09425 test_loss: 0.10545 \n",
      "[128/300] train_loss: 0.08124 valid_loss: 0.09236 test_loss: 0.10434 \n",
      "[129/300] train_loss: 0.08165 valid_loss: 0.09271 test_loss: 0.10302 \n",
      "[130/300] train_loss: 0.07897 valid_loss: 0.09345 test_loss: 0.10411 \n",
      "[131/300] train_loss: 0.08297 valid_loss: 0.09520 test_loss: 0.10407 \n",
      "[132/300] train_loss: 0.08076 valid_loss: 0.09291 test_loss: 0.10547 \n",
      "[133/300] train_loss: 0.08209 valid_loss: 0.09405 test_loss: 0.10364 \n",
      "[134/300] train_loss: 0.08099 valid_loss: 0.09375 test_loss: 0.10588 \n",
      "[135/300] train_loss: 0.07893 valid_loss: 0.09276 test_loss: 0.10351 \n",
      "[136/300] train_loss: 0.08104 valid_loss: 0.09150 test_loss: 0.10259 \n",
      "Validation loss decreased (0.091549 --> 0.091499).  Saving model ...\n",
      "[137/300] train_loss: 0.08078 valid_loss: 0.09083 test_loss: 0.10387 \n",
      "Validation loss decreased (0.091499 --> 0.090827).  Saving model ...\n",
      "[138/300] train_loss: 0.08205 valid_loss: 0.09045 test_loss: 0.10237 \n",
      "Validation loss decreased (0.090827 --> 0.090449).  Saving model ...\n",
      "[139/300] train_loss: 0.07928 valid_loss: 0.09192 test_loss: 0.10211 \n",
      "[140/300] train_loss: 0.08251 valid_loss: 0.09139 test_loss: 0.10201 \n",
      "[141/300] train_loss: 0.08064 valid_loss: 0.09102 test_loss: 0.10314 \n",
      "[142/300] train_loss: 0.08042 valid_loss: 0.09246 test_loss: 0.10191 \n",
      "[143/300] train_loss: 0.08113 valid_loss: 0.08977 test_loss: 0.10342 \n",
      "Validation loss decreased (0.090449 --> 0.089770).  Saving model ...\n",
      "[144/300] train_loss: 0.08291 valid_loss: 0.09220 test_loss: 0.10310 \n",
      "[145/300] train_loss: 0.08057 valid_loss: 0.09165 test_loss: 0.10224 \n",
      "[146/300] train_loss: 0.07994 valid_loss: 0.08891 test_loss: 0.10021 \n",
      "Validation loss decreased (0.089770 --> 0.088906).  Saving model ...\n",
      "[147/300] train_loss: 0.08226 valid_loss: 0.09120 test_loss: 0.10034 \n",
      "[148/300] train_loss: 0.07832 valid_loss: 0.09088 test_loss: 0.10248 \n",
      "[149/300] train_loss: 0.07970 valid_loss: 0.08981 test_loss: 0.10247 \n",
      "[150/300] train_loss: 0.07762 valid_loss: 0.08911 test_loss: 0.10176 \n",
      "[151/300] train_loss: 0.07863 valid_loss: 0.08986 test_loss: 0.10182 \n",
      "[152/300] train_loss: 0.07842 valid_loss: 0.09123 test_loss: 0.10228 \n",
      "[153/300] train_loss: 0.07770 valid_loss: 0.08890 test_loss: 0.10160 \n",
      "Validation loss decreased (0.088906 --> 0.088903).  Saving model ...\n",
      "[154/300] train_loss: 0.08006 valid_loss: 0.09019 test_loss: 0.10125 \n",
      "[155/300] train_loss: 0.07901 valid_loss: 0.08928 test_loss: 0.10175 \n",
      "[156/300] train_loss: 0.08073 valid_loss: 0.08856 test_loss: 0.10070 \n",
      "Validation loss decreased (0.088903 --> 0.088564).  Saving model ...\n",
      "[157/300] train_loss: 0.07755 valid_loss: 0.08734 test_loss: 0.09993 \n",
      "Validation loss decreased (0.088564 --> 0.087340).  Saving model ...\n",
      "[158/300] train_loss: 0.07707 valid_loss: 0.08873 test_loss: 0.10108 \n",
      "[159/300] train_loss: 0.07977 valid_loss: 0.08982 test_loss: 0.10263 \n",
      "[160/300] train_loss: 0.07667 valid_loss: 0.09009 test_loss: 0.10301 \n",
      "[161/300] train_loss: 0.07973 valid_loss: 0.08956 test_loss: 0.10104 \n",
      "[162/300] train_loss: 0.07650 valid_loss: 0.08938 test_loss: 0.10093 \n",
      "[163/300] train_loss: 0.08007 valid_loss: 0.08942 test_loss: 0.10149 \n",
      "[164/300] train_loss: 0.07687 valid_loss: 0.08687 test_loss: 0.09889 \n",
      "Validation loss decreased (0.087340 --> 0.086870).  Saving model ...\n",
      "[165/300] train_loss: 0.07759 valid_loss: 0.09010 test_loss: 0.09959 \n",
      "[166/300] train_loss: 0.07740 valid_loss: 0.09011 test_loss: 0.10011 \n",
      "[167/300] train_loss: 0.07812 valid_loss: 0.08772 test_loss: 0.09984 \n",
      "[168/300] train_loss: 0.07949 valid_loss: 0.08771 test_loss: 0.09902 \n",
      "[169/300] train_loss: 0.07734 valid_loss: 0.09090 test_loss: 0.10062 \n",
      "[170/300] train_loss: 0.07730 valid_loss: 0.08778 test_loss: 0.10061 \n",
      "[171/300] train_loss: 0.07723 valid_loss: 0.08785 test_loss: 0.10024 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172/300] train_loss: 0.07785 valid_loss: 0.08936 test_loss: 0.09952 \n",
      "[173/300] train_loss: 0.07778 valid_loss: 0.09107 test_loss: 0.09924 \n",
      "[174/300] train_loss: 0.07649 valid_loss: 0.08701 test_loss: 0.09899 \n",
      "[175/300] train_loss: 0.07514 valid_loss: 0.08930 test_loss: 0.09955 \n",
      "[176/300] train_loss: 0.07641 valid_loss: 0.08782 test_loss: 0.09843 \n",
      "[177/300] train_loss: 0.07519 valid_loss: 0.08773 test_loss: 0.10112 \n",
      "[178/300] train_loss: 0.07922 valid_loss: 0.08699 test_loss: 0.09991 \n",
      "[179/300] train_loss: 0.07938 valid_loss: 0.08535 test_loss: 0.09850 \n",
      "Validation loss decreased (0.086870 --> 0.085351).  Saving model ...\n",
      "[180/300] train_loss: 0.07785 valid_loss: 0.08740 test_loss: 0.10034 \n",
      "[181/300] train_loss: 0.07766 valid_loss: 0.08666 test_loss: 0.09939 \n",
      "[182/300] train_loss: 0.07640 valid_loss: 0.08459 test_loss: 0.09786 \n",
      "Validation loss decreased (0.085351 --> 0.084592).  Saving model ...\n",
      "[183/300] train_loss: 0.07843 valid_loss: 0.08616 test_loss: 0.09783 \n",
      "[184/300] train_loss: 0.07314 valid_loss: 0.08609 test_loss: 0.09797 \n",
      "[185/300] train_loss: 0.07530 valid_loss: 0.08750 test_loss: 0.09978 \n",
      "[186/300] train_loss: 0.07528 valid_loss: 0.08672 test_loss: 0.09892 \n",
      "[187/300] train_loss: 0.07299 valid_loss: 0.08822 test_loss: 0.09885 \n",
      "[188/300] train_loss: 0.07504 valid_loss: 0.08632 test_loss: 0.10031 \n",
      "[189/300] train_loss: 0.07687 valid_loss: 0.08635 test_loss: 0.09868 \n",
      "[190/300] train_loss: 0.07730 valid_loss: 0.08568 test_loss: 0.09879 \n",
      "[191/300] train_loss: 0.07316 valid_loss: 0.08566 test_loss: 0.09756 \n",
      "[192/300] train_loss: 0.07495 valid_loss: 0.08517 test_loss: 0.09777 \n",
      "[193/300] train_loss: 0.07588 valid_loss: 0.08461 test_loss: 0.09733 \n",
      "[194/300] train_loss: 0.07302 valid_loss: 0.08527 test_loss: 0.09738 \n",
      "[195/300] train_loss: 0.07429 valid_loss: 0.08494 test_loss: 0.09812 \n",
      "[196/300] train_loss: 0.07522 valid_loss: 0.08803 test_loss: 0.09725 \n",
      "[197/300] train_loss: 0.07289 valid_loss: 0.08563 test_loss: 0.09682 \n",
      "[198/300] train_loss: 0.07350 valid_loss: 0.08419 test_loss: 0.09703 \n",
      "Validation loss decreased (0.084592 --> 0.084190).  Saving model ...\n",
      "[199/300] train_loss: 0.07413 valid_loss: 0.08569 test_loss: 0.09844 \n",
      "[200/300] train_loss: 0.07400 valid_loss: 0.08307 test_loss: 0.09584 \n",
      "Validation loss decreased (0.084190 --> 0.083066).  Saving model ...\n",
      "[201/300] train_loss: 0.07540 valid_loss: 0.08457 test_loss: 0.09651 \n",
      "[202/300] train_loss: 0.07222 valid_loss: 0.08562 test_loss: 0.09803 \n",
      "[203/300] train_loss: 0.07396 valid_loss: 0.08417 test_loss: 0.09753 \n",
      "[204/300] train_loss: 0.07190 valid_loss: 0.08715 test_loss: 0.09833 \n",
      "[205/300] train_loss: 0.07389 valid_loss: 0.08392 test_loss: 0.09697 \n",
      "[206/300] train_loss: 0.07439 valid_loss: 0.08487 test_loss: 0.09724 \n",
      "[207/300] train_loss: 0.07431 valid_loss: 0.08467 test_loss: 0.09688 \n",
      "[208/300] train_loss: 0.07510 valid_loss: 0.08464 test_loss: 0.09646 \n",
      "[209/300] train_loss: 0.07189 valid_loss: 0.08367 test_loss: 0.09663 \n",
      "[210/300] train_loss: 0.07274 valid_loss: 0.08492 test_loss: 0.09720 \n",
      "[211/300] train_loss: 0.07207 valid_loss: 0.08456 test_loss: 0.09596 \n",
      "[212/300] train_loss: 0.07182 valid_loss: 0.08620 test_loss: 0.09662 \n",
      "[213/300] train_loss: 0.07017 valid_loss: 0.08318 test_loss: 0.09626 \n",
      "[214/300] train_loss: 0.07185 valid_loss: 0.08274 test_loss: 0.09518 \n",
      "Validation loss decreased (0.083066 --> 0.082737).  Saving model ...\n",
      "[215/300] train_loss: 0.07371 valid_loss: 0.08587 test_loss: 0.09535 \n",
      "[216/300] train_loss: 0.07243 valid_loss: 0.08368 test_loss: 0.09656 \n",
      "[217/300] train_loss: 0.07358 valid_loss: 0.08407 test_loss: 0.09458 \n",
      "[218/300] train_loss: 0.07454 valid_loss: 0.08341 test_loss: 0.09706 \n",
      "[219/300] train_loss: 0.07307 valid_loss: 0.08413 test_loss: 0.09697 \n",
      "[220/300] train_loss: 0.07153 valid_loss: 0.08337 test_loss: 0.09583 \n",
      "[221/300] train_loss: 0.07406 valid_loss: 0.08384 test_loss: 0.09683 \n",
      "[222/300] train_loss: 0.07172 valid_loss: 0.08239 test_loss: 0.09572 \n",
      "Validation loss decreased (0.082737 --> 0.082387).  Saving model ...\n",
      "[223/300] train_loss: 0.07310 valid_loss: 0.08432 test_loss: 0.09537 \n",
      "[224/300] train_loss: 0.07171 valid_loss: 0.08350 test_loss: 0.09563 \n",
      "[225/300] train_loss: 0.07268 valid_loss: 0.08401 test_loss: 0.09481 \n",
      "[226/300] train_loss: 0.07022 valid_loss: 0.08488 test_loss: 0.09555 \n",
      "[227/300] train_loss: 0.07004 valid_loss: 0.08210 test_loss: 0.09470 \n",
      "Validation loss decreased (0.082387 --> 0.082099).  Saving model ...\n",
      "[228/300] train_loss: 0.07066 valid_loss: 0.08299 test_loss: 0.09567 \n",
      "[229/300] train_loss: 0.07064 valid_loss: 0.08685 test_loss: 0.09497 \n",
      "[230/300] train_loss: 0.07287 valid_loss: 0.08157 test_loss: 0.09481 \n",
      "Validation loss decreased (0.082099 --> 0.081574).  Saving model ...\n",
      "[231/300] train_loss: 0.07324 valid_loss: 0.08153 test_loss: 0.09409 \n",
      "Validation loss decreased (0.081574 --> 0.081526).  Saving model ...\n",
      "[232/300] train_loss: 0.06819 valid_loss: 0.08354 test_loss: 0.09579 \n",
      "[233/300] train_loss: 0.06984 valid_loss: 0.08380 test_loss: 0.09487 \n",
      "[234/300] train_loss: 0.07233 valid_loss: 0.08501 test_loss: 0.09514 \n",
      "[235/300] train_loss: 0.06970 valid_loss: 0.08587 test_loss: 0.09485 \n",
      "[236/300] train_loss: 0.07062 valid_loss: 0.08416 test_loss: 0.09470 \n",
      "[237/300] train_loss: 0.07115 valid_loss: 0.08254 test_loss: 0.09441 \n",
      "[238/300] train_loss: 0.07232 valid_loss: 0.08059 test_loss: 0.09353 \n",
      "Validation loss decreased (0.081526 --> 0.080593).  Saving model ...\n",
      "[239/300] train_loss: 0.07492 valid_loss: 0.08379 test_loss: 0.09561 \n",
      "[240/300] train_loss: 0.07067 valid_loss: 0.08095 test_loss: 0.09559 \n",
      "[241/300] train_loss: 0.07266 valid_loss: 0.08102 test_loss: 0.09372 \n",
      "[242/300] train_loss: 0.07062 valid_loss: 0.08127 test_loss: 0.09373 \n",
      "[243/300] train_loss: 0.07099 valid_loss: 0.08092 test_loss: 0.09358 \n",
      "[244/300] train_loss: 0.07154 valid_loss: 0.08256 test_loss: 0.09525 \n",
      "[245/300] train_loss: 0.07122 valid_loss: 0.08154 test_loss: 0.09496 \n",
      "[246/300] train_loss: 0.06966 valid_loss: 0.08172 test_loss: 0.09413 \n",
      "[247/300] train_loss: 0.07051 valid_loss: 0.08098 test_loss: 0.09265 \n",
      "[248/300] train_loss: 0.07044 valid_loss: 0.08213 test_loss: 0.09547 \n",
      "[249/300] train_loss: 0.07064 valid_loss: 0.08063 test_loss: 0.09298 \n",
      "[250/300] train_loss: 0.07124 valid_loss: 0.08245 test_loss: 0.09459 \n",
      "[251/300] train_loss: 0.07024 valid_loss: 0.08141 test_loss: 0.09425 \n",
      "[252/300] train_loss: 0.06931 valid_loss: 0.08071 test_loss: 0.09374 \n",
      "[253/300] train_loss: 0.07073 valid_loss: 0.07929 test_loss: 0.09262 \n",
      "Validation loss decreased (0.080593 --> 0.079295).  Saving model ...\n",
      "[254/300] train_loss: 0.06987 valid_loss: 0.08052 test_loss: 0.09254 \n",
      "[255/300] train_loss: 0.06860 valid_loss: 0.07976 test_loss: 0.09341 \n",
      "[256/300] train_loss: 0.06816 valid_loss: 0.08040 test_loss: 0.09310 \n",
      "[257/300] train_loss: 0.06949 valid_loss: 0.08177 test_loss: 0.09371 \n",
      "[258/300] train_loss: 0.06687 valid_loss: 0.08179 test_loss: 0.09354 \n",
      "[259/300] train_loss: 0.07151 valid_loss: 0.08012 test_loss: 0.09251 \n",
      "[260/300] train_loss: 0.07002 valid_loss: 0.08155 test_loss: 0.09396 \n",
      "[261/300] train_loss: 0.07078 valid_loss: 0.08040 test_loss: 0.09302 \n",
      "[262/300] train_loss: 0.06765 valid_loss: 0.08126 test_loss: 0.09238 \n",
      "[263/300] train_loss: 0.06977 valid_loss: 0.08188 test_loss: 0.09332 \n",
      "[264/300] train_loss: 0.06844 valid_loss: 0.08246 test_loss: 0.09417 \n",
      "[265/300] train_loss: 0.07112 valid_loss: 0.08147 test_loss: 0.09309 \n",
      "[266/300] train_loss: 0.06924 valid_loss: 0.08197 test_loss: 0.09389 \n",
      "[267/300] train_loss: 0.06964 valid_loss: 0.08000 test_loss: 0.09247 \n",
      "[268/300] train_loss: 0.06941 valid_loss: 0.08074 test_loss: 0.09293 \n",
      "[269/300] train_loss: 0.06754 valid_loss: 0.07988 test_loss: 0.09182 \n",
      "[270/300] train_loss: 0.06965 valid_loss: 0.07959 test_loss: 0.09208 \n",
      "[271/300] train_loss: 0.06978 valid_loss: 0.07933 test_loss: 0.09149 \n",
      "[272/300] train_loss: 0.06842 valid_loss: 0.08074 test_loss: 0.09230 \n",
      "[273/300] train_loss: 0.06772 valid_loss: 0.07931 test_loss: 0.09174 \n",
      "[274/300] train_loss: 0.07027 valid_loss: 0.08084 test_loss: 0.09251 \n",
      "[275/300] train_loss: 0.06783 valid_loss: 0.08008 test_loss: 0.09265 \n",
      "[276/300] train_loss: 0.06983 valid_loss: 0.07916 test_loss: 0.09178 \n",
      "Validation loss decreased (0.079295 --> 0.079157).  Saving model ...\n",
      "[277/300] train_loss: 0.06962 valid_loss: 0.08151 test_loss: 0.09348 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[278/300] train_loss: 0.06671 valid_loss: 0.07970 test_loss: 0.09092 \n",
      "[279/300] train_loss: 0.06871 valid_loss: 0.08050 test_loss: 0.09332 \n",
      "[280/300] train_loss: 0.06946 valid_loss: 0.09356 test_loss: 0.09295 \n",
      "[281/300] train_loss: 0.06904 valid_loss: 0.07981 test_loss: 0.09225 \n",
      "[282/300] train_loss: 0.06703 valid_loss: 0.08041 test_loss: 0.09207 \n",
      "[283/300] train_loss: 0.06602 valid_loss: 0.08100 test_loss: 0.09199 \n",
      "[284/300] train_loss: 0.06686 valid_loss: 0.08037 test_loss: 0.09206 \n",
      "[285/300] train_loss: 0.07231 valid_loss: 0.07975 test_loss: 0.09372 \n",
      "[286/300] train_loss: 0.06855 valid_loss: 0.07879 test_loss: 0.09109 \n",
      "Validation loss decreased (0.079157 --> 0.078792).  Saving model ...\n",
      "[287/300] train_loss: 0.06886 valid_loss: 0.07887 test_loss: 0.09142 \n",
      "[288/300] train_loss: 0.06695 valid_loss: 0.07975 test_loss: 0.09173 \n",
      "[289/300] train_loss: 0.06759 valid_loss: 0.07884 test_loss: 0.09149 \n",
      "[290/300] train_loss: 0.06670 valid_loss: 0.07931 test_loss: 0.09143 \n",
      "[291/300] train_loss: 0.06664 valid_loss: 0.07970 test_loss: 0.09111 \n",
      "[292/300] train_loss: 0.06660 valid_loss: 0.08028 test_loss: 0.09159 \n",
      "[293/300] train_loss: 0.06914 valid_loss: 0.07845 test_loss: 0.08998 \n",
      "Validation loss decreased (0.078792 --> 0.078452).  Saving model ...\n",
      "[294/300] train_loss: 0.06688 valid_loss: 0.07936 test_loss: 0.09099 \n",
      "[295/300] train_loss: 0.06611 valid_loss: 0.07946 test_loss: 0.09137 \n",
      "[296/300] train_loss: 0.06742 valid_loss: 0.07829 test_loss: 0.08979 \n",
      "Validation loss decreased (0.078452 --> 0.078286).  Saving model ...\n",
      "[297/300] train_loss: 0.06573 valid_loss: 0.07973 test_loss: 0.09087 \n",
      "[298/300] train_loss: 0.06586 valid_loss: 0.08012 test_loss: 0.09132 \n",
      "[299/300] train_loss: 0.06413 valid_loss: 0.07848 test_loss: 0.09089 \n",
      "[300/300] train_loss: 0.06661 valid_loss: 0.07863 test_loss: 0.09085 \n",
      "TRAINING MODEL 1\n",
      "[  1/300] train_loss: 0.60820 valid_loss: 0.51617 test_loss: 0.50988 \n",
      "Validation loss decreased (inf --> 0.516174).  Saving model ...\n",
      "[  2/300] train_loss: 0.42458 valid_loss: 0.38716 test_loss: 0.38577 \n",
      "Validation loss decreased (0.516174 --> 0.387157).  Saving model ...\n",
      "[  3/300] train_loss: 0.34096 valid_loss: 0.33549 test_loss: 0.34031 \n",
      "Validation loss decreased (0.387157 --> 0.335491).  Saving model ...\n",
      "[  4/300] train_loss: 0.29862 valid_loss: 0.30034 test_loss: 0.31317 \n",
      "Validation loss decreased (0.335491 --> 0.300335).  Saving model ...\n",
      "[  5/300] train_loss: 0.26626 valid_loss: 0.27291 test_loss: 0.28911 \n",
      "Validation loss decreased (0.300335 --> 0.272915).  Saving model ...\n",
      "[  6/300] train_loss: 0.23935 valid_loss: 0.24173 test_loss: 0.26051 \n",
      "Validation loss decreased (0.272915 --> 0.241733).  Saving model ...\n",
      "[  7/300] train_loss: 0.21467 valid_loss: 0.22528 test_loss: 0.24244 \n",
      "Validation loss decreased (0.241733 --> 0.225276).  Saving model ...\n",
      "[  8/300] train_loss: 0.20039 valid_loss: 0.20571 test_loss: 0.22093 \n",
      "Validation loss decreased (0.225276 --> 0.205707).  Saving model ...\n",
      "[  9/300] train_loss: 0.18919 valid_loss: 0.19368 test_loss: 0.20783 \n",
      "Validation loss decreased (0.205707 --> 0.193684).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17840 valid_loss: 0.18481 test_loss: 0.19755 \n",
      "Validation loss decreased (0.193684 --> 0.184814).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16979 valid_loss: 0.17496 test_loss: 0.18968 \n",
      "Validation loss decreased (0.184814 --> 0.174961).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16032 valid_loss: 0.17109 test_loss: 0.18315 \n",
      "Validation loss decreased (0.174961 --> 0.171090).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15877 valid_loss: 0.16567 test_loss: 0.17667 \n",
      "Validation loss decreased (0.171090 --> 0.165670).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15062 valid_loss: 0.16314 test_loss: 0.17314 \n",
      "Validation loss decreased (0.165670 --> 0.163142).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14784 valid_loss: 0.15871 test_loss: 0.17070 \n",
      "Validation loss decreased (0.163142 --> 0.158705).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14436 valid_loss: 0.15355 test_loss: 0.16579 \n",
      "Validation loss decreased (0.158705 --> 0.153546).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14227 valid_loss: 0.15253 test_loss: 0.16343 \n",
      "Validation loss decreased (0.153546 --> 0.152529).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13857 valid_loss: 0.14877 test_loss: 0.16115 \n",
      "Validation loss decreased (0.152529 --> 0.148768).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13660 valid_loss: 0.14532 test_loss: 0.16058 \n",
      "Validation loss decreased (0.148768 --> 0.145323).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13661 valid_loss: 0.14367 test_loss: 0.15648 \n",
      "Validation loss decreased (0.145323 --> 0.143673).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13484 valid_loss: 0.14325 test_loss: 0.15557 \n",
      "Validation loss decreased (0.143673 --> 0.143247).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13296 valid_loss: 0.14044 test_loss: 0.15335 \n",
      "Validation loss decreased (0.143247 --> 0.140438).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12514 valid_loss: 0.13726 test_loss: 0.15194 \n",
      "Validation loss decreased (0.140438 --> 0.137258).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12690 valid_loss: 0.13722 test_loss: 0.14950 \n",
      "Validation loss decreased (0.137258 --> 0.137225).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12463 valid_loss: 0.13261 test_loss: 0.14876 \n",
      "Validation loss decreased (0.137225 --> 0.132608).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12643 valid_loss: 0.12933 test_loss: 0.14636 \n",
      "Validation loss decreased (0.132608 --> 0.129327).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12548 valid_loss: 0.13242 test_loss: 0.14751 \n",
      "[ 28/300] train_loss: 0.12327 valid_loss: 0.13132 test_loss: 0.14680 \n",
      "[ 29/300] train_loss: 0.11751 valid_loss: 0.12906 test_loss: 0.14561 \n",
      "Validation loss decreased (0.129327 --> 0.129057).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11997 valid_loss: 0.13382 test_loss: 0.14508 \n",
      "[ 31/300] train_loss: 0.11847 valid_loss: 0.12721 test_loss: 0.14271 \n",
      "Validation loss decreased (0.129057 --> 0.127209).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11805 valid_loss: 0.12537 test_loss: 0.14219 \n",
      "Validation loss decreased (0.127209 --> 0.125369).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11790 valid_loss: 0.12594 test_loss: 0.13991 \n",
      "[ 34/300] train_loss: 0.11432 valid_loss: 0.12297 test_loss: 0.14014 \n",
      "Validation loss decreased (0.125369 --> 0.122966).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11305 valid_loss: 0.12569 test_loss: 0.14048 \n",
      "[ 36/300] train_loss: 0.11262 valid_loss: 0.12540 test_loss: 0.13890 \n",
      "[ 37/300] train_loss: 0.11169 valid_loss: 0.12170 test_loss: 0.13668 \n",
      "Validation loss decreased (0.122966 --> 0.121701).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11171 valid_loss: 0.11901 test_loss: 0.13628 \n",
      "Validation loss decreased (0.121701 --> 0.119007).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11075 valid_loss: 0.11794 test_loss: 0.13397 \n",
      "Validation loss decreased (0.119007 --> 0.117941).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11184 valid_loss: 0.11955 test_loss: 0.13599 \n",
      "[ 41/300] train_loss: 0.11176 valid_loss: 0.11519 test_loss: 0.13262 \n",
      "Validation loss decreased (0.117941 --> 0.115186).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10907 valid_loss: 0.11887 test_loss: 0.13469 \n",
      "[ 43/300] train_loss: 0.11417 valid_loss: 0.11699 test_loss: 0.13470 \n",
      "[ 44/300] train_loss: 0.10906 valid_loss: 0.11628 test_loss: 0.13317 \n",
      "[ 45/300] train_loss: 0.10630 valid_loss: 0.12030 test_loss: 0.13356 \n",
      "[ 46/300] train_loss: 0.10789 valid_loss: 0.11336 test_loss: 0.13126 \n",
      "Validation loss decreased (0.115186 --> 0.113362).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10481 valid_loss: 0.11158 test_loss: 0.12841 \n",
      "Validation loss decreased (0.113362 --> 0.111576).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10793 valid_loss: 0.11346 test_loss: 0.12905 \n",
      "[ 49/300] train_loss: 0.10455 valid_loss: 0.11011 test_loss: 0.12633 \n",
      "Validation loss decreased (0.111576 --> 0.110113).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10701 valid_loss: 0.11298 test_loss: 0.12820 \n",
      "[ 51/300] train_loss: 0.10427 valid_loss: 0.11318 test_loss: 0.12803 \n",
      "[ 52/300] train_loss: 0.10238 valid_loss: 0.11098 test_loss: 0.12657 \n",
      "[ 53/300] train_loss: 0.10399 valid_loss: 0.10738 test_loss: 0.12411 \n",
      "Validation loss decreased (0.110113 --> 0.107379).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10082 valid_loss: 0.10847 test_loss: 0.12501 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 55/300] train_loss: 0.10155 valid_loss: 0.10984 test_loss: 0.12669 \n",
      "[ 56/300] train_loss: 0.10202 valid_loss: 0.10695 test_loss: 0.12287 \n",
      "Validation loss decreased (0.107379 --> 0.106954).  Saving model ...\n",
      "[ 57/300] train_loss: 0.09985 valid_loss: 0.10924 test_loss: 0.12341 \n",
      "[ 58/300] train_loss: 0.10065 valid_loss: 0.10825 test_loss: 0.12217 \n",
      "[ 59/300] train_loss: 0.10021 valid_loss: 0.10799 test_loss: 0.12313 \n",
      "[ 60/300] train_loss: 0.10186 valid_loss: 0.10884 test_loss: 0.12303 \n",
      "[ 61/300] train_loss: 0.10035 valid_loss: 0.10383 test_loss: 0.12029 \n",
      "Validation loss decreased (0.106954 --> 0.103833).  Saving model ...\n",
      "[ 62/300] train_loss: 0.10067 valid_loss: 0.10503 test_loss: 0.12032 \n",
      "[ 63/300] train_loss: 0.09864 valid_loss: 0.10576 test_loss: 0.11933 \n",
      "[ 64/300] train_loss: 0.09821 valid_loss: 0.10660 test_loss: 0.12037 \n",
      "[ 65/300] train_loss: 0.09642 valid_loss: 0.10695 test_loss: 0.11950 \n",
      "[ 66/300] train_loss: 0.09788 valid_loss: 0.10486 test_loss: 0.11930 \n",
      "[ 67/300] train_loss: 0.09564 valid_loss: 0.10624 test_loss: 0.12047 \n",
      "[ 68/300] train_loss: 0.09822 valid_loss: 0.10338 test_loss: 0.11855 \n",
      "Validation loss decreased (0.103833 --> 0.103378).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09882 valid_loss: 0.10427 test_loss: 0.11852 \n",
      "[ 70/300] train_loss: 0.09608 valid_loss: 0.10664 test_loss: 0.11889 \n",
      "[ 71/300] train_loss: 0.09326 valid_loss: 0.10377 test_loss: 0.11807 \n",
      "[ 72/300] train_loss: 0.09954 valid_loss: 0.10230 test_loss: 0.11732 \n",
      "Validation loss decreased (0.103378 --> 0.102304).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09458 valid_loss: 0.10249 test_loss: 0.11753 \n",
      "[ 74/300] train_loss: 0.09350 valid_loss: 0.10222 test_loss: 0.11678 \n",
      "Validation loss decreased (0.102304 --> 0.102223).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09417 valid_loss: 0.10334 test_loss: 0.11681 \n",
      "[ 76/300] train_loss: 0.09390 valid_loss: 0.10014 test_loss: 0.11441 \n",
      "Validation loss decreased (0.102223 --> 0.100142).  Saving model ...\n",
      "[ 77/300] train_loss: 0.09624 valid_loss: 0.10204 test_loss: 0.11491 \n",
      "[ 78/300] train_loss: 0.09447 valid_loss: 0.10010 test_loss: 0.11541 \n",
      "Validation loss decreased (0.100142 --> 0.100098).  Saving model ...\n",
      "[ 79/300] train_loss: 0.09182 valid_loss: 0.10046 test_loss: 0.11585 \n",
      "[ 80/300] train_loss: 0.09324 valid_loss: 0.09911 test_loss: 0.11423 \n",
      "Validation loss decreased (0.100098 --> 0.099106).  Saving model ...\n",
      "[ 81/300] train_loss: 0.09322 valid_loss: 0.09797 test_loss: 0.11273 \n",
      "Validation loss decreased (0.099106 --> 0.097972).  Saving model ...\n",
      "[ 82/300] train_loss: 0.09293 valid_loss: 0.10479 test_loss: 0.11424 \n",
      "[ 83/300] train_loss: 0.09038 valid_loss: 0.09942 test_loss: 0.11455 \n",
      "[ 84/300] train_loss: 0.09124 valid_loss: 0.09971 test_loss: 0.11434 \n",
      "[ 85/300] train_loss: 0.09331 valid_loss: 0.09715 test_loss: 0.11348 \n",
      "Validation loss decreased (0.097972 --> 0.097153).  Saving model ...\n",
      "[ 86/300] train_loss: 0.09002 valid_loss: 0.10120 test_loss: 0.11233 \n",
      "[ 87/300] train_loss: 0.09269 valid_loss: 0.09935 test_loss: 0.11449 \n",
      "[ 88/300] train_loss: 0.08851 valid_loss: 0.09921 test_loss: 0.11223 \n",
      "[ 89/300] train_loss: 0.09159 valid_loss: 0.09915 test_loss: 0.11131 \n",
      "[ 90/300] train_loss: 0.09097 valid_loss: 0.09807 test_loss: 0.10973 \n",
      "[ 91/300] train_loss: 0.08952 valid_loss: 0.09967 test_loss: 0.11281 \n",
      "[ 92/300] train_loss: 0.09266 valid_loss: 0.10110 test_loss: 0.11094 \n",
      "[ 93/300] train_loss: 0.09080 valid_loss: 0.09761 test_loss: 0.11084 \n",
      "[ 94/300] train_loss: 0.09134 valid_loss: 0.09666 test_loss: 0.11039 \n",
      "Validation loss decreased (0.097153 --> 0.096658).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08858 valid_loss: 0.09707 test_loss: 0.11035 \n",
      "[ 96/300] train_loss: 0.08670 valid_loss: 0.09713 test_loss: 0.11039 \n",
      "[ 97/300] train_loss: 0.08771 valid_loss: 0.09589 test_loss: 0.10899 \n",
      "Validation loss decreased (0.096658 --> 0.095888).  Saving model ...\n",
      "[ 98/300] train_loss: 0.08941 valid_loss: 0.09555 test_loss: 0.10930 \n",
      "Validation loss decreased (0.095888 --> 0.095547).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08948 valid_loss: 0.09644 test_loss: 0.11110 \n",
      "[100/300] train_loss: 0.08755 valid_loss: 0.09719 test_loss: 0.10990 \n",
      "[101/300] train_loss: 0.08739 valid_loss: 0.09380 test_loss: 0.10771 \n",
      "Validation loss decreased (0.095547 --> 0.093796).  Saving model ...\n",
      "[102/300] train_loss: 0.08590 valid_loss: 0.09560 test_loss: 0.10811 \n",
      "[103/300] train_loss: 0.08875 valid_loss: 0.09525 test_loss: 0.10798 \n",
      "[104/300] train_loss: 0.08694 valid_loss: 0.09569 test_loss: 0.10891 \n",
      "[105/300] train_loss: 0.08709 valid_loss: 0.09468 test_loss: 0.10756 \n",
      "[106/300] train_loss: 0.08539 valid_loss: 0.09604 test_loss: 0.10805 \n",
      "[107/300] train_loss: 0.08588 valid_loss: 0.09411 test_loss: 0.10773 \n",
      "[108/300] train_loss: 0.08319 valid_loss: 0.09213 test_loss: 0.10688 \n",
      "Validation loss decreased (0.093796 --> 0.092133).  Saving model ...\n",
      "[109/300] train_loss: 0.08675 valid_loss: 0.09488 test_loss: 0.10932 \n",
      "[110/300] train_loss: 0.08609 valid_loss: 0.09199 test_loss: 0.10597 \n",
      "Validation loss decreased (0.092133 --> 0.091994).  Saving model ...\n",
      "[111/300] train_loss: 0.08732 valid_loss: 0.09461 test_loss: 0.10746 \n",
      "[112/300] train_loss: 0.08577 valid_loss: 0.09393 test_loss: 0.10628 \n",
      "[113/300] train_loss: 0.08509 valid_loss: 0.09488 test_loss: 0.10854 \n",
      "[114/300] train_loss: 0.08694 valid_loss: 0.09531 test_loss: 0.10775 \n",
      "[115/300] train_loss: 0.08392 valid_loss: 0.09339 test_loss: 0.10593 \n",
      "[116/300] train_loss: 0.08679 valid_loss: 0.09325 test_loss: 0.10669 \n",
      "[117/300] train_loss: 0.08467 valid_loss: 0.09202 test_loss: 0.10634 \n",
      "[118/300] train_loss: 0.08536 valid_loss: 0.09097 test_loss: 0.10626 \n",
      "Validation loss decreased (0.091994 --> 0.090966).  Saving model ...\n",
      "[119/300] train_loss: 0.08426 valid_loss: 0.09373 test_loss: 0.10579 \n",
      "[120/300] train_loss: 0.08547 valid_loss: 0.09220 test_loss: 0.10634 \n",
      "[121/300] train_loss: 0.08415 valid_loss: 0.09636 test_loss: 0.10598 \n",
      "[122/300] train_loss: 0.08472 valid_loss: 0.09135 test_loss: 0.10543 \n",
      "[123/300] train_loss: 0.08071 valid_loss: 0.09148 test_loss: 0.10549 \n",
      "[124/300] train_loss: 0.08266 valid_loss: 0.09072 test_loss: 0.10462 \n",
      "Validation loss decreased (0.090966 --> 0.090721).  Saving model ...\n",
      "[125/300] train_loss: 0.08491 valid_loss: 0.08944 test_loss: 0.10376 \n",
      "Validation loss decreased (0.090721 --> 0.089443).  Saving model ...\n",
      "[126/300] train_loss: 0.08262 valid_loss: 0.09077 test_loss: 0.10445 \n",
      "[127/300] train_loss: 0.08137 valid_loss: 0.09050 test_loss: 0.10431 \n",
      "[128/300] train_loss: 0.08329 valid_loss: 0.09318 test_loss: 0.10463 \n",
      "[129/300] train_loss: 0.08302 valid_loss: 0.08997 test_loss: 0.10334 \n",
      "[130/300] train_loss: 0.08273 valid_loss: 0.09040 test_loss: 0.10408 \n",
      "[131/300] train_loss: 0.08213 valid_loss: 0.09066 test_loss: 0.10450 \n",
      "[132/300] train_loss: 0.08264 valid_loss: 0.08968 test_loss: 0.10462 \n",
      "[133/300] train_loss: 0.08183 valid_loss: 0.09123 test_loss: 0.10597 \n",
      "[134/300] train_loss: 0.08060 valid_loss: 0.09207 test_loss: 0.10636 \n",
      "[135/300] train_loss: 0.08122 valid_loss: 0.08959 test_loss: 0.10324 \n",
      "[136/300] train_loss: 0.08355 valid_loss: 0.09035 test_loss: 0.10405 \n",
      "[137/300] train_loss: 0.08310 valid_loss: 0.09044 test_loss: 0.10289 \n",
      "[138/300] train_loss: 0.08125 valid_loss: 0.08903 test_loss: 0.10260 \n",
      "Validation loss decreased (0.089443 --> 0.089027).  Saving model ...\n",
      "[139/300] train_loss: 0.07942 valid_loss: 0.08867 test_loss: 0.10336 \n",
      "Validation loss decreased (0.089027 --> 0.088674).  Saving model ...\n",
      "[140/300] train_loss: 0.08041 valid_loss: 0.08933 test_loss: 0.10287 \n",
      "[141/300] train_loss: 0.08281 valid_loss: 0.08923 test_loss: 0.10335 \n",
      "[142/300] train_loss: 0.08125 valid_loss: 0.08929 test_loss: 0.10357 \n",
      "[143/300] train_loss: 0.08080 valid_loss: 0.08861 test_loss: 0.10213 \n",
      "Validation loss decreased (0.088674 --> 0.088610).  Saving model ...\n",
      "[144/300] train_loss: 0.08214 valid_loss: 0.08885 test_loss: 0.10305 \n",
      "[145/300] train_loss: 0.08167 valid_loss: 0.09073 test_loss: 0.10363 \n",
      "[146/300] train_loss: 0.08047 valid_loss: 0.08770 test_loss: 0.10201 \n",
      "Validation loss decreased (0.088610 --> 0.087704).  Saving model ...\n",
      "[147/300] train_loss: 0.07938 valid_loss: 0.08688 test_loss: 0.10167 \n",
      "Validation loss decreased (0.087704 --> 0.086876).  Saving model ...\n",
      "[148/300] train_loss: 0.08257 valid_loss: 0.08811 test_loss: 0.10290 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149/300] train_loss: 0.07909 valid_loss: 0.09137 test_loss: 0.10167 \n",
      "[150/300] train_loss: 0.08150 valid_loss: 0.09077 test_loss: 0.10179 \n",
      "[151/300] train_loss: 0.07912 valid_loss: 0.08812 test_loss: 0.10192 \n",
      "[152/300] train_loss: 0.08014 valid_loss: 0.08818 test_loss: 0.10227 \n",
      "[153/300] train_loss: 0.07928 valid_loss: 0.08615 test_loss: 0.10160 \n",
      "Validation loss decreased (0.086876 --> 0.086154).  Saving model ...\n",
      "[154/300] train_loss: 0.07949 valid_loss: 0.08721 test_loss: 0.10104 \n",
      "[155/300] train_loss: 0.07955 valid_loss: 0.08777 test_loss: 0.10130 \n",
      "[156/300] train_loss: 0.07878 valid_loss: 0.08684 test_loss: 0.10104 \n",
      "[157/300] train_loss: 0.07834 valid_loss: 0.08792 test_loss: 0.10141 \n",
      "[158/300] train_loss: 0.08247 valid_loss: 0.08842 test_loss: 0.10078 \n",
      "[159/300] train_loss: 0.07858 valid_loss: 0.08970 test_loss: 0.10132 \n",
      "[160/300] train_loss: 0.07730 valid_loss: 0.08677 test_loss: 0.10059 \n",
      "[161/300] train_loss: 0.07926 valid_loss: 0.08623 test_loss: 0.10026 \n",
      "[162/300] train_loss: 0.07670 valid_loss: 0.08682 test_loss: 0.10107 \n",
      "[163/300] train_loss: 0.07934 valid_loss: 0.08642 test_loss: 0.10087 \n",
      "[164/300] train_loss: 0.07760 valid_loss: 0.09057 test_loss: 0.10158 \n",
      "[165/300] train_loss: 0.07882 valid_loss: 0.08616 test_loss: 0.10000 \n",
      "[166/300] train_loss: 0.07680 valid_loss: 0.08871 test_loss: 0.09978 \n",
      "[167/300] train_loss: 0.07659 valid_loss: 0.08726 test_loss: 0.10016 \n",
      "[168/300] train_loss: 0.07640 valid_loss: 0.08521 test_loss: 0.09952 \n",
      "Validation loss decreased (0.086154 --> 0.085211).  Saving model ...\n",
      "[169/300] train_loss: 0.07890 valid_loss: 0.08690 test_loss: 0.09985 \n",
      "[170/300] train_loss: 0.07876 valid_loss: 0.08746 test_loss: 0.10168 \n",
      "[171/300] train_loss: 0.07590 valid_loss: 0.08855 test_loss: 0.09998 \n",
      "[172/300] train_loss: 0.08014 valid_loss: 0.08736 test_loss: 0.09950 \n",
      "[173/300] train_loss: 0.07841 valid_loss: 0.08702 test_loss: 0.10053 \n",
      "[174/300] train_loss: 0.08036 valid_loss: 0.08761 test_loss: 0.10056 \n",
      "[175/300] train_loss: 0.07555 valid_loss: 0.08680 test_loss: 0.10021 \n",
      "[176/300] train_loss: 0.07848 valid_loss: 0.08585 test_loss: 0.09969 \n",
      "[177/300] train_loss: 0.07826 valid_loss: 0.08661 test_loss: 0.09996 \n",
      "[178/300] train_loss: 0.07732 valid_loss: 0.08631 test_loss: 0.10039 \n",
      "[179/300] train_loss: 0.07676 valid_loss: 0.08524 test_loss: 0.09928 \n",
      "[180/300] train_loss: 0.07447 valid_loss: 0.08650 test_loss: 0.09976 \n",
      "[181/300] train_loss: 0.07722 valid_loss: 0.08537 test_loss: 0.09822 \n",
      "[182/300] train_loss: 0.07571 valid_loss: 0.08492 test_loss: 0.09909 \n",
      "Validation loss decreased (0.085211 --> 0.084924).  Saving model ...\n",
      "[183/300] train_loss: 0.07644 valid_loss: 0.08458 test_loss: 0.09883 \n",
      "Validation loss decreased (0.084924 --> 0.084583).  Saving model ...\n",
      "[184/300] train_loss: 0.07639 valid_loss: 0.08441 test_loss: 0.09895 \n",
      "Validation loss decreased (0.084583 --> 0.084408).  Saving model ...\n",
      "[185/300] train_loss: 0.07548 valid_loss: 0.08469 test_loss: 0.09805 \n",
      "[186/300] train_loss: 0.07298 valid_loss: 0.08618 test_loss: 0.09908 \n",
      "[187/300] train_loss: 0.07509 valid_loss: 0.08474 test_loss: 0.09846 \n",
      "[188/300] train_loss: 0.07537 valid_loss: 0.08477 test_loss: 0.09777 \n",
      "[189/300] train_loss: 0.07701 valid_loss: 0.08576 test_loss: 0.09876 \n",
      "[190/300] train_loss: 0.07590 valid_loss: 0.08617 test_loss: 0.09898 \n",
      "[191/300] train_loss: 0.07792 valid_loss: 0.08409 test_loss: 0.09790 \n",
      "Validation loss decreased (0.084408 --> 0.084093).  Saving model ...\n",
      "[192/300] train_loss: 0.07310 valid_loss: 0.08513 test_loss: 0.09878 \n",
      "[193/300] train_loss: 0.07500 valid_loss: 0.08477 test_loss: 0.09832 \n",
      "[194/300] train_loss: 0.07549 valid_loss: 0.08413 test_loss: 0.09773 \n",
      "[195/300] train_loss: 0.07396 valid_loss: 0.08438 test_loss: 0.09786 \n",
      "[196/300] train_loss: 0.07440 valid_loss: 0.08467 test_loss: 0.09766 \n",
      "[197/300] train_loss: 0.07329 valid_loss: 0.08402 test_loss: 0.09722 \n",
      "Validation loss decreased (0.084093 --> 0.084022).  Saving model ...\n",
      "[198/300] train_loss: 0.07463 valid_loss: 0.08580 test_loss: 0.09766 \n",
      "[199/300] train_loss: 0.07397 valid_loss: 0.08538 test_loss: 0.09946 \n",
      "[200/300] train_loss: 0.07463 valid_loss: 0.08353 test_loss: 0.09766 \n",
      "Validation loss decreased (0.084022 --> 0.083535).  Saving model ...\n",
      "[201/300] train_loss: 0.07334 valid_loss: 0.08305 test_loss: 0.09655 \n",
      "Validation loss decreased (0.083535 --> 0.083052).  Saving model ...\n",
      "[202/300] train_loss: 0.07656 valid_loss: 0.08399 test_loss: 0.09829 \n",
      "[203/300] train_loss: 0.07478 valid_loss: 0.08407 test_loss: 0.09718 \n",
      "[204/300] train_loss: 0.07405 valid_loss: 0.08450 test_loss: 0.09733 \n",
      "[205/300] train_loss: 0.07287 valid_loss: 0.08430 test_loss: 0.09718 \n",
      "[206/300] train_loss: 0.07315 valid_loss: 0.08382 test_loss: 0.09734 \n",
      "[207/300] train_loss: 0.07207 valid_loss: 0.08355 test_loss: 0.09679 \n",
      "[208/300] train_loss: 0.07539 valid_loss: 0.08320 test_loss: 0.09740 \n",
      "[209/300] train_loss: 0.07394 valid_loss: 0.08349 test_loss: 0.09718 \n",
      "[210/300] train_loss: 0.07557 valid_loss: 0.08390 test_loss: 0.09685 \n",
      "[211/300] train_loss: 0.07303 valid_loss: 0.08194 test_loss: 0.09615 \n",
      "Validation loss decreased (0.083052 --> 0.081944).  Saving model ...\n",
      "[212/300] train_loss: 0.07290 valid_loss: 0.08272 test_loss: 0.09616 \n",
      "[213/300] train_loss: 0.07131 valid_loss: 0.08349 test_loss: 0.09551 \n",
      "[214/300] train_loss: 0.07352 valid_loss: 0.08409 test_loss: 0.09701 \n",
      "[215/300] train_loss: 0.07195 valid_loss: 0.08313 test_loss: 0.09597 \n",
      "[216/300] train_loss: 0.07500 valid_loss: 0.08254 test_loss: 0.09645 \n",
      "[217/300] train_loss: 0.07230 valid_loss: 0.08218 test_loss: 0.09629 \n",
      "[218/300] train_loss: 0.07174 valid_loss: 0.08344 test_loss: 0.09748 \n",
      "[219/300] train_loss: 0.07109 valid_loss: 0.08199 test_loss: 0.09558 \n",
      "[220/300] train_loss: 0.07275 valid_loss: 0.08207 test_loss: 0.09525 \n",
      "[221/300] train_loss: 0.07206 valid_loss: 0.08345 test_loss: 0.09622 \n",
      "[222/300] train_loss: 0.07356 valid_loss: 0.08215 test_loss: 0.09538 \n",
      "[223/300] train_loss: 0.07450 valid_loss: 0.08216 test_loss: 0.09571 \n",
      "[224/300] train_loss: 0.07088 valid_loss: 0.08235 test_loss: 0.09551 \n",
      "[225/300] train_loss: 0.07148 valid_loss: 0.08299 test_loss: 0.09517 \n",
      "[226/300] train_loss: 0.07173 valid_loss: 0.08233 test_loss: 0.09550 \n",
      "[227/300] train_loss: 0.07257 valid_loss: 0.08256 test_loss: 0.09558 \n",
      "[228/300] train_loss: 0.07190 valid_loss: 0.08218 test_loss: 0.09674 \n",
      "[229/300] train_loss: 0.07144 valid_loss: 0.08150 test_loss: 0.09447 \n",
      "Validation loss decreased (0.081944 --> 0.081501).  Saving model ...\n",
      "[230/300] train_loss: 0.07260 valid_loss: 0.08247 test_loss: 0.09598 \n",
      "[231/300] train_loss: 0.07333 valid_loss: 0.08203 test_loss: 0.09531 \n",
      "[232/300] train_loss: 0.07181 valid_loss: 0.08150 test_loss: 0.09495 \n",
      "Validation loss decreased (0.081501 --> 0.081497).  Saving model ...\n",
      "[233/300] train_loss: 0.07268 valid_loss: 0.08657 test_loss: 0.09610 \n",
      "[234/300] train_loss: 0.07242 valid_loss: 0.08263 test_loss: 0.09556 \n",
      "[235/300] train_loss: 0.07376 valid_loss: 0.08163 test_loss: 0.09394 \n",
      "[236/300] train_loss: 0.07343 valid_loss: 0.08230 test_loss: 0.09547 \n",
      "[237/300] train_loss: 0.07121 valid_loss: 0.08395 test_loss: 0.09517 \n",
      "[238/300] train_loss: 0.07235 valid_loss: 0.08214 test_loss: 0.09456 \n",
      "[239/300] train_loss: 0.07271 valid_loss: 0.08147 test_loss: 0.09469 \n",
      "Validation loss decreased (0.081497 --> 0.081475).  Saving model ...\n",
      "[240/300] train_loss: 0.06962 valid_loss: 0.08249 test_loss: 0.09486 \n",
      "[241/300] train_loss: 0.07050 valid_loss: 0.08160 test_loss: 0.09440 \n",
      "[242/300] train_loss: 0.06951 valid_loss: 0.08126 test_loss: 0.09431 \n",
      "Validation loss decreased (0.081475 --> 0.081265).  Saving model ...\n",
      "[243/300] train_loss: 0.07021 valid_loss: 0.08032 test_loss: 0.09336 \n",
      "Validation loss decreased (0.081265 --> 0.080324).  Saving model ...\n",
      "[244/300] train_loss: 0.07276 valid_loss: 0.08330 test_loss: 0.09474 \n",
      "[245/300] train_loss: 0.07129 valid_loss: 0.08214 test_loss: 0.09588 \n",
      "[246/300] train_loss: 0.07189 valid_loss: 0.08144 test_loss: 0.09544 \n",
      "[247/300] train_loss: 0.07033 valid_loss: 0.08100 test_loss: 0.09539 \n",
      "[248/300] train_loss: 0.07196 valid_loss: 0.08087 test_loss: 0.09434 \n",
      "[249/300] train_loss: 0.07154 valid_loss: 0.08294 test_loss: 0.09560 \n",
      "[250/300] train_loss: 0.07094 valid_loss: 0.08156 test_loss: 0.09472 \n",
      "[251/300] train_loss: 0.07103 valid_loss: 0.08146 test_loss: 0.09545 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[252/300] train_loss: 0.07113 valid_loss: 0.08249 test_loss: 0.09543 \n",
      "[253/300] train_loss: 0.07052 valid_loss: 0.08180 test_loss: 0.09436 \n",
      "[254/300] train_loss: 0.06916 valid_loss: 0.08218 test_loss: 0.09528 \n",
      "[255/300] train_loss: 0.07176 valid_loss: 0.08148 test_loss: 0.09449 \n",
      "[256/300] train_loss: 0.07009 valid_loss: 0.08228 test_loss: 0.09475 \n",
      "[257/300] train_loss: 0.06948 valid_loss: 0.08119 test_loss: 0.09362 \n",
      "[258/300] train_loss: 0.07137 valid_loss: 0.08097 test_loss: 0.09340 \n",
      "[259/300] train_loss: 0.07113 valid_loss: 0.08094 test_loss: 0.09377 \n",
      "[260/300] train_loss: 0.06870 valid_loss: 0.08074 test_loss: 0.09368 \n",
      "[261/300] train_loss: 0.06937 valid_loss: 0.08048 test_loss: 0.09318 \n",
      "[262/300] train_loss: 0.07072 valid_loss: 0.08116 test_loss: 0.09306 \n",
      "[263/300] train_loss: 0.07059 valid_loss: 0.08229 test_loss: 0.09467 \n",
      "[264/300] train_loss: 0.06969 valid_loss: 0.08100 test_loss: 0.09374 \n",
      "[265/300] train_loss: 0.07057 valid_loss: 0.08184 test_loss: 0.09460 \n",
      "[266/300] train_loss: 0.07153 valid_loss: 0.08126 test_loss: 0.09420 \n",
      "[267/300] train_loss: 0.07013 valid_loss: 0.08117 test_loss: 0.09351 \n",
      "[268/300] train_loss: 0.06997 valid_loss: 0.08207 test_loss: 0.09387 \n",
      "[269/300] train_loss: 0.06752 valid_loss: 0.08190 test_loss: 0.09414 \n",
      "[270/300] train_loss: 0.06749 valid_loss: 0.08094 test_loss: 0.09392 \n",
      "[271/300] train_loss: 0.06586 valid_loss: 0.08086 test_loss: 0.09330 \n",
      "[272/300] train_loss: 0.06954 valid_loss: 0.07968 test_loss: 0.09299 \n",
      "Validation loss decreased (0.080324 --> 0.079682).  Saving model ...\n",
      "[273/300] train_loss: 0.06918 valid_loss: 0.08058 test_loss: 0.09305 \n",
      "[274/300] train_loss: 0.06895 valid_loss: 0.08226 test_loss: 0.09417 \n",
      "[275/300] train_loss: 0.06977 valid_loss: 0.08139 test_loss: 0.09366 \n",
      "[276/300] train_loss: 0.06937 valid_loss: 0.07975 test_loss: 0.09323 \n",
      "[277/300] train_loss: 0.06913 valid_loss: 0.08083 test_loss: 0.09397 \n",
      "[278/300] train_loss: 0.07075 valid_loss: 0.08242 test_loss: 0.09336 \n",
      "[279/300] train_loss: 0.07129 valid_loss: 0.08165 test_loss: 0.09367 \n",
      "[280/300] train_loss: 0.06838 valid_loss: 0.08106 test_loss: 0.09319 \n",
      "[281/300] train_loss: 0.06842 valid_loss: 0.08005 test_loss: 0.09345 \n",
      "[282/300] train_loss: 0.06953 valid_loss: 0.08114 test_loss: 0.09414 \n",
      "[283/300] train_loss: 0.06826 valid_loss: 0.07961 test_loss: 0.09265 \n",
      "Validation loss decreased (0.079682 --> 0.079615).  Saving model ...\n",
      "[284/300] train_loss: 0.06819 valid_loss: 0.08212 test_loss: 0.09575 \n",
      "[285/300] train_loss: 0.06652 valid_loss: 0.08019 test_loss: 0.09311 \n",
      "[286/300] train_loss: 0.06940 valid_loss: 0.07955 test_loss: 0.09294 \n",
      "Validation loss decreased (0.079615 --> 0.079545).  Saving model ...\n",
      "[287/300] train_loss: 0.06856 valid_loss: 0.08070 test_loss: 0.09323 \n",
      "[288/300] train_loss: 0.06850 valid_loss: 0.07985 test_loss: 0.09236 \n",
      "[289/300] train_loss: 0.06920 valid_loss: 0.07931 test_loss: 0.09303 \n",
      "Validation loss decreased (0.079545 --> 0.079311).  Saving model ...\n",
      "[290/300] train_loss: 0.06865 valid_loss: 0.08605 test_loss: 0.09300 \n",
      "[291/300] train_loss: 0.06680 valid_loss: 0.07989 test_loss: 0.09335 \n",
      "[292/300] train_loss: 0.06766 valid_loss: 0.07912 test_loss: 0.09297 \n",
      "Validation loss decreased (0.079311 --> 0.079124).  Saving model ...\n",
      "[293/300] train_loss: 0.06835 valid_loss: 0.07886 test_loss: 0.09291 \n",
      "Validation loss decreased (0.079124 --> 0.078859).  Saving model ...\n",
      "[294/300] train_loss: 0.06849 valid_loss: 0.08119 test_loss: 0.09362 \n",
      "[295/300] train_loss: 0.06723 valid_loss: 0.07973 test_loss: 0.09292 \n",
      "[296/300] train_loss: 0.06839 valid_loss: 0.07900 test_loss: 0.09275 \n",
      "[297/300] train_loss: 0.06722 valid_loss: 0.08018 test_loss: 0.09206 \n",
      "[298/300] train_loss: 0.06924 valid_loss: 0.08048 test_loss: 0.09272 \n",
      "[299/300] train_loss: 0.06896 valid_loss: 0.07957 test_loss: 0.09220 \n",
      "[300/300] train_loss: 0.06799 valid_loss: 0.08039 test_loss: 0.09303 \n",
      "TRAINING MODEL 2\n",
      "[  1/300] train_loss: 0.63998 valid_loss: 0.57484 test_loss: 0.56533 \n",
      "Validation loss decreased (inf --> 0.574839).  Saving model ...\n",
      "[  2/300] train_loss: 0.48134 valid_loss: 0.43482 test_loss: 0.42849 \n",
      "Validation loss decreased (0.574839 --> 0.434818).  Saving model ...\n",
      "[  3/300] train_loss: 0.37256 valid_loss: 0.36507 test_loss: 0.36508 \n",
      "Validation loss decreased (0.434818 --> 0.365067).  Saving model ...\n",
      "[  4/300] train_loss: 0.31491 valid_loss: 0.31952 test_loss: 0.32642 \n",
      "Validation loss decreased (0.365067 --> 0.319517).  Saving model ...\n",
      "[  5/300] train_loss: 0.27657 valid_loss: 0.28410 test_loss: 0.29474 \n",
      "Validation loss decreased (0.319517 --> 0.284097).  Saving model ...\n",
      "[  6/300] train_loss: 0.24675 valid_loss: 0.25158 test_loss: 0.26577 \n",
      "Validation loss decreased (0.284097 --> 0.251580).  Saving model ...\n",
      "[  7/300] train_loss: 0.22398 valid_loss: 0.22845 test_loss: 0.24367 \n",
      "Validation loss decreased (0.251580 --> 0.228454).  Saving model ...\n",
      "[  8/300] train_loss: 0.20461 valid_loss: 0.21385 test_loss: 0.22797 \n",
      "Validation loss decreased (0.228454 --> 0.213853).  Saving model ...\n",
      "[  9/300] train_loss: 0.19138 valid_loss: 0.19771 test_loss: 0.21461 \n",
      "Validation loss decreased (0.213853 --> 0.197714).  Saving model ...\n",
      "[ 10/300] train_loss: 0.18254 valid_loss: 0.18637 test_loss: 0.20248 \n",
      "Validation loss decreased (0.197714 --> 0.186370).  Saving model ...\n",
      "[ 11/300] train_loss: 0.17039 valid_loss: 0.17709 test_loss: 0.19269 \n",
      "Validation loss decreased (0.186370 --> 0.177092).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16497 valid_loss: 0.17266 test_loss: 0.18609 \n",
      "Validation loss decreased (0.177092 --> 0.172664).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15760 valid_loss: 0.16797 test_loss: 0.17936 \n",
      "Validation loss decreased (0.172664 --> 0.167973).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15024 valid_loss: 0.16053 test_loss: 0.17541 \n",
      "Validation loss decreased (0.167973 --> 0.160532).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14905 valid_loss: 0.15793 test_loss: 0.16998 \n",
      "Validation loss decreased (0.160532 --> 0.157934).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14475 valid_loss: 0.15344 test_loss: 0.16706 \n",
      "Validation loss decreased (0.157934 --> 0.153438).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13978 valid_loss: 0.15096 test_loss: 0.16458 \n",
      "Validation loss decreased (0.153438 --> 0.150961).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14013 valid_loss: 0.14883 test_loss: 0.16266 \n",
      "Validation loss decreased (0.150961 --> 0.148831).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13594 valid_loss: 0.14420 test_loss: 0.15663 \n",
      "Validation loss decreased (0.148831 --> 0.144201).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13100 valid_loss: 0.14212 test_loss: 0.15672 \n",
      "Validation loss decreased (0.144201 --> 0.142123).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13118 valid_loss: 0.14208 test_loss: 0.15416 \n",
      "Validation loss decreased (0.142123 --> 0.142085).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12839 valid_loss: 0.13816 test_loss: 0.15189 \n",
      "Validation loss decreased (0.142085 --> 0.138164).  Saving model ...\n",
      "[ 23/300] train_loss: 0.13046 valid_loss: 0.13875 test_loss: 0.15095 \n",
      "[ 24/300] train_loss: 0.12654 valid_loss: 0.13567 test_loss: 0.14890 \n",
      "Validation loss decreased (0.138164 --> 0.135666).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12695 valid_loss: 0.13526 test_loss: 0.14798 \n",
      "Validation loss decreased (0.135666 --> 0.135263).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12553 valid_loss: 0.13123 test_loss: 0.14431 \n",
      "Validation loss decreased (0.135263 --> 0.131233).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12644 valid_loss: 0.12873 test_loss: 0.14440 \n",
      "Validation loss decreased (0.131233 --> 0.128728).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12549 valid_loss: 0.13153 test_loss: 0.14677 \n",
      "[ 29/300] train_loss: 0.11849 valid_loss: 0.13027 test_loss: 0.14419 \n",
      "[ 30/300] train_loss: 0.11850 valid_loss: 0.12916 test_loss: 0.14338 \n",
      "[ 31/300] train_loss: 0.12068 valid_loss: 0.12871 test_loss: 0.13998 \n",
      "Validation loss decreased (0.128728 --> 0.128710).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11993 valid_loss: 0.12377 test_loss: 0.13976 \n",
      "Validation loss decreased (0.128710 --> 0.123769).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11874 valid_loss: 0.12293 test_loss: 0.13887 \n",
      "Validation loss decreased (0.123769 --> 0.122927).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11436 valid_loss: 0.12446 test_loss: 0.13966 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 35/300] train_loss: 0.11448 valid_loss: 0.12756 test_loss: 0.14229 \n",
      "[ 36/300] train_loss: 0.11598 valid_loss: 0.11913 test_loss: 0.13622 \n",
      "Validation loss decreased (0.122927 --> 0.119133).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11535 valid_loss: 0.11993 test_loss: 0.13562 \n",
      "[ 38/300] train_loss: 0.11062 valid_loss: 0.11966 test_loss: 0.13573 \n",
      "[ 39/300] train_loss: 0.11193 valid_loss: 0.12180 test_loss: 0.13483 \n",
      "[ 40/300] train_loss: 0.11187 valid_loss: 0.12047 test_loss: 0.13680 \n",
      "[ 41/300] train_loss: 0.11009 valid_loss: 0.11881 test_loss: 0.13437 \n",
      "Validation loss decreased (0.119133 --> 0.118806).  Saving model ...\n",
      "[ 42/300] train_loss: 0.11078 valid_loss: 0.11548 test_loss: 0.13188 \n",
      "Validation loss decreased (0.118806 --> 0.115477).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10954 valid_loss: 0.12001 test_loss: 0.13297 \n",
      "[ 44/300] train_loss: 0.11147 valid_loss: 0.11738 test_loss: 0.13086 \n",
      "[ 45/300] train_loss: 0.11071 valid_loss: 0.11880 test_loss: 0.13232 \n",
      "[ 46/300] train_loss: 0.10623 valid_loss: 0.11510 test_loss: 0.12954 \n",
      "Validation loss decreased (0.115477 --> 0.115105).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10454 valid_loss: 0.11759 test_loss: 0.13107 \n",
      "[ 48/300] train_loss: 0.10485 valid_loss: 0.11657 test_loss: 0.13019 \n",
      "[ 49/300] train_loss: 0.10441 valid_loss: 0.11362 test_loss: 0.12971 \n",
      "Validation loss decreased (0.115105 --> 0.113617).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10654 valid_loss: 0.11223 test_loss: 0.12754 \n",
      "Validation loss decreased (0.113617 --> 0.112234).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10323 valid_loss: 0.11697 test_loss: 0.12672 \n",
      "[ 52/300] train_loss: 0.10217 valid_loss: 0.11742 test_loss: 0.12756 \n",
      "[ 53/300] train_loss: 0.10213 valid_loss: 0.11595 test_loss: 0.12655 \n",
      "[ 54/300] train_loss: 0.10217 valid_loss: 0.11495 test_loss: 0.12676 \n",
      "[ 55/300] train_loss: 0.10350 valid_loss: 0.11226 test_loss: 0.12428 \n",
      "[ 56/300] train_loss: 0.10285 valid_loss: 0.10825 test_loss: 0.12460 \n",
      "Validation loss decreased (0.112234 --> 0.108251).  Saving model ...\n",
      "[ 57/300] train_loss: 0.10200 valid_loss: 0.11259 test_loss: 0.12557 \n",
      "[ 58/300] train_loss: 0.09800 valid_loss: 0.11173 test_loss: 0.12379 \n",
      "[ 59/300] train_loss: 0.10108 valid_loss: 0.11047 test_loss: 0.12300 \n",
      "[ 60/300] train_loss: 0.10194 valid_loss: 0.11166 test_loss: 0.12293 \n",
      "[ 61/300] train_loss: 0.10142 valid_loss: 0.10810 test_loss: 0.12182 \n",
      "Validation loss decreased (0.108251 --> 0.108100).  Saving model ...\n",
      "[ 62/300] train_loss: 0.10119 valid_loss: 0.11000 test_loss: 0.12062 \n",
      "[ 63/300] train_loss: 0.09625 valid_loss: 0.10852 test_loss: 0.12183 \n",
      "[ 64/300] train_loss: 0.09718 valid_loss: 0.10552 test_loss: 0.12013 \n",
      "Validation loss decreased (0.108100 --> 0.105515).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09697 valid_loss: 0.10428 test_loss: 0.11940 \n",
      "Validation loss decreased (0.105515 --> 0.104284).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09913 valid_loss: 0.10819 test_loss: 0.12116 \n",
      "[ 67/300] train_loss: 0.09667 valid_loss: 0.10818 test_loss: 0.12022 \n",
      "[ 68/300] train_loss: 0.09641 valid_loss: 0.10200 test_loss: 0.11962 \n",
      "Validation loss decreased (0.104284 --> 0.102003).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09839 valid_loss: 0.10621 test_loss: 0.11869 \n",
      "[ 70/300] train_loss: 0.09502 valid_loss: 0.10396 test_loss: 0.11896 \n",
      "[ 71/300] train_loss: 0.09870 valid_loss: 0.10310 test_loss: 0.11893 \n",
      "[ 72/300] train_loss: 0.09450 valid_loss: 0.10283 test_loss: 0.11822 \n",
      "[ 73/300] train_loss: 0.09625 valid_loss: 0.10530 test_loss: 0.11800 \n",
      "[ 74/300] train_loss: 0.09478 valid_loss: 0.10580 test_loss: 0.11856 \n",
      "[ 75/300] train_loss: 0.09243 valid_loss: 0.10255 test_loss: 0.11670 \n",
      "[ 76/300] train_loss: 0.09534 valid_loss: 0.10130 test_loss: 0.11759 \n",
      "Validation loss decreased (0.102003 --> 0.101304).  Saving model ...\n",
      "[ 77/300] train_loss: 0.09403 valid_loss: 0.10375 test_loss: 0.11615 \n",
      "[ 78/300] train_loss: 0.09510 valid_loss: 0.10119 test_loss: 0.11528 \n",
      "Validation loss decreased (0.101304 --> 0.101185).  Saving model ...\n",
      "[ 79/300] train_loss: 0.09285 valid_loss: 0.10113 test_loss: 0.11585 \n",
      "Validation loss decreased (0.101185 --> 0.101127).  Saving model ...\n",
      "[ 80/300] train_loss: 0.09361 valid_loss: 0.09957 test_loss: 0.11542 \n",
      "Validation loss decreased (0.101127 --> 0.099569).  Saving model ...\n",
      "[ 81/300] train_loss: 0.09216 valid_loss: 0.10052 test_loss: 0.11509 \n",
      "[ 82/300] train_loss: 0.09346 valid_loss: 0.10190 test_loss: 0.11578 \n",
      "[ 83/300] train_loss: 0.09293 valid_loss: 0.10371 test_loss: 0.11531 \n",
      "[ 84/300] train_loss: 0.09166 valid_loss: 0.10104 test_loss: 0.11573 \n",
      "[ 85/300] train_loss: 0.09251 valid_loss: 0.10210 test_loss: 0.11463 \n",
      "[ 86/300] train_loss: 0.09336 valid_loss: 0.10019 test_loss: 0.11380 \n",
      "[ 87/300] train_loss: 0.09116 valid_loss: 0.10009 test_loss: 0.11502 \n",
      "[ 88/300] train_loss: 0.09384 valid_loss: 0.09843 test_loss: 0.11277 \n",
      "Validation loss decreased (0.099569 --> 0.098429).  Saving model ...\n",
      "[ 89/300] train_loss: 0.09074 valid_loss: 0.09886 test_loss: 0.11306 \n",
      "[ 90/300] train_loss: 0.09415 valid_loss: 0.09873 test_loss: 0.11225 \n",
      "[ 91/300] train_loss: 0.08979 valid_loss: 0.09714 test_loss: 0.11209 \n",
      "Validation loss decreased (0.098429 --> 0.097136).  Saving model ...\n",
      "[ 92/300] train_loss: 0.09068 valid_loss: 0.10072 test_loss: 0.11299 \n",
      "[ 93/300] train_loss: 0.08769 valid_loss: 0.09698 test_loss: 0.11183 \n",
      "Validation loss decreased (0.097136 --> 0.096984).  Saving model ...\n",
      "[ 94/300] train_loss: 0.08925 valid_loss: 0.09789 test_loss: 0.11016 \n",
      "[ 95/300] train_loss: 0.08985 valid_loss: 0.09487 test_loss: 0.11075 \n",
      "Validation loss decreased (0.096984 --> 0.094875).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08926 valid_loss: 0.09582 test_loss: 0.11156 \n",
      "[ 97/300] train_loss: 0.09006 valid_loss: 0.09430 test_loss: 0.11055 \n",
      "Validation loss decreased (0.094875 --> 0.094304).  Saving model ...\n",
      "[ 98/300] train_loss: 0.09146 valid_loss: 0.09413 test_loss: 0.10959 \n",
      "Validation loss decreased (0.094304 --> 0.094130).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08784 valid_loss: 0.09762 test_loss: 0.11232 \n",
      "[100/300] train_loss: 0.08845 valid_loss: 0.09408 test_loss: 0.10901 \n",
      "Validation loss decreased (0.094130 --> 0.094084).  Saving model ...\n",
      "[101/300] train_loss: 0.08838 valid_loss: 0.09581 test_loss: 0.11173 \n",
      "[102/300] train_loss: 0.08925 valid_loss: 0.09439 test_loss: 0.11002 \n",
      "[103/300] train_loss: 0.08829 valid_loss: 0.09532 test_loss: 0.11094 \n",
      "[104/300] train_loss: 0.08913 valid_loss: 0.09794 test_loss: 0.10855 \n",
      "[105/300] train_loss: 0.08689 valid_loss: 0.09617 test_loss: 0.10787 \n",
      "[106/300] train_loss: 0.08807 valid_loss: 0.10136 test_loss: 0.10933 \n",
      "[107/300] train_loss: 0.08528 valid_loss: 0.09343 test_loss: 0.10839 \n",
      "Validation loss decreased (0.094084 --> 0.093431).  Saving model ...\n",
      "[108/300] train_loss: 0.08644 valid_loss: 0.09381 test_loss: 0.10685 \n",
      "[109/300] train_loss: 0.08793 valid_loss: 0.09470 test_loss: 0.10664 \n",
      "[110/300] train_loss: 0.08659 valid_loss: 0.09484 test_loss: 0.10709 \n",
      "[111/300] train_loss: 0.08641 valid_loss: 0.09396 test_loss: 0.10784 \n",
      "[112/300] train_loss: 0.08816 valid_loss: 0.09780 test_loss: 0.10795 \n",
      "[113/300] train_loss: 0.08707 valid_loss: 0.09239 test_loss: 0.10644 \n",
      "Validation loss decreased (0.093431 --> 0.092392).  Saving model ...\n",
      "[114/300] train_loss: 0.08646 valid_loss: 0.09401 test_loss: 0.10746 \n",
      "[115/300] train_loss: 0.08456 valid_loss: 0.09415 test_loss: 0.10762 \n",
      "[116/300] train_loss: 0.08708 valid_loss: 0.09360 test_loss: 0.10678 \n",
      "[117/300] train_loss: 0.08300 valid_loss: 0.09784 test_loss: 0.10750 \n",
      "[118/300] train_loss: 0.08229 valid_loss: 0.09696 test_loss: 0.10559 \n",
      "[119/300] train_loss: 0.08466 valid_loss: 0.09660 test_loss: 0.10691 \n",
      "[120/300] train_loss: 0.08677 valid_loss: 0.09757 test_loss: 0.10610 \n",
      "[121/300] train_loss: 0.08477 valid_loss: 0.10700 test_loss: 0.10628 \n",
      "[122/300] train_loss: 0.08365 valid_loss: 0.09636 test_loss: 0.10571 \n",
      "[123/300] train_loss: 0.08508 valid_loss: 0.09719 test_loss: 0.10483 \n",
      "[124/300] train_loss: 0.08208 valid_loss: 0.09860 test_loss: 0.10489 \n",
      "[125/300] train_loss: 0.08422 valid_loss: 0.10893 test_loss: 0.10589 \n",
      "[126/300] train_loss: 0.08304 valid_loss: 0.09723 test_loss: 0.10542 \n",
      "[127/300] train_loss: 0.08483 valid_loss: 0.09673 test_loss: 0.10525 \n",
      "[128/300] train_loss: 0.08628 valid_loss: 0.09311 test_loss: 0.10387 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129/300] train_loss: 0.08384 valid_loss: 0.09269 test_loss: 0.10451 \n",
      "[130/300] train_loss: 0.08296 valid_loss: 0.09328 test_loss: 0.10587 \n",
      "[131/300] train_loss: 0.08425 valid_loss: 0.09249 test_loss: 0.10457 \n",
      "[132/300] train_loss: 0.08314 valid_loss: 0.09528 test_loss: 0.10426 \n",
      "[133/300] train_loss: 0.08182 valid_loss: 0.10176 test_loss: 0.10371 \n",
      "[134/300] train_loss: 0.08521 valid_loss: 0.09029 test_loss: 0.10309 \n",
      "Validation loss decreased (0.092392 --> 0.090287).  Saving model ...\n",
      "[135/300] train_loss: 0.08445 valid_loss: 0.09271 test_loss: 0.10396 \n",
      "[136/300] train_loss: 0.08311 valid_loss: 0.09081 test_loss: 0.10347 \n",
      "[137/300] train_loss: 0.08308 valid_loss: 0.09402 test_loss: 0.10275 \n",
      "[138/300] train_loss: 0.08305 valid_loss: 0.09372 test_loss: 0.10440 \n",
      "[139/300] train_loss: 0.08389 valid_loss: 0.09367 test_loss: 0.10313 \n",
      "[140/300] train_loss: 0.08378 valid_loss: 0.09879 test_loss: 0.10234 \n",
      "[141/300] train_loss: 0.08057 valid_loss: 0.09952 test_loss: 0.10288 \n",
      "[142/300] train_loss: 0.08269 valid_loss: 0.09227 test_loss: 0.10317 \n",
      "[143/300] train_loss: 0.08092 valid_loss: 0.09238 test_loss: 0.10294 \n",
      "[144/300] train_loss: 0.08175 valid_loss: 0.09313 test_loss: 0.10259 \n",
      "[145/300] train_loss: 0.08110 valid_loss: 0.10151 test_loss: 0.10334 \n",
      "[146/300] train_loss: 0.07880 valid_loss: 0.09307 test_loss: 0.10257 \n",
      "[147/300] train_loss: 0.08184 valid_loss: 0.09606 test_loss: 0.10269 \n",
      "[148/300] train_loss: 0.08012 valid_loss: 0.09970 test_loss: 0.10241 \n",
      "[149/300] train_loss: 0.08110 valid_loss: 0.09522 test_loss: 0.10257 \n",
      "[150/300] train_loss: 0.07976 valid_loss: 0.09301 test_loss: 0.10188 \n",
      "[151/300] train_loss: 0.08296 valid_loss: 0.09220 test_loss: 0.10216 \n",
      "[152/300] train_loss: 0.08067 valid_loss: 0.09285 test_loss: 0.10193 \n",
      "[153/300] train_loss: 0.07983 valid_loss: 0.08982 test_loss: 0.10164 \n",
      "Validation loss decreased (0.090287 --> 0.089822).  Saving model ...\n",
      "[154/300] train_loss: 0.08087 valid_loss: 0.09310 test_loss: 0.10138 \n",
      "[155/300] train_loss: 0.08121 valid_loss: 0.09355 test_loss: 0.10046 \n",
      "[156/300] train_loss: 0.07894 valid_loss: 0.09189 test_loss: 0.10185 \n",
      "[157/300] train_loss: 0.08123 valid_loss: 0.09384 test_loss: 0.10163 \n",
      "[158/300] train_loss: 0.07954 valid_loss: 0.09441 test_loss: 0.10169 \n",
      "[159/300] train_loss: 0.08024 valid_loss: 0.09043 test_loss: 0.10175 \n",
      "[160/300] train_loss: 0.07744 valid_loss: 0.09075 test_loss: 0.10089 \n",
      "[161/300] train_loss: 0.07905 valid_loss: 0.08816 test_loss: 0.10069 \n",
      "Validation loss decreased (0.089822 --> 0.088164).  Saving model ...\n",
      "[162/300] train_loss: 0.08029 valid_loss: 0.09431 test_loss: 0.10064 \n",
      "[163/300] train_loss: 0.07869 valid_loss: 0.09050 test_loss: 0.09987 \n",
      "[164/300] train_loss: 0.07741 valid_loss: 0.09035 test_loss: 0.10069 \n",
      "[165/300] train_loss: 0.08116 valid_loss: 0.09135 test_loss: 0.10091 \n",
      "[166/300] train_loss: 0.08146 valid_loss: 0.08821 test_loss: 0.10045 \n",
      "[167/300] train_loss: 0.07942 valid_loss: 0.08798 test_loss: 0.10069 \n",
      "Validation loss decreased (0.088164 --> 0.087977).  Saving model ...\n",
      "[168/300] train_loss: 0.07823 valid_loss: 0.08941 test_loss: 0.10093 \n",
      "[169/300] train_loss: 0.07683 valid_loss: 0.08942 test_loss: 0.10044 \n",
      "[170/300] train_loss: 0.07575 valid_loss: 0.09268 test_loss: 0.10064 \n",
      "[171/300] train_loss: 0.08086 valid_loss: 0.08786 test_loss: 0.10059 \n",
      "Validation loss decreased (0.087977 --> 0.087863).  Saving model ...\n",
      "[172/300] train_loss: 0.08054 valid_loss: 0.08817 test_loss: 0.10026 \n",
      "[173/300] train_loss: 0.07713 valid_loss: 0.08943 test_loss: 0.10030 \n",
      "[174/300] train_loss: 0.07883 valid_loss: 0.08811 test_loss: 0.09978 \n",
      "[175/300] train_loss: 0.07732 valid_loss: 0.09036 test_loss: 0.09956 \n",
      "[176/300] train_loss: 0.08119 valid_loss: 0.08735 test_loss: 0.10056 \n",
      "Validation loss decreased (0.087863 --> 0.087347).  Saving model ...\n",
      "[177/300] train_loss: 0.07844 valid_loss: 0.09075 test_loss: 0.09995 \n",
      "[178/300] train_loss: 0.07607 valid_loss: 0.08790 test_loss: 0.09997 \n",
      "[179/300] train_loss: 0.07909 valid_loss: 0.08806 test_loss: 0.09934 \n",
      "[180/300] train_loss: 0.07663 valid_loss: 0.08807 test_loss: 0.10008 \n",
      "[181/300] train_loss: 0.08117 valid_loss: 0.08829 test_loss: 0.09851 \n",
      "[182/300] train_loss: 0.07872 valid_loss: 0.08690 test_loss: 0.09939 \n",
      "Validation loss decreased (0.087347 --> 0.086903).  Saving model ...\n",
      "[183/300] train_loss: 0.07538 valid_loss: 0.08658 test_loss: 0.09890 \n",
      "Validation loss decreased (0.086903 --> 0.086584).  Saving model ...\n",
      "[184/300] train_loss: 0.07552 valid_loss: 0.08685 test_loss: 0.09984 \n",
      "[185/300] train_loss: 0.07871 valid_loss: 0.08897 test_loss: 0.09853 \n",
      "[186/300] train_loss: 0.07727 valid_loss: 0.08775 test_loss: 0.10110 \n",
      "[187/300] train_loss: 0.07713 valid_loss: 0.08466 test_loss: 0.09837 \n",
      "Validation loss decreased (0.086584 --> 0.084657).  Saving model ...\n",
      "[188/300] train_loss: 0.07479 valid_loss: 0.09075 test_loss: 0.09931 \n",
      "[189/300] train_loss: 0.07677 valid_loss: 0.08649 test_loss: 0.09851 \n",
      "[190/300] train_loss: 0.07483 valid_loss: 0.08644 test_loss: 0.09793 \n",
      "[191/300] train_loss: 0.07691 valid_loss: 0.08517 test_loss: 0.09770 \n",
      "[192/300] train_loss: 0.07555 valid_loss: 0.08569 test_loss: 0.09728 \n",
      "[193/300] train_loss: 0.07567 valid_loss: 0.09012 test_loss: 0.09847 \n",
      "[194/300] train_loss: 0.07537 valid_loss: 0.08794 test_loss: 0.09825 \n",
      "[195/300] train_loss: 0.07516 valid_loss: 0.08673 test_loss: 0.09809 \n",
      "[196/300] train_loss: 0.07679 valid_loss: 0.08647 test_loss: 0.09671 \n",
      "[197/300] train_loss: 0.07645 valid_loss: 0.08724 test_loss: 0.09927 \n",
      "[198/300] train_loss: 0.07540 valid_loss: 0.09154 test_loss: 0.09905 \n",
      "[199/300] train_loss: 0.07475 valid_loss: 0.09082 test_loss: 0.09738 \n",
      "[200/300] train_loss: 0.07704 valid_loss: 0.08939 test_loss: 0.09900 \n",
      "[201/300] train_loss: 0.07407 valid_loss: 0.08640 test_loss: 0.09768 \n",
      "[202/300] train_loss: 0.07657 valid_loss: 0.08633 test_loss: 0.09641 \n",
      "[203/300] train_loss: 0.07723 valid_loss: 0.08570 test_loss: 0.09658 \n",
      "[204/300] train_loss: 0.07275 valid_loss: 0.08799 test_loss: 0.09718 \n",
      "[205/300] train_loss: 0.07387 valid_loss: 0.09001 test_loss: 0.09757 \n",
      "[206/300] train_loss: 0.07464 valid_loss: 0.08660 test_loss: 0.09739 \n",
      "[207/300] train_loss: 0.07530 valid_loss: 0.08584 test_loss: 0.09663 \n",
      "[208/300] train_loss: 0.07292 valid_loss: 0.08565 test_loss: 0.09598 \n",
      "[209/300] train_loss: 0.07665 valid_loss: 0.08462 test_loss: 0.09614 \n",
      "Validation loss decreased (0.084657 --> 0.084615).  Saving model ...\n",
      "[210/300] train_loss: 0.07507 valid_loss: 0.08542 test_loss: 0.09629 \n",
      "[211/300] train_loss: 0.07540 valid_loss: 0.08581 test_loss: 0.09556 \n",
      "[212/300] train_loss: 0.07489 valid_loss: 0.08532 test_loss: 0.09662 \n",
      "[213/300] train_loss: 0.07638 valid_loss: 0.08498 test_loss: 0.09572 \n",
      "[214/300] train_loss: 0.07285 valid_loss: 0.08586 test_loss: 0.09537 \n",
      "[215/300] train_loss: 0.07363 valid_loss: 0.08553 test_loss: 0.09675 \n",
      "[216/300] train_loss: 0.07482 valid_loss: 0.08600 test_loss: 0.09611 \n",
      "[217/300] train_loss: 0.07224 valid_loss: 0.08436 test_loss: 0.09578 \n",
      "Validation loss decreased (0.084615 --> 0.084356).  Saving model ...\n",
      "[218/300] train_loss: 0.07322 valid_loss: 0.08815 test_loss: 0.09788 \n",
      "[219/300] train_loss: 0.07252 valid_loss: 0.08475 test_loss: 0.09669 \n",
      "[220/300] train_loss: 0.07485 valid_loss: 0.08879 test_loss: 0.09840 \n",
      "[221/300] train_loss: 0.07323 valid_loss: 0.08557 test_loss: 0.09632 \n",
      "[222/300] train_loss: 0.07224 valid_loss: 0.08323 test_loss: 0.09513 \n",
      "Validation loss decreased (0.084356 --> 0.083229).  Saving model ...\n",
      "[223/300] train_loss: 0.07199 valid_loss: 0.08492 test_loss: 0.09642 \n",
      "[224/300] train_loss: 0.07366 valid_loss: 0.08644 test_loss: 0.09600 \n",
      "[225/300] train_loss: 0.07337 valid_loss: 0.08335 test_loss: 0.09521 \n",
      "[226/300] train_loss: 0.07234 valid_loss: 0.08584 test_loss: 0.09704 \n",
      "[227/300] train_loss: 0.07504 valid_loss: 0.08416 test_loss: 0.09515 \n",
      "[228/300] train_loss: 0.07342 valid_loss: 0.08286 test_loss: 0.09502 \n",
      "Validation loss decreased (0.083229 --> 0.082865).  Saving model ...\n",
      "[229/300] train_loss: 0.07258 valid_loss: 0.08584 test_loss: 0.09716 \n",
      "[230/300] train_loss: 0.06986 valid_loss: 0.08586 test_loss: 0.09499 \n",
      "[231/300] train_loss: 0.07315 valid_loss: 0.08470 test_loss: 0.09528 \n",
      "[232/300] train_loss: 0.07212 valid_loss: 0.08378 test_loss: 0.09508 \n",
      "[233/300] train_loss: 0.07068 valid_loss: 0.08619 test_loss: 0.09484 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[234/300] train_loss: 0.07452 valid_loss: 0.08903 test_loss: 0.09460 \n",
      "[235/300] train_loss: 0.07428 valid_loss: 0.08348 test_loss: 0.09664 \n",
      "[236/300] train_loss: 0.07071 valid_loss: 0.08466 test_loss: 0.09534 \n",
      "[237/300] train_loss: 0.07182 valid_loss: 0.08343 test_loss: 0.09473 \n",
      "[238/300] train_loss: 0.07342 valid_loss: 0.08242 test_loss: 0.09408 \n",
      "Validation loss decreased (0.082865 --> 0.082424).  Saving model ...\n",
      "[239/300] train_loss: 0.07088 valid_loss: 0.08548 test_loss: 0.09517 \n",
      "[240/300] train_loss: 0.07249 valid_loss: 0.08259 test_loss: 0.09335 \n",
      "[241/300] train_loss: 0.07137 valid_loss: 0.08451 test_loss: 0.09605 \n",
      "[242/300] train_loss: 0.07312 valid_loss: 0.08177 test_loss: 0.09472 \n",
      "Validation loss decreased (0.082424 --> 0.081771).  Saving model ...\n",
      "[243/300] train_loss: 0.07083 valid_loss: 0.08468 test_loss: 0.09514 \n",
      "[244/300] train_loss: 0.07208 valid_loss: 0.08276 test_loss: 0.09447 \n",
      "[245/300] train_loss: 0.07174 valid_loss: 0.08383 test_loss: 0.09374 \n",
      "[246/300] train_loss: 0.06930 valid_loss: 0.08409 test_loss: 0.09399 \n",
      "[247/300] train_loss: 0.07060 valid_loss: 0.08299 test_loss: 0.09464 \n",
      "[248/300] train_loss: 0.07210 valid_loss: 0.08457 test_loss: 0.09384 \n",
      "[249/300] train_loss: 0.07206 valid_loss: 0.08225 test_loss: 0.09475 \n",
      "[250/300] train_loss: 0.07221 valid_loss: 0.08278 test_loss: 0.09381 \n",
      "[251/300] train_loss: 0.07317 valid_loss: 0.08229 test_loss: 0.09332 \n",
      "[252/300] train_loss: 0.07171 valid_loss: 0.08481 test_loss: 0.09418 \n",
      "[253/300] train_loss: 0.06973 valid_loss: 0.08418 test_loss: 0.09339 \n",
      "[254/300] train_loss: 0.07266 valid_loss: 0.08359 test_loss: 0.09361 \n",
      "[255/300] train_loss: 0.07059 valid_loss: 0.08550 test_loss: 0.09486 \n",
      "[256/300] train_loss: 0.07051 valid_loss: 0.08363 test_loss: 0.09391 \n",
      "[257/300] train_loss: 0.07205 valid_loss: 0.08323 test_loss: 0.09441 \n",
      "[258/300] train_loss: 0.06943 valid_loss: 0.08182 test_loss: 0.09456 \n",
      "[259/300] train_loss: 0.07154 valid_loss: 0.08383 test_loss: 0.09390 \n",
      "[260/300] train_loss: 0.07159 valid_loss: 0.08141 test_loss: 0.09251 \n",
      "Validation loss decreased (0.081771 --> 0.081407).  Saving model ...\n",
      "[261/300] train_loss: 0.07246 valid_loss: 0.08321 test_loss: 0.09375 \n",
      "[262/300] train_loss: 0.06970 valid_loss: 0.08258 test_loss: 0.09410 \n",
      "[263/300] train_loss: 0.07338 valid_loss: 0.08266 test_loss: 0.09359 \n",
      "[264/300] train_loss: 0.07141 valid_loss: 0.08289 test_loss: 0.09306 \n",
      "[265/300] train_loss: 0.07160 valid_loss: 0.08104 test_loss: 0.09269 \n",
      "Validation loss decreased (0.081407 --> 0.081040).  Saving model ...\n",
      "[266/300] train_loss: 0.06959 valid_loss: 0.08482 test_loss: 0.09411 \n",
      "[267/300] train_loss: 0.07060 valid_loss: 0.08196 test_loss: 0.09279 \n",
      "[268/300] train_loss: 0.07142 valid_loss: 0.08191 test_loss: 0.09316 \n",
      "[269/300] train_loss: 0.06973 valid_loss: 0.08200 test_loss: 0.09318 \n",
      "[270/300] train_loss: 0.07017 valid_loss: 0.08388 test_loss: 0.09248 \n",
      "[271/300] train_loss: 0.07115 valid_loss: 0.08354 test_loss: 0.09246 \n",
      "[272/300] train_loss: 0.07138 valid_loss: 0.08293 test_loss: 0.09313 \n",
      "[273/300] train_loss: 0.07015 valid_loss: 0.08216 test_loss: 0.09231 \n",
      "[274/300] train_loss: 0.07233 valid_loss: 0.08213 test_loss: 0.09323 \n",
      "[275/300] train_loss: 0.06937 valid_loss: 0.08256 test_loss: 0.09339 \n",
      "[276/300] train_loss: 0.06802 valid_loss: 0.08169 test_loss: 0.09240 \n",
      "[277/300] train_loss: 0.07212 valid_loss: 0.08078 test_loss: 0.09235 \n",
      "Validation loss decreased (0.081040 --> 0.080782).  Saving model ...\n",
      "[278/300] train_loss: 0.07120 valid_loss: 0.08219 test_loss: 0.09173 \n",
      "[279/300] train_loss: 0.06795 valid_loss: 0.08223 test_loss: 0.09323 \n",
      "[280/300] train_loss: 0.07004 valid_loss: 0.08256 test_loss: 0.09285 \n",
      "[281/300] train_loss: 0.07137 valid_loss: 0.08255 test_loss: 0.09313 \n",
      "[282/300] train_loss: 0.07137 valid_loss: 0.08179 test_loss: 0.09123 \n",
      "[283/300] train_loss: 0.06981 valid_loss: 0.08183 test_loss: 0.09229 \n",
      "[284/300] train_loss: 0.06910 valid_loss: 0.08005 test_loss: 0.09139 \n",
      "Validation loss decreased (0.080782 --> 0.080047).  Saving model ...\n",
      "[285/300] train_loss: 0.07081 valid_loss: 0.08115 test_loss: 0.09244 \n",
      "[286/300] train_loss: 0.06793 valid_loss: 0.08524 test_loss: 0.09284 \n",
      "[287/300] train_loss: 0.06832 valid_loss: 0.08270 test_loss: 0.09217 \n",
      "[288/300] train_loss: 0.06783 valid_loss: 0.08216 test_loss: 0.09265 \n",
      "[289/300] train_loss: 0.06903 valid_loss: 0.08218 test_loss: 0.09189 \n",
      "[290/300] train_loss: 0.06849 valid_loss: 0.08390 test_loss: 0.09219 \n",
      "[291/300] train_loss: 0.06990 valid_loss: 0.08165 test_loss: 0.09145 \n",
      "[292/300] train_loss: 0.07143 valid_loss: 0.08186 test_loss: 0.09177 \n",
      "[293/300] train_loss: 0.06827 valid_loss: 0.08265 test_loss: 0.09295 \n",
      "[294/300] train_loss: 0.06830 valid_loss: 0.08291 test_loss: 0.09361 \n",
      "[295/300] train_loss: 0.06687 valid_loss: 0.08109 test_loss: 0.09146 \n",
      "[296/300] train_loss: 0.06737 valid_loss: 0.08075 test_loss: 0.09191 \n",
      "[297/300] train_loss: 0.06894 valid_loss: 0.08045 test_loss: 0.09229 \n",
      "[298/300] train_loss: 0.06920 valid_loss: 0.08020 test_loss: 0.09240 \n",
      "[299/300] train_loss: 0.06878 valid_loss: 0.08177 test_loss: 0.09266 \n",
      "[300/300] train_loss: 0.06943 valid_loss: 0.08151 test_loss: 0.09185 \n",
      "TRAINING MODEL 3\n",
      "[  1/300] train_loss: 0.63717 valid_loss: 0.58074 test_loss: 0.58800 \n",
      "Validation loss decreased (inf --> 0.580737).  Saving model ...\n",
      "[  2/300] train_loss: 0.48691 valid_loss: 0.43501 test_loss: 0.44329 \n",
      "Validation loss decreased (0.580737 --> 0.435005).  Saving model ...\n",
      "[  3/300] train_loss: 0.37614 valid_loss: 0.36249 test_loss: 0.37274 \n",
      "Validation loss decreased (0.435005 --> 0.362492).  Saving model ...\n",
      "[  4/300] train_loss: 0.31627 valid_loss: 0.31716 test_loss: 0.33176 \n",
      "Validation loss decreased (0.362492 --> 0.317157).  Saving model ...\n",
      "[  5/300] train_loss: 0.27644 valid_loss: 0.28150 test_loss: 0.29930 \n",
      "Validation loss decreased (0.317157 --> 0.281502).  Saving model ...\n",
      "[  6/300] train_loss: 0.24785 valid_loss: 0.24778 test_loss: 0.26801 \n",
      "Validation loss decreased (0.281502 --> 0.247785).  Saving model ...\n",
      "[  7/300] train_loss: 0.22146 valid_loss: 0.22689 test_loss: 0.24423 \n",
      "Validation loss decreased (0.247785 --> 0.226892).  Saving model ...\n",
      "[  8/300] train_loss: 0.20460 valid_loss: 0.20638 test_loss: 0.22102 \n",
      "Validation loss decreased (0.226892 --> 0.206381).  Saving model ...\n",
      "[  9/300] train_loss: 0.18783 valid_loss: 0.19416 test_loss: 0.21057 \n",
      "Validation loss decreased (0.206381 --> 0.194163).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17934 valid_loss: 0.18388 test_loss: 0.19517 \n",
      "Validation loss decreased (0.194163 --> 0.183876).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16757 valid_loss: 0.17439 test_loss: 0.18766 \n",
      "Validation loss decreased (0.183876 --> 0.174390).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16450 valid_loss: 0.16911 test_loss: 0.17999 \n",
      "Validation loss decreased (0.174390 --> 0.169107).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15579 valid_loss: 0.16362 test_loss: 0.17615 \n",
      "Validation loss decreased (0.169107 --> 0.163621).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15254 valid_loss: 0.15949 test_loss: 0.17164 \n",
      "Validation loss decreased (0.163621 --> 0.159491).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14671 valid_loss: 0.15859 test_loss: 0.16948 \n",
      "Validation loss decreased (0.159491 --> 0.158592).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14655 valid_loss: 0.15097 test_loss: 0.16240 \n",
      "Validation loss decreased (0.158592 --> 0.150970).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14514 valid_loss: 0.14722 test_loss: 0.16027 \n",
      "Validation loss decreased (0.150970 --> 0.147225).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14068 valid_loss: 0.14734 test_loss: 0.15891 \n",
      "[ 19/300] train_loss: 0.14216 valid_loss: 0.14560 test_loss: 0.15876 \n",
      "Validation loss decreased (0.147225 --> 0.145601).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13726 valid_loss: 0.14197 test_loss: 0.15472 \n",
      "Validation loss decreased (0.145601 --> 0.141974).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13364 valid_loss: 0.13907 test_loss: 0.15545 \n",
      "Validation loss decreased (0.141974 --> 0.139070).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12825 valid_loss: 0.14042 test_loss: 0.15296 \n",
      "[ 23/300] train_loss: 0.12863 valid_loss: 0.13515 test_loss: 0.15108 \n",
      "Validation loss decreased (0.139070 --> 0.135151).  Saving model ...\n",
      "[ 24/300] train_loss: 0.13152 valid_loss: 0.13482 test_loss: 0.14862 \n",
      "Validation loss decreased (0.135151 --> 0.134823).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25/300] train_loss: 0.12671 valid_loss: 0.13295 test_loss: 0.14597 \n",
      "Validation loss decreased (0.134823 --> 0.132950).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12461 valid_loss: 0.12987 test_loss: 0.14600 \n",
      "Validation loss decreased (0.132950 --> 0.129867).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12574 valid_loss: 0.12801 test_loss: 0.14449 \n",
      "Validation loss decreased (0.129867 --> 0.128014).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12462 valid_loss: 0.12898 test_loss: 0.14476 \n",
      "[ 29/300] train_loss: 0.12149 valid_loss: 0.12964 test_loss: 0.14327 \n",
      "[ 30/300] train_loss: 0.11897 valid_loss: 0.12407 test_loss: 0.14034 \n",
      "Validation loss decreased (0.128014 --> 0.124066).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11656 valid_loss: 0.12435 test_loss: 0.14126 \n",
      "[ 32/300] train_loss: 0.11856 valid_loss: 0.12072 test_loss: 0.14104 \n",
      "Validation loss decreased (0.124066 --> 0.120721).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11489 valid_loss: 0.12294 test_loss: 0.14037 \n",
      "[ 34/300] train_loss: 0.11661 valid_loss: 0.12024 test_loss: 0.13713 \n",
      "Validation loss decreased (0.120721 --> 0.120238).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11706 valid_loss: 0.12175 test_loss: 0.13722 \n",
      "[ 36/300] train_loss: 0.11685 valid_loss: 0.11964 test_loss: 0.13546 \n",
      "Validation loss decreased (0.120238 --> 0.119640).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11457 valid_loss: 0.11809 test_loss: 0.13555 \n",
      "Validation loss decreased (0.119640 --> 0.118094).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11101 valid_loss: 0.12116 test_loss: 0.13573 \n",
      "[ 39/300] train_loss: 0.11298 valid_loss: 0.11854 test_loss: 0.13541 \n",
      "[ 40/300] train_loss: 0.11231 valid_loss: 0.12050 test_loss: 0.13538 \n",
      "[ 41/300] train_loss: 0.10999 valid_loss: 0.11547 test_loss: 0.13292 \n",
      "Validation loss decreased (0.118094 --> 0.115468).  Saving model ...\n",
      "[ 42/300] train_loss: 0.11069 valid_loss: 0.11888 test_loss: 0.13322 \n",
      "[ 43/300] train_loss: 0.11050 valid_loss: 0.11459 test_loss: 0.13234 \n",
      "Validation loss decreased (0.115468 --> 0.114595).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10968 valid_loss: 0.12018 test_loss: 0.13344 \n",
      "[ 45/300] train_loss: 0.10802 valid_loss: 0.11865 test_loss: 0.13200 \n",
      "[ 46/300] train_loss: 0.10754 valid_loss: 0.11665 test_loss: 0.13109 \n",
      "[ 47/300] train_loss: 0.10735 valid_loss: 0.11329 test_loss: 0.12951 \n",
      "Validation loss decreased (0.114595 --> 0.113294).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10846 valid_loss: 0.11228 test_loss: 0.12994 \n",
      "Validation loss decreased (0.113294 --> 0.112275).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10844 valid_loss: 0.11352 test_loss: 0.12992 \n",
      "[ 50/300] train_loss: 0.10553 valid_loss: 0.11542 test_loss: 0.13051 \n",
      "[ 51/300] train_loss: 0.10442 valid_loss: 0.11185 test_loss: 0.12936 \n",
      "Validation loss decreased (0.112275 --> 0.111846).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10667 valid_loss: 0.11092 test_loss: 0.12663 \n",
      "Validation loss decreased (0.111846 --> 0.110917).  Saving model ...\n",
      "[ 53/300] train_loss: 0.10245 valid_loss: 0.10961 test_loss: 0.12591 \n",
      "Validation loss decreased (0.110917 --> 0.109614).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10189 valid_loss: 0.11310 test_loss: 0.12727 \n",
      "[ 55/300] train_loss: 0.10346 valid_loss: 0.11047 test_loss: 0.12560 \n",
      "[ 56/300] train_loss: 0.10331 valid_loss: 0.10845 test_loss: 0.12625 \n",
      "Validation loss decreased (0.109614 --> 0.108454).  Saving model ...\n",
      "[ 57/300] train_loss: 0.10129 valid_loss: 0.11074 test_loss: 0.12652 \n",
      "[ 58/300] train_loss: 0.10024 valid_loss: 0.10743 test_loss: 0.12397 \n",
      "Validation loss decreased (0.108454 --> 0.107426).  Saving model ...\n",
      "[ 59/300] train_loss: 0.10088 valid_loss: 0.10890 test_loss: 0.12584 \n",
      "[ 60/300] train_loss: 0.10037 valid_loss: 0.10729 test_loss: 0.12310 \n",
      "Validation loss decreased (0.107426 --> 0.107288).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09944 valid_loss: 0.10651 test_loss: 0.12207 \n",
      "Validation loss decreased (0.107288 --> 0.106509).  Saving model ...\n",
      "[ 62/300] train_loss: 0.10000 valid_loss: 0.11110 test_loss: 0.12378 \n",
      "[ 63/300] train_loss: 0.09979 valid_loss: 0.11024 test_loss: 0.12335 \n",
      "[ 64/300] train_loss: 0.10225 valid_loss: 0.10950 test_loss: 0.12205 \n",
      "[ 65/300] train_loss: 0.09833 valid_loss: 0.10754 test_loss: 0.11957 \n",
      "[ 66/300] train_loss: 0.09866 valid_loss: 0.10708 test_loss: 0.12013 \n",
      "[ 67/300] train_loss: 0.10245 valid_loss: 0.10411 test_loss: 0.11915 \n",
      "Validation loss decreased (0.106509 --> 0.104109).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09839 valid_loss: 0.10688 test_loss: 0.12030 \n",
      "[ 69/300] train_loss: 0.09903 valid_loss: 0.10476 test_loss: 0.12140 \n",
      "[ 70/300] train_loss: 0.09916 valid_loss: 0.10285 test_loss: 0.11996 \n",
      "Validation loss decreased (0.104109 --> 0.102848).  Saving model ...\n",
      "[ 71/300] train_loss: 0.09800 valid_loss: 0.10315 test_loss: 0.12142 \n",
      "[ 72/300] train_loss: 0.09431 valid_loss: 0.10488 test_loss: 0.12046 \n",
      "[ 73/300] train_loss: 0.09624 valid_loss: 0.10433 test_loss: 0.11914 \n",
      "[ 74/300] train_loss: 0.09965 valid_loss: 0.10233 test_loss: 0.11819 \n",
      "Validation loss decreased (0.102848 --> 0.102331).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09499 valid_loss: 0.10424 test_loss: 0.11947 \n",
      "[ 76/300] train_loss: 0.09772 valid_loss: 0.10179 test_loss: 0.11672 \n",
      "Validation loss decreased (0.102331 --> 0.101793).  Saving model ...\n",
      "[ 77/300] train_loss: 0.09714 valid_loss: 0.10240 test_loss: 0.11850 \n",
      "[ 78/300] train_loss: 0.09436 valid_loss: 0.09959 test_loss: 0.11626 \n",
      "Validation loss decreased (0.101793 --> 0.099585).  Saving model ...\n",
      "[ 79/300] train_loss: 0.09244 valid_loss: 0.10154 test_loss: 0.11782 \n",
      "[ 80/300] train_loss: 0.09524 valid_loss: 0.10323 test_loss: 0.11652 \n",
      "[ 81/300] train_loss: 0.09506 valid_loss: 0.09881 test_loss: 0.11588 \n",
      "Validation loss decreased (0.099585 --> 0.098815).  Saving model ...\n",
      "[ 82/300] train_loss: 0.09710 valid_loss: 0.10028 test_loss: 0.11598 \n",
      "[ 83/300] train_loss: 0.09323 valid_loss: 0.10483 test_loss: 0.11771 \n",
      "[ 84/300] train_loss: 0.09251 valid_loss: 0.10049 test_loss: 0.11691 \n",
      "[ 85/300] train_loss: 0.09190 valid_loss: 0.09922 test_loss: 0.11562 \n",
      "[ 86/300] train_loss: 0.09170 valid_loss: 0.10237 test_loss: 0.11585 \n",
      "[ 87/300] train_loss: 0.09144 valid_loss: 0.10065 test_loss: 0.11509 \n",
      "[ 88/300] train_loss: 0.09223 valid_loss: 0.09919 test_loss: 0.11395 \n",
      "[ 89/300] train_loss: 0.09203 valid_loss: 0.10010 test_loss: 0.11461 \n",
      "[ 90/300] train_loss: 0.09138 valid_loss: 0.09834 test_loss: 0.11454 \n",
      "Validation loss decreased (0.098815 --> 0.098342).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08943 valid_loss: 0.10093 test_loss: 0.11580 \n",
      "[ 92/300] train_loss: 0.09148 valid_loss: 0.09842 test_loss: 0.11452 \n",
      "[ 93/300] train_loss: 0.09122 valid_loss: 0.09758 test_loss: 0.11367 \n",
      "Validation loss decreased (0.098342 --> 0.097576).  Saving model ...\n",
      "[ 94/300] train_loss: 0.09093 valid_loss: 0.10105 test_loss: 0.11320 \n",
      "[122/300] train_loss: 0.08452 valid_loss: 0.09723 test_loss: 0.10967 \n",
      "[123/300] train_loss: 0.08291 valid_loss: 0.09476 test_loss: 0.10797 \n",
      "[124/300] train_loss: 0.08465 valid_loss: 0.09364 test_loss: 0.10645 \n",
      "[125/300] train_loss: 0.08454 valid_loss: 0.09624 test_loss: 0.10764 \n",
      "[126/300] train_loss: 0.08568 valid_loss: 0.09505 test_loss: 0.10658 \n",
      "[127/300] train_loss: 0.08402 valid_loss: 0.09159 test_loss: 0.10680 \n",
      "Validation loss decreased (0.092907 --> 0.091595).  Saving model ...\n",
      "[128/300] train_loss: 0.08381 valid_loss: 0.09352 test_loss: 0.10691 \n",
      "[129/300] train_loss: 0.08720 valid_loss: 0.09345 test_loss: 0.10701 \n",
      "[130/300] train_loss: 0.08318 valid_loss: 0.09328 test_loss: 0.10516 \n",
      "[131/300] train_loss: 0.08489 valid_loss: 0.09314 test_loss: 0.10597 \n",
      "[132/300] train_loss: 0.08544 valid_loss: 0.09287 test_loss: 0.10761 \n",
      "[133/300] train_loss: 0.08488 valid_loss: 0.09156 test_loss: 0.10523 \n",
      "Validation loss decreased (0.091595 --> 0.091565).  Saving model ...\n",
      "[134/300] train_loss: 0.08238 valid_loss: 0.09452 test_loss: 0.10614 \n",
      "[135/300] train_loss: 0.08626 valid_loss: 0.09667 test_loss: 0.10631 \n",
      "[136/300] train_loss: 0.08202 valid_loss: 0.09435 test_loss: 0.10507 \n",
      "[137/300] train_loss: 0.08544 valid_loss: 0.09320 test_loss: 0.10603 \n",
      "[138/300] train_loss: 0.08277 valid_loss: 0.09312 test_loss: 0.10482 \n",
      "[139/300] train_loss: 0.08159 valid_loss: 0.09144 test_loss: 0.10521 \n",
      "Validation loss decreased (0.091565 --> 0.091443).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140/300] train_loss: 0.08284 valid_loss: 0.09010 test_loss: 0.10467 \n",
      "Validation loss decreased (0.091443 --> 0.090105).  Saving model ...\n",
      "[141/300] train_loss: 0.08004 valid_loss: 0.09062 test_loss: 0.10362 \n",
      "[142/300] train_loss: 0.08285 valid_loss: 0.09855 test_loss: 0.10466 \n",
      "[143/300] train_loss: 0.08247 valid_loss: 0.09058 test_loss: 0.10355 \n",
      "[144/300] train_loss: 0.08044 valid_loss: 0.09168 test_loss: 0.10463 \n",
      "[145/300] train_loss: 0.08116 valid_loss: 0.08942 test_loss: 0.10428 \n",
      "Validation loss decreased (0.090105 --> 0.089418).  Saving model ...\n",
      "[146/300] train_loss: 0.08216 valid_loss: 0.09052 test_loss: 0.10331 \n",
      "[147/300] train_loss: 0.08156 valid_loss: 0.09127 test_loss: 0.10414 \n",
      "[148/300] train_loss: 0.08358 valid_loss: 0.08946 test_loss: 0.10317 \n",
      "[149/300] train_loss: 0.08142 valid_loss: 0.08991 test_loss: 0.10324 \n",
      "[150/300] train_loss: 0.08120 valid_loss: 0.09202 test_loss: 0.10354 \n",
      "[151/300] train_loss: 0.07991 valid_loss: 0.09563 test_loss: 0.10402 \n",
      "[152/300] train_loss: 0.08347 valid_loss: 0.09483 test_loss: 0.10351 \n",
      "[153/300] train_loss: 0.08005 valid_loss: 0.09152 test_loss: 0.10498 \n",
      "[154/300] train_loss: 0.07754 valid_loss: 0.09192 test_loss: 0.10350 \n",
      "[155/300] train_loss: 0.08005 valid_loss: 0.08891 test_loss: 0.10241 \n",
      "Validation loss decreased (0.089418 --> 0.088907).  Saving model ...\n",
      "[156/300] train_loss: 0.07876 valid_loss: 0.09227 test_loss: 0.10402 \n",
      "[157/300] train_loss: 0.07877 valid_loss: 0.08916 test_loss: 0.10214 \n",
      "[158/300] train_loss: 0.08153 valid_loss: 0.09048 test_loss: 0.10185 \n",
      "[159/300] train_loss: 0.07916 valid_loss: 0.09072 test_loss: 0.10192 \n",
      "[160/300] train_loss: 0.07898 valid_loss: 0.08759 test_loss: 0.10198 \n",
      "Validation loss decreased (0.088907 --> 0.087593).  Saving model ...\n",
      "[161/300] train_loss: 0.07824 valid_loss: 0.09039 test_loss: 0.10381 \n",
      "[162/300] train_loss: 0.07872 valid_loss: 0.08887 test_loss: 0.10190 \n",
      "[163/300] train_loss: 0.08066 valid_loss: 0.08774 test_loss: 0.10098 \n",
      "[164/300] train_loss: 0.07901 valid_loss: 0.09304 test_loss: 0.10220 \n",
      "[165/300] train_loss: 0.07739 valid_loss: 0.08972 test_loss: 0.10102 \n",
      "[166/300] train_loss: 0.08093 valid_loss: 0.09372 test_loss: 0.10225 \n",
      "[167/300] train_loss: 0.07999 valid_loss: 0.08764 test_loss: 0.10091 \n",
      "[168/300] train_loss: 0.07816 valid_loss: 0.08745 test_loss: 0.10046 \n",
      "Validation loss decreased (0.087593 --> 0.087453).  Saving model ...\n",
      "[169/300] train_loss: 0.08063 valid_loss: 0.08879 test_loss: 0.10153 \n",
      "[170/300] train_loss: 0.07997 valid_loss: 0.10268 test_loss: 0.10158 \n",
      "[171/300] train_loss: 0.07974 valid_loss: 0.09053 test_loss: 0.10086 \n",
      "[172/300] train_loss: 0.07793 valid_loss: 0.08897 test_loss: 0.10036 \n",
      "[173/300] train_loss: 0.07540 valid_loss: 0.08853 test_loss: 0.10054 \n",
      "[174/300] train_loss: 0.07991 valid_loss: 0.08699 test_loss: 0.10038 \n",
      "Validation loss decreased (0.087453 --> 0.086991).  Saving model ...\n",
      "[175/300] train_loss: 0.07625 valid_loss: 0.09048 test_loss: 0.10119 \n",
      "[176/300] train_loss: 0.07575 valid_loss: 0.09213 test_loss: 0.10040 \n",
      "[177/300] train_loss: 0.07859 valid_loss: 0.09167 test_loss: 0.10082 \n",
      "[178/300] train_loss: 0.07794 valid_loss: 0.08875 test_loss: 0.10060 \n",
      "[179/300] train_loss: 0.08103 valid_loss: 0.08961 test_loss: 0.10110 \n",
      "[180/300] train_loss: 0.07612 valid_loss: 0.08728 test_loss: 0.10047 \n",
      "[181/300] train_loss: 0.07800 valid_loss: 0.08844 test_loss: 0.10011 \n",
      "[182/300] train_loss: 0.07740 valid_loss: 0.08770 test_loss: 0.10196 \n",
      "[183/300] train_loss: 0.07773 valid_loss: 0.08803 test_loss: 0.10022 \n",
      "[184/300] train_loss: 0.07803 valid_loss: 0.09076 test_loss: 0.10022 \n",
      "[185/300] train_loss: 0.07990 valid_loss: 0.08627 test_loss: 0.10136 \n",
      "Validation loss decreased (0.086991 --> 0.086271).  Saving model ...\n",
      "[186/300] train_loss: 0.07932 valid_loss: 0.08862 test_loss: 0.10086 \n",
      "[187/300] train_loss: 0.07620 valid_loss: 0.09164 test_loss: 0.10179 \n",
      "[188/300] train_loss: 0.07633 valid_loss: 0.09136 test_loss: 0.10049 \n",
      "[189/300] train_loss: 0.07691 valid_loss: 0.08631 test_loss: 0.09951 \n",
      "[190/300] train_loss: 0.07932 valid_loss: 0.08831 test_loss: 0.10053 \n",
      "[191/300] train_loss: 0.07687 valid_loss: 0.08827 test_loss: 0.09995 \n",
      "[192/300] train_loss: 0.07995 valid_loss: 0.08895 test_loss: 0.09987 \n",
      "[193/300] train_loss: 0.07567 valid_loss: 0.08595 test_loss: 0.09991 \n",
      "Validation loss decreased (0.086271 --> 0.085954).  Saving model ...\n",
      "[194/300] train_loss: 0.07665 valid_loss: 0.08784 test_loss: 0.09983 \n",
      "[195/300] train_loss: 0.07686 valid_loss: 0.08856 test_loss: 0.09879 \n",
      "[196/300] train_loss: 0.07407 valid_loss: 0.08946 test_loss: 0.09905 \n",
      "[197/300] train_loss: 0.07557 valid_loss: 0.09642 test_loss: 0.09935 \n",
      "[198/300] train_loss: 0.07587 valid_loss: 0.09028 test_loss: 0.10089 \n",
      "[199/300] train_loss: 0.07533 valid_loss: 0.09314 test_loss: 0.09861 \n",
      "[200/300] train_loss: 0.07512 valid_loss: 0.08901 test_loss: 0.09908 \n",
      "[201/300] train_loss: 0.07548 valid_loss: 0.08914 test_loss: 0.09854 \n",
      "[202/300] train_loss: 0.07441 valid_loss: 0.09139 test_loss: 0.09827 \n",
      "[203/300] train_loss: 0.07251 valid_loss: 0.08910 test_loss: 0.09945 \n",
      "[204/300] train_loss: 0.07634 valid_loss: 0.08890 test_loss: 0.09844 \n",
      "[205/300] train_loss: 0.07511 valid_loss: 0.09002 test_loss: 0.09861 \n",
      "[206/300] train_loss: 0.07403 valid_loss: 0.09374 test_loss: 0.09821 \n",
      "[207/300] train_loss: 0.07464 valid_loss: 0.09715 test_loss: 0.09873 \n",
      "[208/300] train_loss: 0.07496 valid_loss: 0.09189 test_loss: 0.09883 \n",
      "[209/300] train_loss: 0.07465 valid_loss: 0.10055 test_loss: 0.09769 \n",
      "[210/300] train_loss: 0.07441 valid_loss: 0.10029 test_loss: 0.09835 \n",
      "[211/300] train_loss: 0.07301 valid_loss: 0.09958 test_loss: 0.09741 \n",
      "[212/300] train_loss: 0.07444 valid_loss: 0.09320 test_loss: 0.09829 \n",
      "[213/300] train_loss: 0.07639 valid_loss: 0.09575 test_loss: 0.09894 \n",
      "[214/300] train_loss: 0.07340 valid_loss: 0.09159 test_loss: 0.09781 \n",
      "[215/300] train_loss: 0.07484 valid_loss: 0.08983 test_loss: 0.09826 \n",
      "[216/300] train_loss: 0.07547 valid_loss: 0.08897 test_loss: 0.09751 \n",
      "[217/300] train_loss: 0.07542 valid_loss: 0.08978 test_loss: 0.09739 \n",
      "[218/300] train_loss: 0.07387 valid_loss: 0.08916 test_loss: 0.09809 \n",
      "[219/300] train_loss: 0.07405 valid_loss: 0.09865 test_loss: 0.09719 \n",
      "[220/300] train_loss: 0.07371 valid_loss: 0.09010 test_loss: 0.09669 \n",
      "[221/300] train_loss: 0.07538 valid_loss: 0.09446 test_loss: 0.09644 \n",
      "[222/300] train_loss: 0.07503 valid_loss: 0.08435 test_loss: 0.09667 \n",
      "Validation loss decreased (0.085954 --> 0.084349).  Saving model ...\n",
      "[223/300] train_loss: 0.07451 valid_loss: 0.08796 test_loss: 0.09657 \n",
      "[224/300] train_loss: 0.07333 valid_loss: 0.08798 test_loss: 0.09725 \n",
      "[225/300] train_loss: 0.07358 valid_loss: 0.09424 test_loss: 0.09718 \n",
      "[226/300] train_loss: 0.07644 valid_loss: 0.08747 test_loss: 0.09791 \n",
      "[227/300] train_loss: 0.07371 valid_loss: 0.08815 test_loss: 0.09825 \n",
      "[228/300] train_loss: 0.07449 valid_loss: 0.08614 test_loss: 0.09709 \n",
      "[229/300] train_loss: 0.07436 valid_loss: 0.09112 test_loss: 0.09823 \n",
      "[230/300] train_loss: 0.07516 valid_loss: 0.08950 test_loss: 0.09778 \n",
      "[231/300] train_loss: 0.07406 valid_loss: 0.09255 test_loss: 0.09812 \n",
      "[232/300] train_loss: 0.07280 valid_loss: 0.08423 test_loss: 0.09741 \n",
      "Validation loss decreased (0.084349 --> 0.084228).  Saving model ...\n",
      "[233/300] train_loss: 0.07230 valid_loss: 0.08526 test_loss: 0.09648 \n",
      "[234/300] train_loss: 0.07288 valid_loss: 0.08426 test_loss: 0.09659 \n",
      "[235/300] train_loss: 0.07325 valid_loss: 0.08475 test_loss: 0.09693 \n",
      "[236/300] train_loss: 0.07198 valid_loss: 0.08679 test_loss: 0.09613 \n",
      "[237/300] train_loss: 0.07226 valid_loss: 0.08934 test_loss: 0.09774 \n",
      "[238/300] train_loss: 0.07315 valid_loss: 0.08624 test_loss: 0.09599 \n",
      "[239/300] train_loss: 0.07137 valid_loss: 0.08724 test_loss: 0.09620 \n",
      "[240/300] train_loss: 0.07164 valid_loss: 0.08271 test_loss: 0.09677 \n",
      "Validation loss decreased (0.084228 --> 0.082714).  Saving model ...\n",
      "[241/300] train_loss: 0.07356 valid_loss: 0.08387 test_loss: 0.09571 \n",
      "[242/300] train_loss: 0.07192 valid_loss: 0.08405 test_loss: 0.09612 \n",
      "[243/300] train_loss: 0.07045 valid_loss: 0.08522 test_loss: 0.09585 \n",
      "[244/300] train_loss: 0.07175 valid_loss: 0.08534 test_loss: 0.09560 \n",
      "[245/300] train_loss: 0.07077 valid_loss: 0.08530 test_loss: 0.09568 \n",
      "[246/300] train_loss: 0.07101 valid_loss: 0.08915 test_loss: 0.09539 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247/300] train_loss: 0.07276 valid_loss: 0.08570 test_loss: 0.09453 \n",
      "[248/300] train_loss: 0.06980 valid_loss: 0.08976 test_loss: 0.09490 \n",
      "[249/300] train_loss: 0.07201 valid_loss: 0.08874 test_loss: 0.09524 \n",
      "[250/300] train_loss: 0.06974 valid_loss: 0.08662 test_loss: 0.09576 \n",
      "[251/300] train_loss: 0.07275 valid_loss: 0.09515 test_loss: 0.09548 \n",
      "[252/300] train_loss: 0.07222 valid_loss: 0.08674 test_loss: 0.09538 \n",
      "[253/300] train_loss: 0.07068 valid_loss: 0.08997 test_loss: 0.09531 \n",
      "[254/300] train_loss: 0.07255 valid_loss: 0.08671 test_loss: 0.09523 \n",
      "[255/300] train_loss: 0.07279 valid_loss: 0.09134 test_loss: 0.09602 \n",
      "[256/300] train_loss: 0.07174 valid_loss: 0.08273 test_loss: 0.09575 \n",
      "[257/300] train_loss: 0.07489 valid_loss: 0.08346 test_loss: 0.09450 \n",
      "[258/300] train_loss: 0.07077 valid_loss: 0.08565 test_loss: 0.09444 \n",
      "[259/300] train_loss: 0.07170 valid_loss: 0.08256 test_loss: 0.09465 \n",
      "Validation loss decreased (0.082714 --> 0.082556).  Saving model ...\n",
      "[260/300] train_loss: 0.06971 valid_loss: 0.08812 test_loss: 0.09507 \n",
      "[261/300] train_loss: 0.07207 valid_loss: 0.08398 test_loss: 0.09518 \n",
      "[262/300] train_loss: 0.07283 valid_loss: 0.08778 test_loss: 0.09462 \n",
      "[263/300] train_loss: 0.06998 valid_loss: 0.08382 test_loss: 0.09536 \n",
      "[264/300] train_loss: 0.06931 valid_loss: 0.09036 test_loss: 0.09532 \n",
      "[265/300] train_loss: 0.07151 valid_loss: 0.08165 test_loss: 0.09335 \n",
      "Validation loss decreased (0.082556 --> 0.081646).  Saving model ...\n",
      "[266/300] train_loss: 0.07021 valid_loss: 0.08848 test_loss: 0.09469 \n",
      "[267/300] train_loss: 0.07263 valid_loss: 0.08384 test_loss: 0.09357 \n",
      "[268/300] train_loss: 0.06835 valid_loss: 0.08687 test_loss: 0.09490 \n",
      "[269/300] train_loss: 0.06954 valid_loss: 0.08830 test_loss: 0.09615 \n",
      "[270/300] train_loss: 0.06876 valid_loss: 0.08687 test_loss: 0.09486 \n",
      "[271/300] train_loss: 0.07058 valid_loss: 0.08427 test_loss: 0.09406 \n",
      "[272/300] train_loss: 0.06780 valid_loss: 0.08136 test_loss: 0.09416 \n",
      "Validation loss decreased (0.081646 --> 0.081357).  Saving model ...\n",
      "[273/300] train_loss: 0.07162 valid_loss: 0.08547 test_loss: 0.09304 \n",
      "[274/300] train_loss: 0.06972 valid_loss: 0.08341 test_loss: 0.09481 \n",
      "[275/300] train_loss: 0.07198 valid_loss: 0.08258 test_loss: 0.09376 \n",
      "[276/300] train_loss: 0.07012 valid_loss: 0.08064 test_loss: 0.09400 \n",
      "Validation loss decreased (0.081357 --> 0.080639).  Saving model ...\n",
      "[277/300] train_loss: 0.07078 valid_loss: 0.08114 test_loss: 0.09418 \n",
      "[278/300] train_loss: 0.06851 valid_loss: 0.08689 test_loss: 0.09526 \n",
      "[279/300] train_loss: 0.07038 valid_loss: 0.08225 test_loss: 0.09357 \n",
      "[280/300] train_loss: 0.07070 valid_loss: 0.08239 test_loss: 0.09587 \n",
      "[281/300] train_loss: 0.07089 valid_loss: 0.08289 test_loss: 0.09449 \n",
      "[282/300] train_loss: 0.07184 valid_loss: 0.08356 test_loss: 0.09374 \n",
      "[283/300] train_loss: 0.06930 valid_loss: 0.08164 test_loss: 0.09301 \n",
      "[284/300] train_loss: 0.06923 valid_loss: 0.08351 test_loss: 0.09442 \n",
      "[285/300] train_loss: 0.06997 valid_loss: 0.08884 test_loss: 0.09492 \n",
      "[286/300] train_loss: 0.07040 valid_loss: 0.08034 test_loss: 0.09435 \n",
      "Validation loss decreased (0.080639 --> 0.080342).  Saving model ...\n",
      "[287/300] train_loss: 0.07082 valid_loss: 0.08468 test_loss: 0.09415 \n",
      "[288/300] train_loss: 0.07114 valid_loss: 0.08215 test_loss: 0.09377 \n",
      "[289/300] train_loss: 0.06939 valid_loss: 0.08435 test_loss: 0.09454 \n",
      "[290/300] train_loss: 0.07030 valid_loss: 0.09248 test_loss: 0.09436 \n",
      "[291/300] train_loss: 0.06804 valid_loss: 0.08820 test_loss: 0.09465 \n",
      "[292/300] train_loss: 0.06994 valid_loss: 0.08067 test_loss: 0.09423 \n",
      "[293/300] train_loss: 0.06943 valid_loss: 0.08226 test_loss: 0.09433 \n",
      "[294/300] train_loss: 0.06935 valid_loss: 0.08361 test_loss: 0.09367 \n",
      "[295/300] train_loss: 0.06977 valid_loss: 0.08531 test_loss: 0.09491 \n",
      "[296/300] train_loss: 0.06884 valid_loss: 0.09902 test_loss: 0.09480 \n",
      "[297/300] train_loss: 0.06805 valid_loss: 0.08313 test_loss: 0.09577 \n",
      "[298/300] train_loss: 0.06865 valid_loss: 0.08757 test_loss: 0.09491 \n",
      "[299/300] train_loss: 0.06837 valid_loss: 0.08541 test_loss: 0.09479 \n",
      "[300/300] train_loss: 0.06862 valid_loss: 0.08331 test_loss: 0.09369 \n",
      "TRAINING MODEL 4\n",
      "[  1/300] train_loss: 0.61375 valid_loss: 0.53655 test_loss: 0.54001 \n",
      "Validation loss decreased (inf --> 0.536552).  Saving model ...\n",
      "[  2/300] train_loss: 0.45027 valid_loss: 0.39458 test_loss: 0.39917 \n",
      "Validation loss decreased (0.536552 --> 0.394580).  Saving model ...\n",
      "[  3/300] train_loss: 0.35086 valid_loss: 0.33538 test_loss: 0.34010 \n",
      "Validation loss decreased (0.394580 --> 0.335378).  Saving model ...\n",
      "[  4/300] train_loss: 0.29837 valid_loss: 0.28995 test_loss: 0.29774 \n",
      "Validation loss decreased (0.335378 --> 0.289953).  Saving model ...\n",
      "[  5/300] train_loss: 0.26223 valid_loss: 0.25424 test_loss: 0.26296 \n",
      "Validation loss decreased (0.289953 --> 0.254238).  Saving model ...\n",
      "[  6/300] train_loss: 0.23198 valid_loss: 0.22858 test_loss: 0.23953 \n",
      "Validation loss decreased (0.254238 --> 0.228578).  Saving model ...\n",
      "[  7/300] train_loss: 0.20780 valid_loss: 0.20952 test_loss: 0.22051 \n",
      "Validation loss decreased (0.228578 --> 0.209522).  Saving model ...\n",
      "[  8/300] train_loss: 0.19584 valid_loss: 0.19577 test_loss: 0.20790 \n",
      "Validation loss decreased (0.209522 --> 0.195769).  Saving model ...\n",
      "[  9/300] train_loss: 0.18464 valid_loss: 0.18562 test_loss: 0.19853 \n",
      "Validation loss decreased (0.195769 --> 0.185619).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17367 valid_loss: 0.17750 test_loss: 0.19068 \n",
      "Validation loss decreased (0.185619 --> 0.177497).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16090 valid_loss: 0.16834 test_loss: 0.18160 \n",
      "Validation loss decreased (0.177497 --> 0.168342).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15800 valid_loss: 0.16364 test_loss: 0.17626 \n",
      "Validation loss decreased (0.168342 --> 0.163637).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15466 valid_loss: 0.16164 test_loss: 0.17204 \n",
      "Validation loss decreased (0.163637 --> 0.161637).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15068 valid_loss: 0.15549 test_loss: 0.16685 \n",
      "Validation loss decreased (0.161637 --> 0.155491).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14431 valid_loss: 0.15411 test_loss: 0.16545 \n",
      "Validation loss decreased (0.155491 --> 0.154105).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14372 valid_loss: 0.14981 test_loss: 0.16234 \n",
      "Validation loss decreased (0.154105 --> 0.149810).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13854 valid_loss: 0.14797 test_loss: 0.16060 \n",
      "Validation loss decreased (0.149810 --> 0.147968).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13692 valid_loss: 0.14350 test_loss: 0.15772 \n",
      "Validation loss decreased (0.147968 --> 0.143503).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13523 valid_loss: 0.14100 test_loss: 0.15614 \n",
      "Validation loss decreased (0.143503 --> 0.140997).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13111 valid_loss: 0.13978 test_loss: 0.15335 \n",
      "Validation loss decreased (0.140997 --> 0.139782).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13416 valid_loss: 0.14011 test_loss: 0.15388 \n",
      "[ 22/300] train_loss: 0.12787 valid_loss: 0.13804 test_loss: 0.15487 \n",
      "Validation loss decreased (0.139782 --> 0.138041).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12753 valid_loss: 0.13797 test_loss: 0.15099 \n",
      "Validation loss decreased (0.138041 --> 0.137974).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12342 valid_loss: 0.13669 test_loss: 0.14945 \n",
      "Validation loss decreased (0.137974 --> 0.136691).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12523 valid_loss: 0.12929 test_loss: 0.14669 \n",
      "Validation loss decreased (0.136691 --> 0.129294).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12329 valid_loss: 0.12979 test_loss: 0.14630 \n",
      "[ 27/300] train_loss: 0.12161 valid_loss: 0.13218 test_loss: 0.14696 \n",
      "[ 28/300] train_loss: 0.11788 valid_loss: 0.12831 test_loss: 0.14351 \n",
      "Validation loss decreased (0.129294 --> 0.128307).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12057 valid_loss: 0.12861 test_loss: 0.14415 \n",
      "[ 30/300] train_loss: 0.11554 valid_loss: 0.12710 test_loss: 0.14240 \n",
      "Validation loss decreased (0.128307 --> 0.127100).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11837 valid_loss: 0.12505 test_loss: 0.14164 \n",
      "Validation loss decreased (0.127100 --> 0.125050).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11818 valid_loss: 0.12542 test_loss: 0.14142 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 33/300] train_loss: 0.11604 valid_loss: 0.12290 test_loss: 0.13932 \n",
      "Validation loss decreased (0.125050 --> 0.122896).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11517 valid_loss: 0.12103 test_loss: 0.13772 \n",
      "Validation loss decreased (0.122896 --> 0.121027).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11379 valid_loss: 0.12296 test_loss: 0.13598 \n",
      "[ 36/300] train_loss: 0.11471 valid_loss: 0.12042 test_loss: 0.13584 \n",
      "Validation loss decreased (0.121027 --> 0.120423).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11420 valid_loss: 0.12156 test_loss: 0.13709 \n",
      "[ 38/300] train_loss: 0.11300 valid_loss: 0.12046 test_loss: 0.13416 \n",
      "[ 39/300] train_loss: 0.11035 valid_loss: 0.11968 test_loss: 0.13436 \n",
      "Validation loss decreased (0.120423 --> 0.119681).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11077 valid_loss: 0.12012 test_loss: 0.13186 \n",
      "[ 41/300] train_loss: 0.10857 valid_loss: 0.11798 test_loss: 0.13372 \n",
      "Validation loss decreased (0.119681 --> 0.117975).  Saving model ...\n",
      "[ 42/300] train_loss: 0.11380 valid_loss: 0.11649 test_loss: 0.13506 \n",
      "Validation loss decreased (0.117975 --> 0.116492).  Saving model ...\n",
      "[ 43/300] train_loss: 0.11053 valid_loss: 0.11835 test_loss: 0.13074 \n",
      "[ 44/300] train_loss: 0.10773 valid_loss: 0.12071 test_loss: 0.13043 \n",
      "[ 45/300] train_loss: 0.10852 valid_loss: 0.11647 test_loss: 0.13086 \n",
      "Validation loss decreased (0.116492 --> 0.116473).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10805 valid_loss: 0.11662 test_loss: 0.12894 \n",
      "[ 47/300] train_loss: 0.10345 valid_loss: 0.11378 test_loss: 0.12837 \n",
      "Validation loss decreased (0.116473 --> 0.113778).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10571 valid_loss: 0.11595 test_loss: 0.13099 \n",
      "[ 49/300] train_loss: 0.10827 valid_loss: 0.11411 test_loss: 0.12815 \n",
      "[ 50/300] train_loss: 0.10314 valid_loss: 0.11358 test_loss: 0.12790 \n",
      "Validation loss decreased (0.113778 --> 0.113585).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10334 valid_loss: 0.11350 test_loss: 0.12615 \n",
      "Validation loss decreased (0.113585 --> 0.113503).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10608 valid_loss: 0.11422 test_loss: 0.12653 \n",
      "[ 53/300] train_loss: 0.10695 valid_loss: 0.11308 test_loss: 0.12509 \n",
      "Validation loss decreased (0.113503 --> 0.113081).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10522 valid_loss: 0.11267 test_loss: 0.12541 \n",
      "Validation loss decreased (0.113081 --> 0.112670).  Saving model ...\n",
      "[ 55/300] train_loss: 0.09976 valid_loss: 0.11530 test_loss: 0.12579 \n",
      "[ 56/300] train_loss: 0.10090 valid_loss: 0.11569 test_loss: 0.12398 \n",
      "[ 57/300] train_loss: 0.10247 valid_loss: 0.11210 test_loss: 0.12564 \n",
      "Validation loss decreased (0.112670 --> 0.112096).  Saving model ...\n",
      "[ 58/300] train_loss: 0.10205 valid_loss: 0.10761 test_loss: 0.12135 \n",
      "Validation loss decreased (0.112096 --> 0.107613).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09920 valid_loss: 0.10570 test_loss: 0.12071 \n",
      "Validation loss decreased (0.107613 --> 0.105698).  Saving model ...\n",
      "[ 60/300] train_loss: 0.10144 valid_loss: 0.10975 test_loss: 0.12146 \n",
      "[ 61/300] train_loss: 0.10084 valid_loss: 0.11143 test_loss: 0.12009 \n",
      "[ 62/300] train_loss: 0.09637 valid_loss: 0.10906 test_loss: 0.11948 \n",
      "[ 63/300] train_loss: 0.10095 valid_loss: 0.11072 test_loss: 0.11906 \n",
      "[ 64/300] train_loss: 0.10067 valid_loss: 0.10517 test_loss: 0.12025 \n",
      "Validation loss decreased (0.105698 --> 0.105166).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09688 valid_loss: 0.10493 test_loss: 0.11984 \n",
      "Validation loss decreased (0.105166 --> 0.104930).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09752 valid_loss: 0.10482 test_loss: 0.11822 \n",
      "Validation loss decreased (0.104930 --> 0.104819).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09754 valid_loss: 0.11138 test_loss: 0.11832 \n",
      "[ 68/300] train_loss: 0.09889 valid_loss: 0.10410 test_loss: 0.11780 \n",
      "Validation loss decreased (0.104819 --> 0.104102).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09579 valid_loss: 0.10266 test_loss: 0.11759 \n",
      "Validation loss decreased (0.104102 --> 0.102658).  Saving model ...\n",
      "[ 70/300] train_loss: 0.09868 valid_loss: 0.10272 test_loss: 0.11671 \n",
      "[ 71/300] train_loss: 0.09626 valid_loss: 0.10375 test_loss: 0.11694 \n",
      "[ 72/300] train_loss: 0.09726 valid_loss: 0.10434 test_loss: 0.11628 \n",
      "[ 73/300] train_loss: 0.09326 valid_loss: 0.10305 test_loss: 0.11661 \n",
      "[ 74/300] train_loss: 0.09422 valid_loss: 0.10697 test_loss: 0.11509 \n",
      "[ 75/300] train_loss: 0.09461 valid_loss: 0.10161 test_loss: 0.11715 \n",
      "Validation loss decreased (0.102658 --> 0.101606).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09508 valid_loss: 0.09946 test_loss: 0.11476 \n",
      "Validation loss decreased (0.101606 --> 0.099457).  Saving model ...\n",
      "[ 77/300] train_loss: 0.09542 valid_loss: 0.10054 test_loss: 0.11520 \n",
      "[ 78/300] train_loss: 0.09267 valid_loss: 0.10149 test_loss: 0.11374 \n",
      "[ 79/300] train_loss: 0.09089 valid_loss: 0.10800 test_loss: 0.11402 \n",
      "[ 80/300] train_loss: 0.09368 valid_loss: 0.09906 test_loss: 0.11514 \n",
      "Validation loss decreased (0.099457 --> 0.099060).  Saving model ...\n",
      "[ 81/300] train_loss: 0.09203 valid_loss: 0.09997 test_loss: 0.11508 \n",
      "[ 82/300] train_loss: 0.09339 valid_loss: 0.10604 test_loss: 0.11359 \n",
      "[ 83/300] train_loss: 0.09102 valid_loss: 0.09761 test_loss: 0.11337 \n",
      "Validation loss decreased (0.099060 --> 0.097609).  Saving model ...\n",
      "[ 84/300] train_loss: 0.09366 valid_loss: 0.09950 test_loss: 0.11158 \n",
      "[ 85/300] train_loss: 0.09312 valid_loss: 0.09891 test_loss: 0.11296 \n",
      "[ 86/300] train_loss: 0.09496 valid_loss: 0.10133 test_loss: 0.11140 \n",
      "[ 87/300] train_loss: 0.09003 valid_loss: 0.09905 test_loss: 0.11212 \n",
      "[ 88/300] train_loss: 0.09169 valid_loss: 0.10102 test_loss: 0.11183 \n",
      "[ 89/300] train_loss: 0.09232 valid_loss: 0.10458 test_loss: 0.11211 \n",
      "[ 90/300] train_loss: 0.09189 valid_loss: 0.10138 test_loss: 0.11174 \n",
      "[ 91/300] train_loss: 0.09116 valid_loss: 0.09769 test_loss: 0.11054 \n",
      "[ 92/300] train_loss: 0.08907 valid_loss: 0.09810 test_loss: 0.11195 \n",
      "[ 93/300] train_loss: 0.08831 valid_loss: 0.09999 test_loss: 0.11239 \n",
      "[ 94/300] train_loss: 0.09031 valid_loss: 0.10370 test_loss: 0.11030 \n",
      "[ 95/300] train_loss: 0.09009 valid_loss: 0.10216 test_loss: 0.11101 \n",
      "[ 96/300] train_loss: 0.08981 valid_loss: 0.10483 test_loss: 0.11165 \n",
      "[ 97/300] train_loss: 0.08939 valid_loss: 0.10391 test_loss: 0.11113 \n",
      "[ 98/300] train_loss: 0.08766 valid_loss: 0.09505 test_loss: 0.11019 \n",
      "Validation loss decreased (0.097609 --> 0.095049).  Saving model ...\n",
      "[ 99/300] train_loss: 0.08771 valid_loss: 0.09972 test_loss: 0.10950 \n",
      "[100/300] train_loss: 0.09017 valid_loss: 0.10008 test_loss: 0.11309 \n",
      "[101/300] train_loss: 0.08913 valid_loss: 0.09591 test_loss: 0.10795 \n",
      "[102/300] train_loss: 0.08837 valid_loss: 0.10388 test_loss: 0.10830 \n",
      "[103/300] train_loss: 0.08742 valid_loss: 0.10089 test_loss: 0.10932 \n",
      "[104/300] train_loss: 0.08921 valid_loss: 0.09837 test_loss: 0.10864 \n",
      "[105/300] train_loss: 0.08572 valid_loss: 0.09807 test_loss: 0.10766 \n",
      "[106/300] train_loss: 0.08876 valid_loss: 0.09390 test_loss: 0.10784 \n",
      "Validation loss decreased (0.095049 --> 0.093903).  Saving model ...\n",
      "[107/300] train_loss: 0.08628 valid_loss: 0.09352 test_loss: 0.10739 \n",
      "Validation loss decreased (0.093903 --> 0.093517).  Saving model ...\n",
      "[108/300] train_loss: 0.08727 valid_loss: 0.09516 test_loss: 0.10798 \n",
      "[109/300] train_loss: 0.08354 valid_loss: 0.10094 test_loss: 0.10800 \n",
      "[110/300] train_loss: 0.08529 valid_loss: 0.09504 test_loss: 0.10848 \n",
      "[111/300] train_loss: 0.08578 valid_loss: 0.09565 test_loss: 0.10701 \n",
      "[112/300] train_loss: 0.08673 valid_loss: 0.10106 test_loss: 0.10858 \n",
      "[113/300] train_loss: 0.08702 valid_loss: 0.10323 test_loss: 0.10716 \n",
      "[114/300] train_loss: 0.08446 valid_loss: 0.09591 test_loss: 0.10721 \n",
      "[115/300] train_loss: 0.08533 valid_loss: 0.10228 test_loss: 0.10762 \n",
      "[116/300] train_loss: 0.08803 valid_loss: 0.10156 test_loss: 0.10648 \n",
      "[117/300] train_loss: 0.08478 valid_loss: 0.10606 test_loss: 0.10687 \n",
      "[118/300] train_loss: 0.08547 valid_loss: 0.09374 test_loss: 0.10624 \n",
      "[119/300] train_loss: 0.08432 valid_loss: 0.09296 test_loss: 0.10645 \n",
      "Validation loss decreased (0.093517 --> 0.092963).  Saving model ...\n",
      "[120/300] train_loss: 0.08567 valid_loss: 0.09360 test_loss: 0.10731 \n",
      "[121/300] train_loss: 0.08369 valid_loss: 0.09180 test_loss: 0.10663 \n",
      "Validation loss decreased (0.092963 --> 0.091803).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122/300] train_loss: 0.08593 valid_loss: 0.10214 test_loss: 0.10600 \n",
      "[123/300] train_loss: 0.08674 valid_loss: 0.09851 test_loss: 0.10447 \n",
      "[124/300] train_loss: 0.08651 valid_loss: 0.09245 test_loss: 0.10604 \n",
      "[125/300] train_loss: 0.08349 valid_loss: 0.09350 test_loss: 0.10626 \n",
      "[126/300] train_loss: 0.08397 valid_loss: 0.09064 test_loss: 0.10529 \n",
      "Validation loss decreased (0.091803 --> 0.090640).  Saving model ...\n",
      "[127/300] train_loss: 0.08384 valid_loss: 0.09265 test_loss: 0.10549 \n",
      "[128/300] train_loss: 0.08327 valid_loss: 0.09394 test_loss: 0.10447 \n",
      "[129/300] train_loss: 0.08450 valid_loss: 0.09630 test_loss: 0.10361 \n",
      "[130/300] train_loss: 0.08116 valid_loss: 0.09909 test_loss: 0.10486 \n",
      "[131/300] train_loss: 0.08320 valid_loss: 0.09893 test_loss: 0.10436 \n",
      "[132/300] train_loss: 0.08334 valid_loss: 0.08972 test_loss: 0.10320 \n",
      "Validation loss decreased (0.090640 --> 0.089723).  Saving model ...\n",
      "[133/300] train_loss: 0.08575 valid_loss: 0.09984 test_loss: 0.10524 \n",
      "[134/300] train_loss: 0.08161 valid_loss: 0.09325 test_loss: 0.10451 \n",
      "[135/300] train_loss: 0.08293 valid_loss: 0.10170 test_loss: 0.10592 \n",
      "[136/300] train_loss: 0.08276 valid_loss: 0.09631 test_loss: 0.10532 \n",
      "[137/300] train_loss: 0.08074 valid_loss: 0.09193 test_loss: 0.10545 \n",
      "[138/300] train_loss: 0.08243 valid_loss: 0.09315 test_loss: 0.10366 \n",
      "[139/300] train_loss: 0.08322 valid_loss: 0.09044 test_loss: 0.10425 \n",
      "[140/300] train_loss: 0.07985 valid_loss: 0.09866 test_loss: 0.10333 \n",
      "[141/300] train_loss: 0.08170 valid_loss: 0.09454 test_loss: 0.10371 \n",
      "[142/300] train_loss: 0.08380 valid_loss: 0.09122 test_loss: 0.10269 \n",
      "[143/300] train_loss: 0.08252 valid_loss: 0.09396 test_loss: 0.10448 \n",
      "[144/300] train_loss: 0.08181 valid_loss: 0.09229 test_loss: 0.10333 \n",
      "[145/300] train_loss: 0.08193 valid_loss: 0.09763 test_loss: 0.10319 \n",
      "[146/300] train_loss: 0.08211 valid_loss: 0.09035 test_loss: 0.10447 \n",
      "[147/300] train_loss: 0.08248 valid_loss: 0.09243 test_loss: 0.10148 \n",
      "[148/300] train_loss: 0.08056 valid_loss: 0.09123 test_loss: 0.10148 \n",
      "[149/300] train_loss: 0.08052 valid_loss: 0.09000 test_loss: 0.10222 \n",
      "[150/300] train_loss: 0.07951 valid_loss: 0.09465 test_loss: 0.10297 \n",
      "[151/300] train_loss: 0.08098 valid_loss: 0.09341 test_loss: 0.10349 \n",
      "[152/300] train_loss: 0.08076 valid_loss: 0.09328 test_loss: 0.10187 \n",
      "[153/300] train_loss: 0.08182 valid_loss: 0.09264 test_loss: 0.10164 \n",
      "[154/300] train_loss: 0.07866 valid_loss: 0.08917 test_loss: 0.10190 \n",
      "Validation loss decreased (0.089723 --> 0.089167).  Saving model ...\n",
      "[155/300] train_loss: 0.08109 valid_loss: 0.09413 test_loss: 0.10131 \n",
      "[156/300] train_loss: 0.08056 valid_loss: 0.08784 test_loss: 0.10118 \n",
      "Validation loss decreased (0.089167 --> 0.087839).  Saving model ...\n",
      "[157/300] train_loss: 0.08032 valid_loss: 0.08723 test_loss: 0.10061 \n",
      "Validation loss decreased (0.087839 --> 0.087230).  Saving model ...\n",
      "[158/300] train_loss: 0.08095 valid_loss: 0.08972 test_loss: 0.10042 \n",
      "[159/300] train_loss: 0.07745 valid_loss: 0.08898 test_loss: 0.10170 \n",
      "[160/300] train_loss: 0.08073 valid_loss: 0.08768 test_loss: 0.10075 \n",
      "[161/300] train_loss: 0.08071 valid_loss: 0.08937 test_loss: 0.10087 \n",
      "[162/300] train_loss: 0.07669 valid_loss: 0.08912 test_loss: 0.10183 \n",
      "[163/300] train_loss: 0.08015 valid_loss: 0.09056 test_loss: 0.10063 \n",
      "[164/300] train_loss: 0.08103 valid_loss: 0.08777 test_loss: 0.10097 \n",
      "[165/300] train_loss: 0.08052 valid_loss: 0.08986 test_loss: 0.10064 \n",
      "[166/300] train_loss: 0.07744 valid_loss: 0.09795 test_loss: 0.10203 \n",
      "[167/300] train_loss: 0.07959 valid_loss: 0.09278 test_loss: 0.10091 \n",
      "[168/300] train_loss: 0.07992 valid_loss: 0.09046 test_loss: 0.10167 \n",
      "[169/300] train_loss: 0.07816 valid_loss: 0.08932 test_loss: 0.10029 \n",
      "[170/300] train_loss: 0.07633 valid_loss: 0.09114 test_loss: 0.10039 \n",
      "[171/300] train_loss: 0.07967 valid_loss: 0.08881 test_loss: 0.09961 \n",
      "[172/300] train_loss: 0.07671 valid_loss: 0.09063 test_loss: 0.10134 \n",
      "[173/300] train_loss: 0.07885 valid_loss: 0.09568 test_loss: 0.09979 \n",
      "[174/300] train_loss: 0.07858 valid_loss: 0.09137 test_loss: 0.09934 \n",
      "[175/300] train_loss: 0.07927 valid_loss: 0.08998 test_loss: 0.10126 \n",
      "[176/300] train_loss: 0.07907 valid_loss: 0.09083 test_loss: 0.10114 \n",
      "[177/300] train_loss: 0.08031 valid_loss: 0.09154 test_loss: 0.10005 \n",
      "[178/300] train_loss: 0.07868 valid_loss: 0.09127 test_loss: 0.09944 \n",
      "[179/300] train_loss: 0.07602 valid_loss: 0.08849 test_loss: 0.10043 \n",
      "[180/300] train_loss: 0.07700 valid_loss: 0.08747 test_loss: 0.09965 \n",
      "[181/300] train_loss: 0.07662 valid_loss: 0.08657 test_loss: 0.09963 \n",
      "Validation loss decreased (0.087230 --> 0.086568).  Saving model ...\n",
      "[182/300] train_loss: 0.07814 valid_loss: 0.08783 test_loss: 0.09833 \n",
      "[183/300] train_loss: 0.07919 valid_loss: 0.09202 test_loss: 0.09928 \n",
      "[184/300] train_loss: 0.07726 valid_loss: 0.09195 test_loss: 0.09855 \n",
      "[185/300] train_loss: 0.07700 valid_loss: 0.09278 test_loss: 0.10219 \n",
      "[186/300] train_loss: 0.07654 valid_loss: 0.08911 test_loss: 0.10043 \n",
      "[187/300] train_loss: 0.07537 valid_loss: 0.08735 test_loss: 0.09901 \n",
      "[188/300] train_loss: 0.07701 valid_loss: 0.10397 test_loss: 0.09904 \n",
      "[189/300] train_loss: 0.07788 valid_loss: 0.08578 test_loss: 0.09766 \n",
      "Validation loss decreased (0.086568 --> 0.085777).  Saving model ...\n",
      "[190/300] train_loss: 0.07818 valid_loss: 0.08641 test_loss: 0.09943 \n",
      "[191/300] train_loss: 0.07700 valid_loss: 0.08795 test_loss: 0.09927 \n",
      "[192/300] train_loss: 0.07536 valid_loss: 0.08944 test_loss: 0.09901 \n",
      "[193/300] train_loss: 0.07451 valid_loss: 0.08571 test_loss: 0.09811 \n",
      "Validation loss decreased (0.085777 --> 0.085710).  Saving model ...\n",
      "[194/300] train_loss: 0.07551 valid_loss: 0.08652 test_loss: 0.09845 \n",
      "[195/300] train_loss: 0.07541 valid_loss: 0.09497 test_loss: 0.10038 \n",
      "[196/300] train_loss: 0.07523 valid_loss: 0.09286 test_loss: 0.09935 \n",
      "[197/300] train_loss: 0.07682 valid_loss: 0.08713 test_loss: 0.09823 \n",
      "[198/300] train_loss: 0.07628 valid_loss: 0.08894 test_loss: 0.09788 \n",
      "[199/300] train_loss: 0.07474 valid_loss: 0.08558 test_loss: 0.09701 \n",
      "Validation loss decreased (0.085710 --> 0.085578).  Saving model ...\n",
      "[200/300] train_loss: 0.07330 valid_loss: 0.08744 test_loss: 0.09872 \n",
      "[201/300] train_loss: 0.07617 valid_loss: 0.09199 test_loss: 0.09796 \n",
      "[202/300] train_loss: 0.07456 valid_loss: 0.09145 test_loss: 0.09846 \n",
      "[203/300] train_loss: 0.07438 valid_loss: 0.09155 test_loss: 0.09939 \n",
      "[204/300] train_loss: 0.07489 valid_loss: 0.08796 test_loss: 0.10039 \n",
      "[205/300] train_loss: 0.07587 valid_loss: 0.08670 test_loss: 0.09862 \n",
      "[206/300] train_loss: 0.07468 valid_loss: 0.08508 test_loss: 0.09689 \n",
      "Validation loss decreased (0.085578 --> 0.085084).  Saving model ...\n",
      "[207/300] train_loss: 0.07670 valid_loss: 0.08383 test_loss: 0.09610 \n",
      "Validation loss decreased (0.085084 --> 0.083834).  Saving model ...\n",
      "[208/300] train_loss: 0.07420 valid_loss: 0.08456 test_loss: 0.09645 \n",
      "[209/300] train_loss: 0.07392 valid_loss: 0.08533 test_loss: 0.09696 \n",
      "[210/300] train_loss: 0.07732 valid_loss: 0.08463 test_loss: 0.09775 \n",
      "[211/300] train_loss: 0.07599 valid_loss: 0.08454 test_loss: 0.09656 \n",
      "[212/300] train_loss: 0.07488 valid_loss: 0.08505 test_loss: 0.09704 \n",
      "[213/300] train_loss: 0.07450 valid_loss: 0.08904 test_loss: 0.09841 \n",
      "[214/300] train_loss: 0.07338 valid_loss: 0.08525 test_loss: 0.09844 \n",
      "[215/300] train_loss: 0.07420 valid_loss: 0.08619 test_loss: 0.09680 \n",
      "[216/300] train_loss: 0.07428 valid_loss: 0.09241 test_loss: 0.09720 \n",
      "[217/300] train_loss: 0.07520 valid_loss: 0.08550 test_loss: 0.09784 \n",
      "[218/300] train_loss: 0.07335 valid_loss: 0.08428 test_loss: 0.09674 \n",
      "[219/300] train_loss: 0.07312 valid_loss: 0.08585 test_loss: 0.09727 \n",
      "[220/300] train_loss: 0.07440 valid_loss: 0.08353 test_loss: 0.09625 \n",
      "Validation loss decreased (0.083834 --> 0.083529).  Saving model ...\n",
      "[221/300] train_loss: 0.07404 valid_loss: 0.08285 test_loss: 0.09537 \n",
      "Validation loss decreased (0.083529 --> 0.082855).  Saving model ...\n",
      "[222/300] train_loss: 0.07493 valid_loss: 0.08393 test_loss: 0.09639 \n",
      "[223/300] train_loss: 0.07416 valid_loss: 0.08412 test_loss: 0.09644 \n",
      "[224/300] train_loss: 0.07115 valid_loss: 0.08503 test_loss: 0.09694 \n",
      "[225/300] train_loss: 0.07362 valid_loss: 0.08576 test_loss: 0.09694 \n",
      "[226/300] train_loss: 0.07253 valid_loss: 0.08580 test_loss: 0.09637 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[227/300] train_loss: 0.07409 valid_loss: 0.08777 test_loss: 0.09616 \n",
      "[228/300] train_loss: 0.07238 valid_loss: 0.08605 test_loss: 0.09601 \n",
      "[229/300] train_loss: 0.07582 valid_loss: 0.09401 test_loss: 0.09627 \n",
      "[230/300] train_loss: 0.07067 valid_loss: 0.08926 test_loss: 0.09710 \n",
      "[231/300] train_loss: 0.07435 valid_loss: 0.09362 test_loss: 0.09553 \n",
      "[232/300] train_loss: 0.07419 valid_loss: 0.08891 test_loss: 0.09517 \n",
      "[233/300] train_loss: 0.07535 valid_loss: 0.08458 test_loss: 0.09533 \n",
      "[234/300] train_loss: 0.07186 valid_loss: 0.08349 test_loss: 0.09647 \n",
      "[235/300] train_loss: 0.07262 valid_loss: 0.09710 test_loss: 0.09592 \n",
      "[236/300] train_loss: 0.07382 valid_loss: 0.08754 test_loss: 0.09658 \n",
      "[237/300] train_loss: 0.07253 valid_loss: 0.10080 test_loss: 0.09536 \n",
      "[238/300] train_loss: 0.07393 valid_loss: 0.08363 test_loss: 0.09542 \n",
      "[239/300] train_loss: 0.07147 valid_loss: 0.08621 test_loss: 0.09770 \n",
      "[240/300] train_loss: 0.07074 valid_loss: 0.08305 test_loss: 0.09597 \n",
      "[241/300] train_loss: 0.07169 valid_loss: 0.08456 test_loss: 0.09700 \n",
      "[242/300] train_loss: 0.07238 valid_loss: 0.08341 test_loss: 0.09566 \n",
      "[243/300] train_loss: 0.07087 valid_loss: 0.08378 test_loss: 0.09608 \n",
      "[244/300] train_loss: 0.07366 valid_loss: 0.08689 test_loss: 0.09655 \n",
      "[245/300] train_loss: 0.07365 valid_loss: 0.08217 test_loss: 0.09498 \n",
      "Validation loss decreased (0.082855 --> 0.082171).  Saving model ...\n",
      "[246/300] train_loss: 0.07193 valid_loss: 0.08274 test_loss: 0.09462 \n",
      "[247/300] train_loss: 0.07112 valid_loss: 0.08227 test_loss: 0.09554 \n",
      "[248/300] train_loss: 0.07236 valid_loss: 0.08249 test_loss: 0.09489 \n",
      "[249/300] train_loss: 0.06833 valid_loss: 0.08338 test_loss: 0.09514 \n",
      "[250/300] train_loss: 0.07230 valid_loss: 0.08541 test_loss: 0.09561 \n",
      "[251/300] train_loss: 0.07244 valid_loss: 0.08421 test_loss: 0.09476 \n",
      "[252/300] train_loss: 0.07381 valid_loss: 0.08359 test_loss: 0.09484 \n",
      "[253/300] train_loss: 0.07092 valid_loss: 0.08601 test_loss: 0.09553 \n",
      "[254/300] train_loss: 0.07258 valid_loss: 0.08589 test_loss: 0.09420 \n",
      "[255/300] train_loss: 0.07451 valid_loss: 0.09030 test_loss: 0.09525 \n",
      "[256/300] train_loss: 0.07156 valid_loss: 0.09796 test_loss: 0.09469 \n",
      "[257/300] train_loss: 0.07455 valid_loss: 0.08383 test_loss: 0.09445 \n",
      "[258/300] train_loss: 0.07139 valid_loss: 0.08437 test_loss: 0.09407 \n",
      "[259/300] train_loss: 0.06999 valid_loss: 0.08276 test_loss: 0.09451 \n",
      "[260/300] train_loss: 0.06964 valid_loss: 0.08357 test_loss: 0.09519 \n",
      "[261/300] train_loss: 0.06981 valid_loss: 0.08715 test_loss: 0.09457 \n",
      "[262/300] train_loss: 0.07144 valid_loss: 0.08600 test_loss: 0.09307 \n",
      "[263/300] train_loss: 0.07013 valid_loss: 0.08370 test_loss: 0.09357 \n",
      "[264/300] train_loss: 0.07208 valid_loss: 0.08880 test_loss: 0.09357 \n",
      "[265/300] train_loss: 0.07161 valid_loss: 0.08666 test_loss: 0.09505 \n",
      "[266/300] train_loss: 0.06786 valid_loss: 0.08382 test_loss: 0.09325 \n",
      "[267/300] train_loss: 0.06936 valid_loss: 0.08135 test_loss: 0.09383 \n",
      "Validation loss decreased (0.082171 --> 0.081348).  Saving model ...\n",
      "[268/300] train_loss: 0.07214 valid_loss: 0.08278 test_loss: 0.09462 \n",
      "[269/300] train_loss: 0.07075 valid_loss: 0.08123 test_loss: 0.09404 \n",
      "Validation loss decreased (0.081348 --> 0.081232).  Saving model ...\n",
      "[270/300] train_loss: 0.06942 valid_loss: 0.08528 test_loss: 0.09403 \n",
      "[271/300] train_loss: 0.06785 valid_loss: 0.08130 test_loss: 0.09361 \n",
      "[272/300] train_loss: 0.07153 valid_loss: 0.08268 test_loss: 0.09403 \n",
      "[273/300] train_loss: 0.07135 valid_loss: 0.08423 test_loss: 0.09377 \n",
      "[274/300] train_loss: 0.07117 valid_loss: 0.08287 test_loss: 0.09311 \n",
      "[275/300] train_loss: 0.07057 valid_loss: 0.08284 test_loss: 0.09378 \n",
      "[276/300] train_loss: 0.07376 valid_loss: 0.08328 test_loss: 0.09350 \n",
      "[277/300] train_loss: 0.07040 valid_loss: 0.08544 test_loss: 0.09304 \n",
      "[278/300] train_loss: 0.06881 valid_loss: 0.09540 test_loss: 0.09319 \n",
      "[279/300] train_loss: 0.07098 valid_loss: 0.08966 test_loss: 0.09377 \n",
      "[280/300] train_loss: 0.06907 valid_loss: 0.08929 test_loss: 0.09250 \n",
      "[281/300] train_loss: 0.07041 valid_loss: 0.09007 test_loss: 0.09344 \n",
      "[282/300] train_loss: 0.06997 valid_loss: 0.08315 test_loss: 0.09262 \n",
      "[283/300] train_loss: 0.06927 valid_loss: 0.08094 test_loss: 0.09141 \n",
      "Validation loss decreased (0.081232 --> 0.080942).  Saving model ...\n",
      "[284/300] train_loss: 0.07014 valid_loss: 0.08093 test_loss: 0.09204 \n",
      "Validation loss decreased (0.080942 --> 0.080934).  Saving model ...\n",
      "[285/300] train_loss: 0.06906 valid_loss: 0.08575 test_loss: 0.09341 \n",
      "[286/300] train_loss: 0.06884 valid_loss: 0.08908 test_loss: 0.09269 \n",
      "[287/300] train_loss: 0.06804 valid_loss: 0.09073 test_loss: 0.09371 \n",
      "[288/300] train_loss: 0.07037 valid_loss: 0.08254 test_loss: 0.09179 \n",
      "[289/300] train_loss: 0.06890 valid_loss: 0.08098 test_loss: 0.09233 \n",
      "[290/300] train_loss: 0.06859 valid_loss: 0.08210 test_loss: 0.09245 \n",
      "[291/300] train_loss: 0.06768 valid_loss: 0.08484 test_loss: 0.09249 \n",
      "[292/300] train_loss: 0.06833 valid_loss: 0.08319 test_loss: 0.09427 \n",
      "[293/300] train_loss: 0.06730 valid_loss: 0.09426 test_loss: 0.09267 \n",
      "[294/300] train_loss: 0.07044 valid_loss: 0.08827 test_loss: 0.09297 \n",
      "[295/300] train_loss: 0.06884 valid_loss: 0.08743 test_loss: 0.09195 \n",
      "[296/300] train_loss: 0.06830 valid_loss: 0.08574 test_loss: 0.09139 \n",
      "[297/300] train_loss: 0.06661 valid_loss: 0.08111 test_loss: 0.09157 \n",
      "[298/300] train_loss: 0.07088 valid_loss: 0.08270 test_loss: 0.09184 \n",
      "[299/300] train_loss: 0.06788 valid_loss: 0.08334 test_loss: 0.09182 \n",
      "[300/300] train_loss: 0.06851 valid_loss: 0.09380 test_loss: 0.09284 \n",
      "TRAINING MODEL 5\n",
      "[  1/300] train_loss: 0.58034 valid_loss: 0.50589 test_loss: 0.50051 \n",
      "Validation loss decreased (inf --> 0.505891).  Saving model ...\n",
      "[  2/300] train_loss: 0.42421 valid_loss: 0.38613 test_loss: 0.38614 \n",
      "Validation loss decreased (0.505891 --> 0.386129).  Saving model ...\n",
      "[  3/300] train_loss: 0.33447 valid_loss: 0.32664 test_loss: 0.33286 \n",
      "Validation loss decreased (0.386129 --> 0.326635).  Saving model ...\n",
      "[  4/300] train_loss: 0.28222 valid_loss: 0.28726 test_loss: 0.29746 \n",
      "Validation loss decreased (0.326635 --> 0.287257).  Saving model ...\n",
      "[  5/300] train_loss: 0.25232 valid_loss: 0.24796 test_loss: 0.26231 \n",
      "Validation loss decreased (0.287257 --> 0.247956).  Saving model ...\n",
      "[  6/300] train_loss: 0.22179 valid_loss: 0.22188 test_loss: 0.23650 \n",
      "Validation loss decreased (0.247956 --> 0.221885).  Saving model ...\n",
      "[  7/300] train_loss: 0.20168 valid_loss: 0.20070 test_loss: 0.21295 \n",
      "Validation loss decreased (0.221885 --> 0.200703).  Saving model ...\n",
      "[  8/300] train_loss: 0.18473 valid_loss: 0.18857 test_loss: 0.20000 \n",
      "Validation loss decreased (0.200703 --> 0.188573).  Saving model ...\n",
      "[  9/300] train_loss: 0.17313 valid_loss: 0.17790 test_loss: 0.18910 \n",
      "Validation loss decreased (0.188573 --> 0.177902).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16704 valid_loss: 0.17220 test_loss: 0.18199 \n",
      "Validation loss decreased (0.177902 --> 0.172199).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15759 valid_loss: 0.16720 test_loss: 0.17714 \n",
      "Validation loss decreased (0.172199 --> 0.167197).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15548 valid_loss: 0.15988 test_loss: 0.17047 \n",
      "Validation loss decreased (0.167197 --> 0.159881).  Saving model ...\n",
      "[ 13/300] train_loss: 0.14702 valid_loss: 0.15966 test_loss: 0.16803 \n",
      "Validation loss decreased (0.159881 --> 0.159656).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14534 valid_loss: 0.15240 test_loss: 0.16433 \n",
      "Validation loss decreased (0.159656 --> 0.152402).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14080 valid_loss: 0.14803 test_loss: 0.16006 \n",
      "Validation loss decreased (0.152402 --> 0.148032).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13883 valid_loss: 0.14385 test_loss: 0.15641 \n",
      "Validation loss decreased (0.148032 --> 0.143854).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14050 valid_loss: 0.14197 test_loss: 0.15429 \n",
      "Validation loss decreased (0.143854 --> 0.141967).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13242 valid_loss: 0.14051 test_loss: 0.15318 \n",
      "Validation loss decreased (0.141967 --> 0.140508).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13290 valid_loss: 0.13853 test_loss: 0.15110 \n",
      "Validation loss decreased (0.140508 --> 0.138533).  Saving model ...\n",
      "[ 20/300] train_loss: 0.12918 valid_loss: 0.13617 test_loss: 0.15094 \n",
      "Validation loss decreased (0.138533 --> 0.136170).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 21/300] train_loss: 0.12809 valid_loss: 0.13452 test_loss: 0.14996 \n",
      "Validation loss decreased (0.136170 --> 0.134519).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12995 valid_loss: 0.13368 test_loss: 0.14828 \n",
      "Validation loss decreased (0.134519 --> 0.133676).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12458 valid_loss: 0.13121 test_loss: 0.14555 \n",
      "Validation loss decreased (0.133676 --> 0.131206).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12668 valid_loss: 0.13090 test_loss: 0.14437 \n",
      "Validation loss decreased (0.131206 --> 0.130902).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12277 valid_loss: 0.12691 test_loss: 0.14179 \n",
      "Validation loss decreased (0.130902 --> 0.126906).  Saving model ...\n",
      "[ 26/300] train_loss: 0.11948 valid_loss: 0.12589 test_loss: 0.14123 \n",
      "Validation loss decreased (0.126906 --> 0.125894).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11929 valid_loss: 0.12519 test_loss: 0.13964 \n",
      "Validation loss decreased (0.125894 --> 0.125194).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11939 valid_loss: 0.12242 test_loss: 0.13976 \n",
      "Validation loss decreased (0.125194 --> 0.122425).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12127 valid_loss: 0.12274 test_loss: 0.13765 \n",
      "[ 30/300] train_loss: 0.11779 valid_loss: 0.12190 test_loss: 0.13761 \n",
      "Validation loss decreased (0.122425 --> 0.121902).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11801 valid_loss: 0.12185 test_loss: 0.13703 \n",
      "Validation loss decreased (0.121902 --> 0.121850).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11511 valid_loss: 0.12172 test_loss: 0.13663 \n",
      "Validation loss decreased (0.121850 --> 0.121723).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11462 valid_loss: 0.11851 test_loss: 0.13498 \n",
      "Validation loss decreased (0.121723 --> 0.118511).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11649 valid_loss: 0.12126 test_loss: 0.13646 \n",
      "[ 35/300] train_loss: 0.11690 valid_loss: 0.11862 test_loss: 0.13345 \n",
      "[ 36/300] train_loss: 0.11168 valid_loss: 0.12164 test_loss: 0.13544 \n",
      "[ 37/300] train_loss: 0.10936 valid_loss: 0.11706 test_loss: 0.13199 \n",
      "Validation loss decreased (0.118511 --> 0.117057).  Saving model ...\n",
      "[ 38/300] train_loss: 0.10954 valid_loss: 0.11796 test_loss: 0.13491 \n",
      "[ 39/300] train_loss: 0.11106 valid_loss: 0.11913 test_loss: 0.13216 \n",
      "[ 40/300] train_loss: 0.10714 valid_loss: 0.11726 test_loss: 0.13148 \n",
      "[ 41/300] train_loss: 0.11096 valid_loss: 0.11657 test_loss: 0.12961 \n",
      "Validation loss decreased (0.117057 --> 0.116567).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10741 valid_loss: 0.11536 test_loss: 0.13064 \n",
      "Validation loss decreased (0.116567 --> 0.115363).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10552 valid_loss: 0.11609 test_loss: 0.13026 \n",
      "[ 44/300] train_loss: 0.10774 valid_loss: 0.11507 test_loss: 0.12803 \n",
      "Validation loss decreased (0.115363 --> 0.115073).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10600 valid_loss: 0.11127 test_loss: 0.12763 \n",
      "Validation loss decreased (0.115073 --> 0.111268).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10416 valid_loss: 0.11415 test_loss: 0.12730 \n",
      "[ 47/300] train_loss: 0.10577 valid_loss: 0.12145 test_loss: 0.12919 \n",
      "[ 48/300] train_loss: 0.10310 valid_loss: 0.11345 test_loss: 0.12629 \n",
      "[ 49/300] train_loss: 0.10712 valid_loss: 0.11294 test_loss: 0.12536 \n",
      "[ 50/300] train_loss: 0.10294 valid_loss: 0.11242 test_loss: 0.12437 \n",
      "[ 51/300] train_loss: 0.10226 valid_loss: 0.11117 test_loss: 0.12555 \n",
      "Validation loss decreased (0.111268 --> 0.111173).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10115 valid_loss: 0.11472 test_loss: 0.12535 \n",
      "[ 53/300] train_loss: 0.09926 valid_loss: 0.10998 test_loss: 0.12448 \n",
      "Validation loss decreased (0.111173 --> 0.109981).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10346 valid_loss: 0.11000 test_loss: 0.12256 \n",
      "[ 55/300] train_loss: 0.10072 valid_loss: 0.11218 test_loss: 0.12318 \n",
      "[ 56/300] train_loss: 0.09968 valid_loss: 0.11168 test_loss: 0.12397 \n",
      "[ 57/300] train_loss: 0.10186 valid_loss: 0.10725 test_loss: 0.12165 \n",
      "Validation loss decreased (0.109981 --> 0.107245).  Saving model ...\n",
      "[ 58/300] train_loss: 0.09962 valid_loss: 0.10759 test_loss: 0.12075 \n",
      "[ 59/300] train_loss: 0.10181 valid_loss: 0.11110 test_loss: 0.12147 \n",
      "[ 60/300] train_loss: 0.09889 valid_loss: 0.10720 test_loss: 0.12085 \n",
      "Validation loss decreased (0.107245 --> 0.107200).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09664 valid_loss: 0.10901 test_loss: 0.12012 \n",
      "[ 62/300] train_loss: 0.09692 valid_loss: 0.11049 test_loss: 0.12000 \n",
      "[ 63/300] train_loss: 0.09617 valid_loss: 0.11134 test_loss: 0.11955 \n",
      "[ 64/300] train_loss: 0.09791 valid_loss: 0.10877 test_loss: 0.11875 \n",
      "[ 65/300] train_loss: 0.09734 valid_loss: 0.11213 test_loss: 0.11946 \n",
      "[ 66/300] train_loss: 0.09834 valid_loss: 0.10358 test_loss: 0.11830 \n",
      "Validation loss decreased (0.107200 --> 0.103583).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09803 valid_loss: 0.10841 test_loss: 0.11844 \n",
      "[ 68/300] train_loss: 0.09555 valid_loss: 0.10809 test_loss: 0.11930 \n",
      "[ 69/300] train_loss: 0.09522 valid_loss: 0.10492 test_loss: 0.11809 \n",
      "[ 70/300] train_loss: 0.09663 valid_loss: 0.10601 test_loss: 0.11788 \n",
      "[ 71/300] train_loss: 0.09364 valid_loss: 0.10478 test_loss: 0.11697 \n",
      "[ 72/300] train_loss: 0.09424 valid_loss: 0.10691 test_loss: 0.11585 \n",
      "[ 73/300] train_loss: 0.09240 valid_loss: 0.10625 test_loss: 0.11629 \n",
      "[ 74/300] train_loss: 0.09400 valid_loss: 0.10362 test_loss: 0.11508 \n",
      "[ 75/300] train_loss: 0.09442 valid_loss: 0.11031 test_loss: 0.11647 \n",
      "[ 76/300] train_loss: 0.09235 valid_loss: 0.10538 test_loss: 0.11486 \n",
      "[ 77/300] train_loss: 0.09206 valid_loss: 0.10284 test_loss: 0.11556 \n",
      "Validation loss decreased (0.103583 --> 0.102841).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09404 valid_loss: 0.10102 test_loss: 0.11428 \n",
      "Validation loss decreased (0.102841 --> 0.101021).  Saving model ...\n",
      "[ 79/300] train_loss: 0.09217 valid_loss: 0.10035 test_loss: 0.11414 \n",
      "Validation loss decreased (0.101021 --> 0.100352).  Saving model ...\n",
      "[ 80/300] train_loss: 0.09078 valid_loss: 0.10101 test_loss: 0.11325 \n",
      "[ 81/300] train_loss: 0.09115 valid_loss: 0.11021 test_loss: 0.11351 \n",
      "[ 82/300] train_loss: 0.09020 valid_loss: 0.10669 test_loss: 0.11331 \n",
      "[ 83/300] train_loss: 0.09458 valid_loss: 0.10729 test_loss: 0.11242 \n",
      "[ 84/300] train_loss: 0.09284 valid_loss: 0.10263 test_loss: 0.11219 \n",
      "[ 85/300] train_loss: 0.08960 valid_loss: 0.10172 test_loss: 0.11163 \n",
      "[ 86/300] train_loss: 0.08976 valid_loss: 0.10275 test_loss: 0.11272 \n",
      "[ 87/300] train_loss: 0.09054 valid_loss: 0.09749 test_loss: 0.11219 \n",
      "Validation loss decreased (0.100352 --> 0.097493).  Saving model ...\n",
      "[ 88/300] train_loss: 0.08995 valid_loss: 0.09880 test_loss: 0.11268 \n",
      "[ 89/300] train_loss: 0.09002 valid_loss: 0.10229 test_loss: 0.11088 \n",
      "[ 90/300] train_loss: 0.09008 valid_loss: 0.09625 test_loss: 0.11020 \n",
      "Validation loss decreased (0.097493 --> 0.096251).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08688 valid_loss: 0.10560 test_loss: 0.11188 \n",
      "[ 92/300] train_loss: 0.09161 valid_loss: 0.10160 test_loss: 0.11187 \n",
      "[ 93/300] train_loss: 0.09058 valid_loss: 0.10147 test_loss: 0.11006 \n",
      "[ 94/300] train_loss: 0.08902 valid_loss: 0.10137 test_loss: 0.11149 \n",
      "[ 95/300] train_loss: 0.08832 valid_loss: 0.09803 test_loss: 0.10981 \n",
      "[ 96/300] train_loss: 0.08533 valid_loss: 0.09814 test_loss: 0.10971 \n",
      "[ 97/300] train_loss: 0.08919 valid_loss: 0.10388 test_loss: 0.11100 \n",
      "[ 98/300] train_loss: 0.08969 valid_loss: 0.10402 test_loss: 0.10987 \n",
      "[ 99/300] train_loss: 0.08835 valid_loss: 0.09730 test_loss: 0.10978 \n",
      "[100/300] train_loss: 0.08635 valid_loss: 0.09660 test_loss: 0.10856 \n",
      "[101/300] train_loss: 0.08663 valid_loss: 0.10331 test_loss: 0.11012 \n",
      "[102/300] train_loss: 0.08809 valid_loss: 0.10597 test_loss: 0.10893 \n",
      "[103/300] train_loss: 0.08825 valid_loss: 0.10229 test_loss: 0.10808 \n",
      "[104/300] train_loss: 0.08778 valid_loss: 0.09358 test_loss: 0.10800 \n",
      "Validation loss decreased (0.096251 --> 0.093577).  Saving model ...\n",
      "[105/300] train_loss: 0.08592 valid_loss: 0.09316 test_loss: 0.10791 \n",
      "Validation loss decreased (0.093577 --> 0.093156).  Saving model ...\n",
      "[106/300] train_loss: 0.08789 valid_loss: 0.10110 test_loss: 0.10550 \n",
      "[107/300] train_loss: 0.08864 valid_loss: 0.10103 test_loss: 0.10871 \n",
      "[108/300] train_loss: 0.08774 valid_loss: 0.10580 test_loss: 0.10613 \n",
      "[109/300] train_loss: 0.08309 valid_loss: 0.09990 test_loss: 0.10690 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110/300] train_loss: 0.08577 valid_loss: 0.11367 test_loss: 0.10728 \n",
      "[111/300] train_loss: 0.08388 valid_loss: 0.10043 test_loss: 0.10837 \n",
      "[112/300] train_loss: 0.08679 valid_loss: 0.10096 test_loss: 0.10652 \n",
      "[113/300] train_loss: 0.08687 valid_loss: 0.10548 test_loss: 0.10543 \n",
      "[114/300] train_loss: 0.08390 valid_loss: 0.10115 test_loss: 0.10540 \n",
      "[115/300] train_loss: 0.08455 valid_loss: 0.09742 test_loss: 0.10564 \n",
      "[116/300] train_loss: 0.08559 valid_loss: 0.09835 test_loss: 0.10510 \n",
      "[117/300] train_loss: 0.08252 valid_loss: 0.09516 test_loss: 0.10499 \n",
      "[118/300] train_loss: 0.08592 valid_loss: 0.09536 test_loss: 0.10659 \n",
      "[119/300] train_loss: 0.08319 valid_loss: 0.09346 test_loss: 0.10479 \n",
      "[120/300] train_loss: 0.08456 valid_loss: 0.10098 test_loss: 0.10583 \n",
      "[121/300] train_loss: 0.08361 valid_loss: 0.09503 test_loss: 0.10517 \n",
      "[122/300] train_loss: 0.08351 valid_loss: 0.11544 test_loss: 0.10402 \n",
      "[123/300] train_loss: 0.08512 valid_loss: 0.09977 test_loss: 0.10529 \n",
      "[124/300] train_loss: 0.08125 valid_loss: 0.09722 test_loss: 0.10357 \n",
      "[125/300] train_loss: 0.08519 valid_loss: 0.09054 test_loss: 0.10351 \n",
      "Validation loss decreased (0.093156 --> 0.090545).  Saving model ...\n",
      "[126/300] train_loss: 0.08401 valid_loss: 0.09809 test_loss: 0.10352 \n",
      "[127/300] train_loss: 0.08529 valid_loss: 0.09073 test_loss: 0.10281 \n",
      "[128/300] train_loss: 0.08514 valid_loss: 0.09330 test_loss: 0.10410 \n",
      "[129/300] train_loss: 0.08194 valid_loss: 0.09145 test_loss: 0.10352 \n",
      "[130/300] train_loss: 0.08162 valid_loss: 0.09865 test_loss: 0.10369 \n",
      "[131/300] train_loss: 0.08348 valid_loss: 0.09782 test_loss: 0.10351 \n",
      "[132/300] train_loss: 0.08268 valid_loss: 0.09663 test_loss: 0.10407 \n",
      "[133/300] train_loss: 0.08004 valid_loss: 0.09437 test_loss: 0.10326 \n",
      "[134/300] train_loss: 0.08267 valid_loss: 0.09221 test_loss: 0.10289 \n",
      "[135/300] train_loss: 0.08425 valid_loss: 0.09682 test_loss: 0.10246 \n",
      "[136/300] train_loss: 0.08159 valid_loss: 0.09376 test_loss: 0.10346 \n",
      "[137/300] train_loss: 0.08011 valid_loss: 0.09359 test_loss: 0.10313 \n",
      "[138/300] train_loss: 0.08001 valid_loss: 0.09465 test_loss: 0.10359 \n",
      "[139/300] train_loss: 0.08090 valid_loss: 0.09036 test_loss: 0.10235 \n",
      "Validation loss decreased (0.090545 --> 0.090358).  Saving model ...\n",
      "[140/300] train_loss: 0.08311 valid_loss: 0.10285 test_loss: 0.10275 \n",
      "[141/300] train_loss: 0.08249 valid_loss: 0.09698 test_loss: 0.10242 \n",
      "[142/300] train_loss: 0.08213 valid_loss: 0.09365 test_loss: 0.10192 \n",
      "[143/300] train_loss: 0.08248 valid_loss: 0.09278 test_loss: 0.10206 \n",
      "[144/300] train_loss: 0.08083 valid_loss: 0.09195 test_loss: 0.10168 \n",
      "[145/300] train_loss: 0.07901 valid_loss: 0.09617 test_loss: 0.10182 \n",
      "[146/300] train_loss: 0.08249 valid_loss: 0.09429 test_loss: 0.10003 \n",
      "[147/300] train_loss: 0.08108 valid_loss: 0.09138 test_loss: 0.10078 \n",
      "[148/300] train_loss: 0.08079 valid_loss: 0.09117 test_loss: 0.10018 \n",
      "[149/300] train_loss: 0.08022 valid_loss: 0.08969 test_loss: 0.10190 \n",
      "Validation loss decreased (0.090358 --> 0.089687).  Saving model ...\n",
      "[150/300] train_loss: 0.07882 valid_loss: 0.09595 test_loss: 0.10164 \n",
      "[151/300] train_loss: 0.07891 valid_loss: 0.08940 test_loss: 0.10126 \n",
      "Validation loss decreased (0.089687 --> 0.089397).  Saving model ...\n",
      "[152/300] train_loss: 0.08181 valid_loss: 0.09008 test_loss: 0.10104 \n",
      "[153/300] train_loss: 0.08000 valid_loss: 0.09261 test_loss: 0.10126 \n",
      "[154/300] train_loss: 0.07994 valid_loss: 0.09595 test_loss: 0.10013 \n",
      "[155/300] train_loss: 0.08243 valid_loss: 0.10089 test_loss: 0.10108 \n",
      "[156/300] train_loss: 0.08038 valid_loss: 0.09297 test_loss: 0.09968 \n",
      "[157/300] train_loss: 0.08073 valid_loss: 0.08950 test_loss: 0.10019 \n",
      "[158/300] train_loss: 0.08138 valid_loss: 0.08913 test_loss: 0.10132 \n",
      "Validation loss decreased (0.089397 --> 0.089127).  Saving model ...\n",
      "[159/300] train_loss: 0.08337 valid_loss: 0.09628 test_loss: 0.09938 \n",
      "[160/300] train_loss: 0.07977 valid_loss: 0.09540 test_loss: 0.09988 \n",
      "[161/300] train_loss: 0.08003 valid_loss: 0.10189 test_loss: 0.09958 \n",
      "[162/300] train_loss: 0.07599 valid_loss: 0.09132 test_loss: 0.10106 \n",
      "[163/300] train_loss: 0.07737 valid_loss: 0.10093 test_loss: 0.09975 \n",
      "[164/300] train_loss: 0.07853 valid_loss: 0.10720 test_loss: 0.09942 \n",
      "[165/300] train_loss: 0.07744 valid_loss: 0.09870 test_loss: 0.10017 \n",
      "[166/300] train_loss: 0.07751 valid_loss: 0.10115 test_loss: 0.09820 \n",
      "[167/300] train_loss: 0.07756 valid_loss: 0.09406 test_loss: 0.09848 \n",
      "[168/300] train_loss: 0.07834 valid_loss: 0.09169 test_loss: 0.09809 \n",
      "[169/300] train_loss: 0.07676 valid_loss: 0.09098 test_loss: 0.09858 \n",
      "[170/300] train_loss: 0.07793 valid_loss: 0.09401 test_loss: 0.09896 \n",
      "[171/300] train_loss: 0.07868 valid_loss: 0.09058 test_loss: 0.09743 \n",
      "[172/300] train_loss: 0.07876 valid_loss: 0.08965 test_loss: 0.09965 \n",
      "[173/300] train_loss: 0.07677 valid_loss: 0.09150 test_loss: 0.09874 \n",
      "[174/300] train_loss: 0.07823 valid_loss: 0.09646 test_loss: 0.09824 \n",
      "[175/300] train_loss: 0.07770 valid_loss: 0.09408 test_loss: 0.09775 \n",
      "[176/300] train_loss: 0.07608 valid_loss: 0.09866 test_loss: 0.09864 \n",
      "[177/300] train_loss: 0.07811 valid_loss: 0.09248 test_loss: 0.09872 \n",
      "[178/300] train_loss: 0.07508 valid_loss: 0.08846 test_loss: 0.09806 \n",
      "Validation loss decreased (0.089127 --> 0.088464).  Saving model ...\n",
      "[179/300] train_loss: 0.07685 valid_loss: 0.09226 test_loss: 0.09703 \n",
      "[180/300] train_loss: 0.07610 valid_loss: 0.08782 test_loss: 0.09826 \n",
      "Validation loss decreased (0.088464 --> 0.087817).  Saving model ...\n",
      "[181/300] train_loss: 0.07524 valid_loss: 0.09449 test_loss: 0.09754 \n",
      "[182/300] train_loss: 0.07796 valid_loss: 0.08877 test_loss: 0.09786 \n",
      "[183/300] train_loss: 0.07747 valid_loss: 0.08834 test_loss: 0.09717 \n",
      "[184/300] train_loss: 0.07852 valid_loss: 0.09503 test_loss: 0.09788 \n",
      "[185/300] train_loss: 0.07991 valid_loss: 0.08718 test_loss: 0.09686 \n",
      "Validation loss decreased (0.087817 --> 0.087181).  Saving model ...\n",
      "[186/300] train_loss: 0.07713 valid_loss: 0.10367 test_loss: 0.09684 \n",
      "[187/300] train_loss: 0.07924 valid_loss: 0.09040 test_loss: 0.09748 \n",
      "[188/300] train_loss: 0.07814 valid_loss: 0.09398 test_loss: 0.09717 \n",
      "[189/300] train_loss: 0.07221 valid_loss: 0.10653 test_loss: 0.09755 \n",
      "[190/300] train_loss: 0.07767 valid_loss: 0.09584 test_loss: 0.09721 \n",
      "[191/300] train_loss: 0.07708 valid_loss: 0.09082 test_loss: 0.09660 \n",
      "[192/300] train_loss: 0.07807 valid_loss: 0.09785 test_loss: 0.09644 \n",
      "[193/300] train_loss: 0.07626 valid_loss: 0.10680 test_loss: 0.09632 \n",
      "[194/300] train_loss: 0.07511 valid_loss: 0.09270 test_loss: 0.09710 \n",
      "[195/300] train_loss: 0.07668 valid_loss: 0.09276 test_loss: 0.09764 \n",
      "[196/300] train_loss: 0.07568 valid_loss: 0.09104 test_loss: 0.09713 \n",
      "[197/300] train_loss: 0.07611 valid_loss: 0.08898 test_loss: 0.09732 \n",
      "[198/300] train_loss: 0.07578 valid_loss: 0.09306 test_loss: 0.09644 \n",
      "[199/300] train_loss: 0.07546 valid_loss: 0.09377 test_loss: 0.09535 \n",
      "[200/300] train_loss: 0.07580 valid_loss: 0.09483 test_loss: 0.09606 \n",
      "[201/300] train_loss: 0.07584 valid_loss: 0.08710 test_loss: 0.09726 \n",
      "Validation loss decreased (0.087181 --> 0.087103).  Saving model ...\n",
      "[202/300] train_loss: 0.07675 valid_loss: 0.08590 test_loss: 0.09551 \n",
      "Validation loss decreased (0.087103 --> 0.085896).  Saving model ...\n",
      "[203/300] train_loss: 0.07199 valid_loss: 0.09029 test_loss: 0.09618 \n",
      "[204/300] train_loss: 0.07515 valid_loss: 0.08583 test_loss: 0.09704 \n",
      "Validation loss decreased (0.085896 --> 0.085834).  Saving model ...\n",
      "[205/300] train_loss: 0.07604 valid_loss: 0.08710 test_loss: 0.09644 \n",
      "[206/300] train_loss: 0.07555 valid_loss: 0.08505 test_loss: 0.09560 \n",
      "Validation loss decreased (0.085834 --> 0.085046).  Saving model ...\n",
      "[207/300] train_loss: 0.07506 valid_loss: 0.09356 test_loss: 0.09567 \n",
      "[208/300] train_loss: 0.07615 valid_loss: 0.08289 test_loss: 0.09540 \n",
      "Validation loss decreased (0.085046 --> 0.082893).  Saving model ...\n",
      "[209/300] train_loss: 0.07473 valid_loss: 0.08463 test_loss: 0.09529 \n",
      "[210/300] train_loss: 0.07097 valid_loss: 0.08478 test_loss: 0.09628 \n",
      "[211/300] train_loss: 0.07385 valid_loss: 0.09055 test_loss: 0.09568 \n",
      "[212/300] train_loss: 0.07543 valid_loss: 0.08732 test_loss: 0.09496 \n",
      "[213/300] train_loss: 0.07314 valid_loss: 0.08704 test_loss: 0.09469 \n",
      "[214/300] train_loss: 0.07558 valid_loss: 0.08342 test_loss: 0.09527 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215/300] train_loss: 0.07526 valid_loss: 0.09057 test_loss: 0.09434 \n",
      "[216/300] train_loss: 0.07578 valid_loss: 0.08435 test_loss: 0.09506 \n",
      "[217/300] train_loss: 0.07224 valid_loss: 0.09184 test_loss: 0.09534 \n",
      "[218/300] train_loss: 0.07522 valid_loss: 0.08578 test_loss: 0.09447 \n",
      "[219/300] train_loss: 0.07265 valid_loss: 0.09460 test_loss: 0.09552 \n",
      "[220/300] train_loss: 0.07195 valid_loss: 0.08475 test_loss: 0.09624 \n",
      "[221/300] train_loss: 0.07322 valid_loss: 0.08479 test_loss: 0.09498 \n",
      "[222/300] train_loss: 0.07336 valid_loss: 0.09349 test_loss: 0.09607 \n",
      "[223/300] train_loss: 0.07451 valid_loss: 0.08707 test_loss: 0.09537 \n",
      "[224/300] train_loss: 0.07141 valid_loss: 0.09025 test_loss: 0.09513 \n",
      "[225/300] train_loss: 0.07360 valid_loss: 0.09344 test_loss: 0.09523 \n",
      "[226/300] train_loss: 0.07174 valid_loss: 0.08626 test_loss: 0.09391 \n",
      "[227/300] train_loss: 0.07423 valid_loss: 0.10301 test_loss: 0.09648 \n",
      "[228/300] train_loss: 0.07108 valid_loss: 0.10178 test_loss: 0.09423 \n",
      "[229/300] train_loss: 0.07284 valid_loss: 0.08901 test_loss: 0.09394 \n",
      "[230/300] train_loss: 0.07412 valid_loss: 0.09552 test_loss: 0.09484 \n",
      "[231/300] train_loss: 0.07188 valid_loss: 0.09546 test_loss: 0.09440 \n",
      "[232/300] train_loss: 0.07119 valid_loss: 0.09800 test_loss: 0.09505 \n",
      "[233/300] train_loss: 0.07164 valid_loss: 0.09597 test_loss: 0.09428 \n",
      "[234/300] train_loss: 0.07325 valid_loss: 0.09428 test_loss: 0.09460 \n",
      "[235/300] train_loss: 0.06998 valid_loss: 0.09326 test_loss: 0.09461 \n",
      "[236/300] train_loss: 0.07246 valid_loss: 0.09327 test_loss: 0.09454 \n",
      "[237/300] train_loss: 0.06961 valid_loss: 0.10312 test_loss: 0.09532 \n",
      "[238/300] train_loss: 0.07196 valid_loss: 0.09348 test_loss: 0.09426 \n",
      "[239/300] train_loss: 0.07434 valid_loss: 0.09261 test_loss: 0.09517 \n",
      "[240/300] train_loss: 0.07186 valid_loss: 0.08514 test_loss: 0.09452 \n",
      "[241/300] train_loss: 0.07155 valid_loss: 0.08345 test_loss: 0.09408 \n",
      "[242/300] train_loss: 0.06821 valid_loss: 0.08389 test_loss: 0.09482 \n",
      "[243/300] train_loss: 0.07207 valid_loss: 0.08387 test_loss: 0.09402 \n",
      "[244/300] train_loss: 0.06985 valid_loss: 0.08678 test_loss: 0.09336 \n",
      "[245/300] train_loss: 0.07165 valid_loss: 0.08734 test_loss: 0.09204 \n",
      "[246/300] train_loss: 0.07128 valid_loss: 0.08858 test_loss: 0.09349 \n",
      "[247/300] train_loss: 0.07204 valid_loss: 0.09111 test_loss: 0.09341 \n",
      "[248/300] train_loss: 0.07208 valid_loss: 0.08820 test_loss: 0.09217 \n",
      "[249/300] train_loss: 0.07047 valid_loss: 0.08879 test_loss: 0.09375 \n",
      "[250/300] train_loss: 0.07137 valid_loss: 0.08426 test_loss: 0.09373 \n",
      "[251/300] train_loss: 0.07216 valid_loss: 0.08805 test_loss: 0.09492 \n",
      "[252/300] train_loss: 0.07251 valid_loss: 0.09073 test_loss: 0.09327 \n",
      "[253/300] train_loss: 0.07016 valid_loss: 0.08447 test_loss: 0.09433 \n",
      "[254/300] train_loss: 0.07145 valid_loss: 0.09774 test_loss: 0.09312 \n",
      "[255/300] train_loss: 0.07234 valid_loss: 0.08839 test_loss: 0.09202 \n",
      "[256/300] train_loss: 0.07090 valid_loss: 0.08587 test_loss: 0.09332 \n",
      "[257/300] train_loss: 0.07075 valid_loss: 0.09201 test_loss: 0.09262 \n",
      "[258/300] train_loss: 0.06847 valid_loss: 0.09485 test_loss: 0.09431 \n",
      "[259/300] train_loss: 0.06875 valid_loss: 0.09661 test_loss: 0.09224 \n",
      "[260/300] train_loss: 0.07026 valid_loss: 0.09420 test_loss: 0.09329 \n",
      "[261/300] train_loss: 0.07021 valid_loss: 0.08258 test_loss: 0.09241 \n",
      "Validation loss decreased (0.082893 --> 0.082582).  Saving model ...\n",
      "[262/300] train_loss: 0.06882 valid_loss: 0.08086 test_loss: 0.09192 \n",
      "Validation loss decreased (0.082582 --> 0.080862).  Saving model ...\n",
      "[263/300] train_loss: 0.06808 valid_loss: 0.08056 test_loss: 0.09179 \n",
      "Validation loss decreased (0.080862 --> 0.080563).  Saving model ...\n",
      "[264/300] train_loss: 0.06939 valid_loss: 0.08435 test_loss: 0.09450 \n",
      "[265/300] train_loss: 0.07200 valid_loss: 0.08116 test_loss: 0.09217 \n",
      "[266/300] train_loss: 0.07164 valid_loss: 0.08796 test_loss: 0.09228 \n",
      "[267/300] train_loss: 0.06930 valid_loss: 0.08091 test_loss: 0.09331 \n",
      "[268/300] train_loss: 0.06846 valid_loss: 0.08233 test_loss: 0.09162 \n",
      "[269/300] train_loss: 0.06902 valid_loss: 0.08093 test_loss: 0.09174 \n",
      "[270/300] train_loss: 0.06885 valid_loss: 0.08199 test_loss: 0.09177 \n",
      "[271/300] train_loss: 0.07243 valid_loss: 0.07954 test_loss: 0.09244 \n",
      "Validation loss decreased (0.080563 --> 0.079541).  Saving model ...\n",
      "[272/300] train_loss: 0.07172 valid_loss: 0.08107 test_loss: 0.09249 \n",
      "[273/300] train_loss: 0.07079 valid_loss: 0.07990 test_loss: 0.09141 \n",
      "[274/300] train_loss: 0.06926 valid_loss: 0.07894 test_loss: 0.09201 \n",
      "Validation loss decreased (0.079541 --> 0.078936).  Saving model ...\n",
      "[275/300] train_loss: 0.06981 valid_loss: 0.08156 test_loss: 0.09133 \n",
      "[276/300] train_loss: 0.06733 valid_loss: 0.08002 test_loss: 0.09205 \n",
      "[277/300] train_loss: 0.06982 valid_loss: 0.08055 test_loss: 0.09224 \n",
      "[278/300] train_loss: 0.06802 valid_loss: 0.09194 test_loss: 0.09215 \n",
      "[279/300] train_loss: 0.06908 valid_loss: 0.08052 test_loss: 0.09125 \n",
      "[280/300] train_loss: 0.07029 valid_loss: 0.08500 test_loss: 0.09159 \n",
      "[281/300] train_loss: 0.06880 valid_loss: 0.08090 test_loss: 0.09217 \n",
      "[282/300] train_loss: 0.06747 valid_loss: 0.08054 test_loss: 0.09095 \n",
      "[283/300] train_loss: 0.06904 valid_loss: 0.08120 test_loss: 0.09105 \n",
      "[284/300] train_loss: 0.06769 valid_loss: 0.08032 test_loss: 0.09126 \n",
      "[285/300] train_loss: 0.06930 valid_loss: 0.08402 test_loss: 0.09050 \n",
      "[286/300] train_loss: 0.06812 valid_loss: 0.08263 test_loss: 0.09017 \n",
      "[287/300] train_loss: 0.06850 valid_loss: 0.08670 test_loss: 0.09011 \n",
      "[288/300] train_loss: 0.07021 valid_loss: 0.08638 test_loss: 0.09140 \n",
      "[289/300] train_loss: 0.07147 valid_loss: 0.08302 test_loss: 0.09027 \n",
      "[290/300] train_loss: 0.06967 valid_loss: 0.08083 test_loss: 0.09209 \n",
      "[291/300] train_loss: 0.07014 valid_loss: 0.09317 test_loss: 0.09072 \n",
      "[292/300] train_loss: 0.06757 valid_loss: 0.08403 test_loss: 0.09014 \n",
      "[293/300] train_loss: 0.06866 valid_loss: 0.08316 test_loss: 0.09031 \n",
      "[294/300] train_loss: 0.06778 valid_loss: 0.08694 test_loss: 0.09080 \n",
      "[295/300] train_loss: 0.06739 valid_loss: 0.08398 test_loss: 0.09029 \n",
      "[296/300] train_loss: 0.06724 valid_loss: 0.08177 test_loss: 0.09014 \n",
      "[297/300] train_loss: 0.06751 valid_loss: 0.08535 test_loss: 0.08969 \n",
      "[298/300] train_loss: 0.06888 valid_loss: 0.08368 test_loss: 0.09153 \n",
      "[299/300] train_loss: 0.07084 valid_loss: 0.09455 test_loss: 0.09068 \n",
      "[300/300] train_loss: 0.07003 valid_loss: 0.09027 test_loss: 0.08978 \n",
      "TRAINING MODEL 6\n",
      "[  1/300] train_loss: 0.55284 valid_loss: 0.47797 test_loss: 0.47515 \n",
      "Validation loss decreased (inf --> 0.477967).  Saving model ...\n",
      "[  2/300] train_loss: 0.39092 valid_loss: 0.35639 test_loss: 0.36150 \n",
      "Validation loss decreased (0.477967 --> 0.356388).  Saving model ...\n",
      "[  3/300] train_loss: 0.31047 valid_loss: 0.30641 test_loss: 0.31862 \n",
      "Validation loss decreased (0.356388 --> 0.306410).  Saving model ...\n",
      "[  4/300] train_loss: 0.27004 valid_loss: 0.27114 test_loss: 0.28885 \n",
      "Validation loss decreased (0.306410 --> 0.271136).  Saving model ...\n",
      "[  5/300] train_loss: 0.24267 valid_loss: 0.23993 test_loss: 0.25881 \n",
      "Validation loss decreased (0.271136 --> 0.239932).  Saving model ...\n",
      "[  6/300] train_loss: 0.21687 valid_loss: 0.22122 test_loss: 0.23833 \n",
      "Validation loss decreased (0.239932 --> 0.221218).  Saving model ...\n",
      "[  7/300] train_loss: 0.19878 valid_loss: 0.20531 test_loss: 0.22209 \n",
      "Validation loss decreased (0.221218 --> 0.205308).  Saving model ...\n",
      "[  8/300] train_loss: 0.18542 valid_loss: 0.19173 test_loss: 0.20646 \n",
      "Validation loss decreased (0.205308 --> 0.191732).  Saving model ...\n",
      "[  9/300] train_loss: 0.18227 valid_loss: 0.18118 test_loss: 0.19521 \n",
      "Validation loss decreased (0.191732 --> 0.181179).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16877 valid_loss: 0.17562 test_loss: 0.18865 \n",
      "Validation loss decreased (0.181179 --> 0.175623).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16085 valid_loss: 0.17041 test_loss: 0.18202 \n",
      "Validation loss decreased (0.175623 --> 0.170408).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15785 valid_loss: 0.16306 test_loss: 0.17564 \n",
      "Validation loss decreased (0.170408 --> 0.163062).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15109 valid_loss: 0.16076 test_loss: 0.17224 \n",
      "Validation loss decreased (0.163062 --> 0.160757).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14651 valid_loss: 0.15781 test_loss: 0.16839 \n",
      "Validation loss decreased (0.160757 --> 0.157814).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 15/300] train_loss: 0.14421 valid_loss: 0.15346 test_loss: 0.16372 \n",
      "Validation loss decreased (0.157814 --> 0.153456).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14564 valid_loss: 0.14858 test_loss: 0.16125 \n",
      "Validation loss decreased (0.153456 --> 0.148583).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13885 valid_loss: 0.14811 test_loss: 0.15996 \n",
      "Validation loss decreased (0.148583 --> 0.148109).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13697 valid_loss: 0.14714 test_loss: 0.15859 \n",
      "Validation loss decreased (0.148109 --> 0.147142).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13693 valid_loss: 0.14388 test_loss: 0.15553 \n",
      "Validation loss decreased (0.147142 --> 0.143878).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13045 valid_loss: 0.14093 test_loss: 0.15213 \n",
      "Validation loss decreased (0.143878 --> 0.140933).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12935 valid_loss: 0.13878 test_loss: 0.15098 \n",
      "Validation loss decreased (0.140933 --> 0.138784).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12662 valid_loss: 0.13589 test_loss: 0.14940 \n",
      "Validation loss decreased (0.138784 --> 0.135893).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12725 valid_loss: 0.13406 test_loss: 0.14945 \n",
      "Validation loss decreased (0.135893 --> 0.134061).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12686 valid_loss: 0.13477 test_loss: 0.14685 \n",
      "[ 25/300] train_loss: 0.12579 valid_loss: 0.13991 test_loss: 0.14693 \n",
      "[ 26/300] train_loss: 0.12463 valid_loss: 0.13334 test_loss: 0.14479 \n",
      "Validation loss decreased (0.134061 --> 0.133337).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12413 valid_loss: 0.13461 test_loss: 0.14449 \n",
      "[ 28/300] train_loss: 0.12227 valid_loss: 0.13036 test_loss: 0.14204 \n",
      "Validation loss decreased (0.133337 --> 0.130363).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12094 valid_loss: 0.12582 test_loss: 0.14110 \n",
      "Validation loss decreased (0.130363 --> 0.125821).  Saving model ...\n",
      "[ 30/300] train_loss: 0.12052 valid_loss: 0.12662 test_loss: 0.14215 \n",
      "[ 31/300] train_loss: 0.11523 valid_loss: 0.12656 test_loss: 0.14013 \n",
      "[ 32/300] train_loss: 0.11473 valid_loss: 0.12469 test_loss: 0.13938 \n",
      "Validation loss decreased (0.125821 --> 0.124685).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11666 valid_loss: 0.12657 test_loss: 0.13943 \n",
      "[ 34/300] train_loss: 0.11484 valid_loss: 0.12458 test_loss: 0.13640 \n",
      "Validation loss decreased (0.124685 --> 0.124581).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11492 valid_loss: 0.12374 test_loss: 0.13697 \n",
      "Validation loss decreased (0.124581 --> 0.123740).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11312 valid_loss: 0.12216 test_loss: 0.13566 \n",
      "Validation loss decreased (0.123740 --> 0.122164).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11452 valid_loss: 0.12304 test_loss: 0.13552 \n",
      "[ 38/300] train_loss: 0.10774 valid_loss: 0.12032 test_loss: 0.13456 \n",
      "Validation loss decreased (0.122164 --> 0.120320).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11102 valid_loss: 0.11723 test_loss: 0.13202 \n",
      "Validation loss decreased (0.120320 --> 0.117231).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11362 valid_loss: 0.11717 test_loss: 0.13254 \n",
      "Validation loss decreased (0.117231 --> 0.117166).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10975 valid_loss: 0.11793 test_loss: 0.13204 \n",
      "[ 42/300] train_loss: 0.11017 valid_loss: 0.11816 test_loss: 0.13272 \n",
      "[ 43/300] train_loss: 0.10881 valid_loss: 0.11425 test_loss: 0.12860 \n",
      "Validation loss decreased (0.117166 --> 0.114247).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10791 valid_loss: 0.11479 test_loss: 0.12836 \n",
      "[ 45/300] train_loss: 0.10825 valid_loss: 0.11671 test_loss: 0.12862 \n",
      "[ 46/300] train_loss: 0.10546 valid_loss: 0.11646 test_loss: 0.13069 \n",
      "[ 47/300] train_loss: 0.10845 valid_loss: 0.11564 test_loss: 0.12736 \n",
      "[ 48/300] train_loss: 0.10833 valid_loss: 0.11310 test_loss: 0.12649 \n",
      "Validation loss decreased (0.114247 --> 0.113104).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10709 valid_loss: 0.11382 test_loss: 0.12562 \n",
      "[ 50/300] train_loss: 0.10498 valid_loss: 0.11290 test_loss: 0.12535 \n",
      "Validation loss decreased (0.113104 --> 0.112899).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10312 valid_loss: 0.11316 test_loss: 0.12627 \n",
      "[ 52/300] train_loss: 0.10673 valid_loss: 0.11090 test_loss: 0.12470 \n",
      "Validation loss decreased (0.112899 --> 0.110904).  Saving model ...\n",
      "[ 53/300] train_loss: 0.10351 valid_loss: 0.10986 test_loss: 0.12593 \n",
      "Validation loss decreased (0.110904 --> 0.109862).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10202 valid_loss: 0.10991 test_loss: 0.12434 \n",
      "[ 55/300] train_loss: 0.10123 valid_loss: 0.10716 test_loss: 0.12389 \n",
      "Validation loss decreased (0.109862 --> 0.107156).  Saving model ...\n",
      "[ 56/300] train_loss: 0.10160 valid_loss: 0.10845 test_loss: 0.12298 \n",
      "[ 57/300] train_loss: 0.10164 valid_loss: 0.11083 test_loss: 0.12261 \n",
      "[ 58/300] train_loss: 0.09911 valid_loss: 0.10706 test_loss: 0.12298 \n",
      "Validation loss decreased (0.107156 --> 0.107064).  Saving model ...\n",
      "[ 59/300] train_loss: 0.10038 valid_loss: 0.10838 test_loss: 0.12198 \n",
      "[ 60/300] train_loss: 0.09793 valid_loss: 0.10817 test_loss: 0.12138 \n",
      "[ 61/300] train_loss: 0.09766 valid_loss: 0.10867 test_loss: 0.12124 \n",
      "[ 62/300] train_loss: 0.10164 valid_loss: 0.10611 test_loss: 0.11940 \n",
      "Validation loss decreased (0.107064 --> 0.106113).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09816 valid_loss: 0.11199 test_loss: 0.11981 \n",
      "[ 64/300] train_loss: 0.09987 valid_loss: 0.10740 test_loss: 0.11986 \n",
      "[ 65/300] train_loss: 0.09732 valid_loss: 0.10924 test_loss: 0.12037 \n",
      "[ 66/300] train_loss: 0.09738 valid_loss: 0.11007 test_loss: 0.11932 \n",
      "[ 67/300] train_loss: 0.09728 valid_loss: 0.10821 test_loss: 0.11839 \n",
      "[ 68/300] train_loss: 0.09499 valid_loss: 0.10856 test_loss: 0.11779 \n",
      "[ 69/300] train_loss: 0.09165 valid_loss: 0.10614 test_loss: 0.11764 \n",
      "[ 70/300] train_loss: 0.09623 valid_loss: 0.10715 test_loss: 0.11558 \n",
      "[ 71/300] train_loss: 0.09402 valid_loss: 0.10685 test_loss: 0.11558 \n",
      "[ 72/300] train_loss: 0.09584 valid_loss: 0.10477 test_loss: 0.11774 \n",
      "Validation loss decreased (0.106113 --> 0.104769).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09539 valid_loss: 0.10788 test_loss: 0.11765 \n",
      "[ 74/300] train_loss: 0.09471 valid_loss: 0.11177 test_loss: 0.11923 \n",
      "[ 75/300] train_loss: 0.09401 valid_loss: 0.10878 test_loss: 0.11522 \n",
      "[ 76/300] train_loss: 0.09430 valid_loss: 0.11038 test_loss: 0.11484 \n",
      "[ 77/300] train_loss: 0.09280 valid_loss: 0.10707 test_loss: 0.11550 \n",
      "[ 78/300] train_loss: 0.09277 valid_loss: 0.10081 test_loss: 0.11607 \n",
      "Validation loss decreased (0.104769 --> 0.100812).  Saving model ...\n",
      "[ 79/300] train_loss: 0.09567 valid_loss: 0.10190 test_loss: 0.11525 \n",
      "[ 80/300] train_loss: 0.09343 valid_loss: 0.10416 test_loss: 0.11470 \n",
      "[ 81/300] train_loss: 0.09336 valid_loss: 0.09936 test_loss: 0.11306 \n",
      "Validation loss decreased (0.100812 --> 0.099365).  Saving model ...\n",
      "[ 82/300] train_loss: 0.09046 valid_loss: 0.09848 test_loss: 0.11227 \n",
      "Validation loss decreased (0.099365 --> 0.098481).  Saving model ...\n",
      "[ 83/300] train_loss: 0.09074 valid_loss: 0.09871 test_loss: 0.11517 \n",
      "[ 84/300] train_loss: 0.09179 valid_loss: 0.10462 test_loss: 0.11264 \n",
      "[ 85/300] train_loss: 0.09330 valid_loss: 0.10818 test_loss: 0.11351 \n",
      "[ 86/300] train_loss: 0.09147 valid_loss: 0.10010 test_loss: 0.11268 \n",
      "[ 87/300] train_loss: 0.08908 valid_loss: 0.10267 test_loss: 0.11456 \n",
      "[ 88/300] train_loss: 0.09073 valid_loss: 0.09818 test_loss: 0.11091 \n",
      "Validation loss decreased (0.098481 --> 0.098176).  Saving model ...\n",
      "[ 89/300] train_loss: 0.09037 valid_loss: 0.10129 test_loss: 0.11311 \n",
      "[ 90/300] train_loss: 0.09048 valid_loss: 0.10057 test_loss: 0.11127 \n",
      "[ 91/300] train_loss: 0.08878 valid_loss: 0.10119 test_loss: 0.11398 \n",
      "[ 92/300] train_loss: 0.08841 valid_loss: 0.10085 test_loss: 0.11077 \n",
      "[ 93/300] train_loss: 0.08868 valid_loss: 0.09995 test_loss: 0.11321 \n",
      "[ 94/300] train_loss: 0.09050 valid_loss: 0.10085 test_loss: 0.11056 \n",
      "[ 95/300] train_loss: 0.09111 valid_loss: 0.09697 test_loss: 0.11002 \n",
      "Validation loss decreased (0.098176 --> 0.096971).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08956 valid_loss: 0.10192 test_loss: 0.11094 \n",
      "[ 97/300] train_loss: 0.09077 valid_loss: 0.09618 test_loss: 0.10871 \n",
      "Validation loss decreased (0.096971 --> 0.096178).  Saving model ...\n",
      "[ 98/300] train_loss: 0.08720 valid_loss: 0.09513 test_loss: 0.10882 \n",
      "Validation loss decreased (0.096178 --> 0.095126).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 99/300] train_loss: 0.08747 valid_loss: 0.10343 test_loss: 0.10926 \n",
      "[100/300] train_loss: 0.08748 valid_loss: 0.09473 test_loss: 0.10905 \n",
      "Validation loss decreased (0.095126 --> 0.094732).  Saving model ...\n",
      "[101/300] train_loss: 0.08817 valid_loss: 0.09484 test_loss: 0.11022 \n",
      "[102/300] train_loss: 0.08769 valid_loss: 0.09825 test_loss: 0.10822 \n",
      "[103/300] train_loss: 0.08602 valid_loss: 0.09922 test_loss: 0.10929 \n",
      "[104/300] train_loss: 0.08390 valid_loss: 0.09928 test_loss: 0.10926 \n",
      "[105/300] train_loss: 0.08454 valid_loss: 0.09612 test_loss: 0.10965 \n",
      "[106/300] train_loss: 0.08415 valid_loss: 0.09415 test_loss: 0.10889 \n",
      "Validation loss decreased (0.094732 --> 0.094154).  Saving model ...\n",
      "[107/300] train_loss: 0.08824 valid_loss: 0.10183 test_loss: 0.10869 \n",
      "[108/300] train_loss: 0.08506 valid_loss: 0.09812 test_loss: 0.10780 \n",
      "[109/300] train_loss: 0.08446 valid_loss: 0.09331 test_loss: 0.10834 \n",
      "Validation loss decreased (0.094154 --> 0.093311).  Saving model ...\n",
      "[110/300] train_loss: 0.08464 valid_loss: 0.09460 test_loss: 0.10741 \n",
      "[111/300] train_loss: 0.08828 valid_loss: 0.10101 test_loss: 0.10812 \n",
      "[112/300] train_loss: 0.08618 valid_loss: 0.09888 test_loss: 0.10796 \n",
      "[113/300] train_loss: 0.08504 valid_loss: 0.09747 test_loss: 0.10756 \n",
      "[114/300] train_loss: 0.08603 valid_loss: 0.09730 test_loss: 0.10742 \n",
      "[115/300] train_loss: 0.08544 valid_loss: 0.09456 test_loss: 0.10604 \n",
      "[116/300] train_loss: 0.08651 valid_loss: 0.09964 test_loss: 0.10706 \n",
      "[117/300] train_loss: 0.08196 valid_loss: 0.09698 test_loss: 0.10687 \n",
      "[118/300] train_loss: 0.08722 valid_loss: 0.09697 test_loss: 0.10620 \n",
      "[119/300] train_loss: 0.08417 valid_loss: 0.09275 test_loss: 0.10708 \n",
      "Validation loss decreased (0.093311 --> 0.092754).  Saving model ...\n",
      "[120/300] train_loss: 0.08268 valid_loss: 0.09808 test_loss: 0.10524 \n",
      "[121/300] train_loss: 0.08126 valid_loss: 0.09353 test_loss: 0.10590 \n",
      "[122/300] train_loss: 0.08497 valid_loss: 0.09780 test_loss: 0.10563 \n",
      "[123/300] train_loss: 0.08275 valid_loss: 0.09267 test_loss: 0.10524 \n",
      "Validation loss decreased (0.092754 --> 0.092673).  Saving model ...\n",
      "[124/300] train_loss: 0.08362 valid_loss: 0.09611 test_loss: 0.10467 \n",
      "[125/300] train_loss: 0.08367 valid_loss: 0.09223 test_loss: 0.10396 \n",
      "Validation loss decreased (0.092673 --> 0.092234).  Saving model ...\n",
      "[126/300] train_loss: 0.08430 valid_loss: 0.08955 test_loss: 0.10388 \n",
      "Validation loss decreased (0.092234 --> 0.089553).  Saving model ...\n",
      "[127/300] train_loss: 0.08513 valid_loss: 0.09014 test_loss: 0.10478 \n",
      "[128/300] train_loss: 0.08406 valid_loss: 0.09597 test_loss: 0.10606 \n",
      "[129/300] train_loss: 0.08290 valid_loss: 0.09652 test_loss: 0.10514 \n",
      "[130/300] train_loss: 0.08205 valid_loss: 0.08968 test_loss: 0.10439 \n",
      "[131/300] train_loss: 0.08224 valid_loss: 0.09255 test_loss: 0.10485 \n",
      "[132/300] train_loss: 0.08311 valid_loss: 0.09020 test_loss: 0.10418 \n",
      "[133/300] train_loss: 0.08272 valid_loss: 0.09241 test_loss: 0.10327 \n",
      "[134/300] train_loss: 0.08322 valid_loss: 0.09208 test_loss: 0.10378 \n",
      "[135/300] train_loss: 0.08117 valid_loss: 0.09351 test_loss: 0.10325 \n",
      "[136/300] train_loss: 0.08276 valid_loss: 0.08991 test_loss: 0.10372 \n",
      "[137/300] train_loss: 0.08009 valid_loss: 0.09021 test_loss: 0.10363 \n",
      "[138/300] train_loss: 0.08256 valid_loss: 0.09011 test_loss: 0.10319 \n",
      "[139/300] train_loss: 0.08284 valid_loss: 0.08923 test_loss: 0.10230 \n",
      "Validation loss decreased (0.089553 --> 0.089231).  Saving model ...\n",
      "[140/300] train_loss: 0.07988 valid_loss: 0.09211 test_loss: 0.10291 \n",
      "[141/300] train_loss: 0.08265 valid_loss: 0.09326 test_loss: 0.10285 \n",
      "[142/300] train_loss: 0.08319 valid_loss: 0.09007 test_loss: 0.10313 \n",
      "[143/300] train_loss: 0.08164 valid_loss: 0.08856 test_loss: 0.10247 \n",
      "Validation loss decreased (0.089231 --> 0.088562).  Saving model ...\n",
      "[144/300] train_loss: 0.07901 valid_loss: 0.09991 test_loss: 0.10224 \n",
      "[145/300] train_loss: 0.07800 valid_loss: 0.08750 test_loss: 0.10070 \n",
      "Validation loss decreased (0.088562 --> 0.087504).  Saving model ...\n",
      "[146/300] train_loss: 0.08163 valid_loss: 0.08867 test_loss: 0.10246 \n",
      "[147/300] train_loss: 0.08062 valid_loss: 0.09036 test_loss: 0.10185 \n",
      "[148/300] train_loss: 0.08037 valid_loss: 0.09233 test_loss: 0.10188 \n",
      "[149/300] train_loss: 0.07984 valid_loss: 0.08986 test_loss: 0.10216 \n",
      "[150/300] train_loss: 0.08144 valid_loss: 0.09155 test_loss: 0.10127 \n",
      "[151/300] train_loss: 0.08106 valid_loss: 0.09113 test_loss: 0.10024 \n",
      "[152/300] train_loss: 0.07839 valid_loss: 0.09336 test_loss: 0.10225 \n",
      "[153/300] train_loss: 0.07877 valid_loss: 0.09387 test_loss: 0.10326 \n",
      "[154/300] train_loss: 0.07850 valid_loss: 0.08719 test_loss: 0.10067 \n",
      "Validation loss decreased (0.087504 --> 0.087193).  Saving model ...\n",
      "[155/300] train_loss: 0.07813 valid_loss: 0.08988 test_loss: 0.10011 \n",
      "[156/300] train_loss: 0.07664 valid_loss: 0.08952 test_loss: 0.10061 \n",
      "[157/300] train_loss: 0.07939 valid_loss: 0.08796 test_loss: 0.10127 \n",
      "[158/300] train_loss: 0.07912 valid_loss: 0.08765 test_loss: 0.10065 \n",
      "[159/300] train_loss: 0.07872 valid_loss: 0.08758 test_loss: 0.10040 \n",
      "[160/300] train_loss: 0.07891 valid_loss: 0.09547 test_loss: 0.10188 \n",
      "[161/300] train_loss: 0.07990 valid_loss: 0.08874 test_loss: 0.10214 \n",
      "[162/300] train_loss: 0.08087 valid_loss: 0.08884 test_loss: 0.10097 \n",
      "[163/300] train_loss: 0.07923 valid_loss: 0.08815 test_loss: 0.10056 \n",
      "[164/300] train_loss: 0.07776 valid_loss: 0.08616 test_loss: 0.09986 \n",
      "Validation loss decreased (0.087193 --> 0.086159).  Saving model ...\n",
      "[165/300] train_loss: 0.07710 valid_loss: 0.08774 test_loss: 0.10064 \n",
      "[166/300] train_loss: 0.07718 valid_loss: 0.08677 test_loss: 0.09962 \n",
      "[167/300] train_loss: 0.07752 valid_loss: 0.08868 test_loss: 0.10108 \n",
      "[168/300] train_loss: 0.07771 valid_loss: 0.08897 test_loss: 0.10092 \n",
      "[169/300] train_loss: 0.08158 valid_loss: 0.09573 test_loss: 0.09868 \n",
      "[170/300] train_loss: 0.07964 valid_loss: 0.08535 test_loss: 0.09809 \n",
      "Validation loss decreased (0.086159 --> 0.085351).  Saving model ...\n",
      "[171/300] train_loss: 0.07487 valid_loss: 0.08648 test_loss: 0.10002 \n",
      "[172/300] train_loss: 0.07559 valid_loss: 0.08603 test_loss: 0.09782 \n",
      "[173/300] train_loss: 0.07775 valid_loss: 0.09091 test_loss: 0.10002 \n",
      "[174/300] train_loss: 0.07743 valid_loss: 0.09071 test_loss: 0.10011 \n",
      "[175/300] train_loss: 0.07680 valid_loss: 0.09245 test_loss: 0.09991 \n",
      "[176/300] train_loss: 0.07704 valid_loss: 0.09353 test_loss: 0.10007 \n",
      "[177/300] train_loss: 0.07382 valid_loss: 0.08709 test_loss: 0.10011 \n",
      "[178/300] train_loss: 0.08108 valid_loss: 0.09168 test_loss: 0.10019 \n",
      "[179/300] train_loss: 0.07419 valid_loss: 0.09454 test_loss: 0.09975 \n",
      "[180/300] train_loss: 0.07693 valid_loss: 0.09241 test_loss: 0.09958 \n",
      "[181/300] train_loss: 0.07792 valid_loss: 0.09655 test_loss: 0.09846 \n",
      "[182/300] train_loss: 0.07711 valid_loss: 0.09748 test_loss: 0.09876 \n",
      "[183/300] train_loss: 0.07504 valid_loss: 0.08581 test_loss: 0.09832 \n",
      "[184/300] train_loss: 0.07500 valid_loss: 0.09103 test_loss: 0.09906 \n",
      "[185/300] train_loss: 0.07481 valid_loss: 0.09297 test_loss: 0.09883 \n",
      "[186/300] train_loss: 0.07785 valid_loss: 0.09664 test_loss: 0.09835 \n",
      "[187/300] train_loss: 0.07534 valid_loss: 0.08818 test_loss: 0.09687 \n",
      "[188/300] train_loss: 0.07550 valid_loss: 0.09278 test_loss: 0.09787 \n",
      "[189/300] train_loss: 0.07433 valid_loss: 0.10048 test_loss: 0.09912 \n",
      "[190/300] train_loss: 0.07748 valid_loss: 0.08864 test_loss: 0.09841 \n",
      "[191/300] train_loss: 0.07420 valid_loss: 0.08873 test_loss: 0.09756 \n",
      "[192/300] train_loss: 0.07552 valid_loss: 0.08861 test_loss: 0.09800 \n",
      "[193/300] train_loss: 0.07391 valid_loss: 0.09266 test_loss: 0.09859 \n",
      "[194/300] train_loss: 0.07452 valid_loss: 0.08446 test_loss: 0.09822 \n",
      "Validation loss decreased (0.085351 --> 0.084456).  Saving model ...\n",
      "[195/300] train_loss: 0.07595 valid_loss: 0.09413 test_loss: 0.09765 \n",
      "[196/300] train_loss: 0.07510 valid_loss: 0.08909 test_loss: 0.09970 \n",
      "[197/300] train_loss: 0.07775 valid_loss: 0.08928 test_loss: 0.09669 \n",
      "[198/300] train_loss: 0.07589 valid_loss: 0.08864 test_loss: 0.09803 \n",
      "[199/300] train_loss: 0.07559 valid_loss: 0.09504 test_loss: 0.09692 \n",
      "[200/300] train_loss: 0.07564 valid_loss: 0.08921 test_loss: 0.09624 \n",
      "[201/300] train_loss: 0.07395 valid_loss: 0.09230 test_loss: 0.09805 \n",
      "[202/300] train_loss: 0.07381 valid_loss: 0.09121 test_loss: 0.09831 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203/300] train_loss: 0.07314 valid_loss: 0.09099 test_loss: 0.09673 \n",
      "[204/300] train_loss: 0.07475 valid_loss: 0.08670 test_loss: 0.09626 \n",
      "[205/300] train_loss: 0.07375 valid_loss: 0.09827 test_loss: 0.09704 \n",
      "[206/300] train_loss: 0.07322 valid_loss: 0.09398 test_loss: 0.09689 \n",
      "[207/300] train_loss: 0.07609 valid_loss: 0.08947 test_loss: 0.09614 \n",
      "[208/300] train_loss: 0.07374 valid_loss: 0.08608 test_loss: 0.09698 \n",
      "[209/300] train_loss: 0.07339 valid_loss: 0.08787 test_loss: 0.09742 \n",
      "[210/300] train_loss: 0.07308 valid_loss: 0.08894 test_loss: 0.09701 \n",
      "[211/300] train_loss: 0.07422 valid_loss: 0.09143 test_loss: 0.09588 \n",
      "[212/300] train_loss: 0.07188 valid_loss: 0.08641 test_loss: 0.09610 \n",
      "[213/300] train_loss: 0.07475 valid_loss: 0.08695 test_loss: 0.09670 \n",
      "[214/300] train_loss: 0.07250 valid_loss: 0.08563 test_loss: 0.09651 \n",
      "[215/300] train_loss: 0.07168 valid_loss: 0.08721 test_loss: 0.09867 \n",
      "[216/300] train_loss: 0.07369 valid_loss: 0.08637 test_loss: 0.09645 \n",
      "[217/300] train_loss: 0.07417 valid_loss: 0.08887 test_loss: 0.09534 \n",
      "[218/300] train_loss: 0.07235 valid_loss: 0.08666 test_loss: 0.09598 \n",
      "[219/300] train_loss: 0.07468 valid_loss: 0.08695 test_loss: 0.09632 \n",
      "[220/300] train_loss: 0.07357 valid_loss: 0.09041 test_loss: 0.09507 \n",
      "[221/300] train_loss: 0.07249 valid_loss: 0.09876 test_loss: 0.09759 \n",
      "[222/300] train_loss: 0.07342 valid_loss: 0.08945 test_loss: 0.09580 \n",
      "[223/300] train_loss: 0.07508 valid_loss: 0.08242 test_loss: 0.09562 \n",
      "Validation loss decreased (0.084456 --> 0.082421).  Saving model ...\n",
      "[224/300] train_loss: 0.07397 valid_loss: 0.08814 test_loss: 0.09570 \n",
      "[225/300] train_loss: 0.07062 valid_loss: 0.08546 test_loss: 0.09593 \n",
      "[226/300] train_loss: 0.07228 valid_loss: 0.08896 test_loss: 0.09593 \n",
      "[227/300] train_loss: 0.07138 valid_loss: 0.08550 test_loss: 0.09506 \n",
      "[228/300] train_loss: 0.07290 valid_loss: 0.08754 test_loss: 0.09559 \n",
      "[229/300] train_loss: 0.07206 valid_loss: 0.08397 test_loss: 0.09584 \n",
      "[230/300] train_loss: 0.06917 valid_loss: 0.08545 test_loss: 0.09577 \n",
      "[231/300] train_loss: 0.06996 valid_loss: 0.08641 test_loss: 0.09581 \n",
      "[232/300] train_loss: 0.07123 valid_loss: 0.08281 test_loss: 0.09568 \n",
      "[233/300] train_loss: 0.07088 valid_loss: 0.08908 test_loss: 0.09385 \n",
      "[234/300] train_loss: 0.07332 valid_loss: 0.09417 test_loss: 0.09572 \n",
      "[235/300] train_loss: 0.07381 valid_loss: 0.10070 test_loss: 0.09467 \n",
      "[236/300] train_loss: 0.07133 valid_loss: 0.09636 test_loss: 0.09641 \n",
      "[237/300] train_loss: 0.07056 valid_loss: 0.08572 test_loss: 0.09413 \n",
      "[238/300] train_loss: 0.07159 valid_loss: 0.08389 test_loss: 0.09402 \n",
      "[239/300] train_loss: 0.06924 valid_loss: 0.08514 test_loss: 0.09423 \n",
      "[240/300] train_loss: 0.07157 valid_loss: 0.08207 test_loss: 0.09518 \n",
      "Validation loss decreased (0.082421 --> 0.082068).  Saving model ...\n",
      "[241/300] train_loss: 0.07190 valid_loss: 0.08473 test_loss: 0.09465 \n",
      "[242/300] train_loss: 0.07080 valid_loss: 0.08543 test_loss: 0.09475 \n",
      "[243/300] train_loss: 0.07075 valid_loss: 0.08543 test_loss: 0.09444 \n",
      "[244/300] train_loss: 0.06746 valid_loss: 0.08704 test_loss: 0.09501 \n",
      "[245/300] train_loss: 0.07294 valid_loss: 0.08711 test_loss: 0.09473 \n",
      "[246/300] train_loss: 0.07035 valid_loss: 0.08510 test_loss: 0.09533 \n",
      "[247/300] train_loss: 0.07064 valid_loss: 0.08510 test_loss: 0.09446 \n",
      "[248/300] train_loss: 0.07136 valid_loss: 0.08943 test_loss: 0.09564 \n",
      "[249/300] train_loss: 0.07142 valid_loss: 0.08073 test_loss: 0.09541 \n",
      "Validation loss decreased (0.082068 --> 0.080729).  Saving model ...\n",
      "[250/300] train_loss: 0.07220 valid_loss: 0.08082 test_loss: 0.09373 \n",
      "[251/300] train_loss: 0.07013 valid_loss: 0.08378 test_loss: 0.09561 \n",
      "[252/300] train_loss: 0.07222 valid_loss: 0.08142 test_loss: 0.09415 \n",
      "[253/300] train_loss: 0.06941 valid_loss: 0.08153 test_loss: 0.09562 \n",
      "[254/300] train_loss: 0.07087 valid_loss: 0.08256 test_loss: 0.09455 \n",
      "[255/300] train_loss: 0.06970 valid_loss: 0.09110 test_loss: 0.09438 \n",
      "[256/300] train_loss: 0.07179 valid_loss: 0.08245 test_loss: 0.09389 \n",
      "[257/300] train_loss: 0.06892 valid_loss: 0.08994 test_loss: 0.09426 \n",
      "[258/300] train_loss: 0.06988 valid_loss: 0.08139 test_loss: 0.09328 \n",
      "[259/300] train_loss: 0.07203 valid_loss: 0.08100 test_loss: 0.09358 \n",
      "[260/300] train_loss: 0.07066 valid_loss: 0.08170 test_loss: 0.09426 \n",
      "[261/300] train_loss: 0.07060 valid_loss: 0.08362 test_loss: 0.09342 \n",
      "[262/300] train_loss: 0.06950 valid_loss: 0.09732 test_loss: 0.09379 \n",
      "[263/300] train_loss: 0.07164 valid_loss: 0.08732 test_loss: 0.09405 \n",
      "[264/300] train_loss: 0.07093 valid_loss: 0.10166 test_loss: 0.09344 \n",
      "[265/300] train_loss: 0.06826 valid_loss: 0.08707 test_loss: 0.09298 \n",
      "[266/300] train_loss: 0.07000 valid_loss: 0.08201 test_loss: 0.09386 \n",
      "[267/300] train_loss: 0.06861 valid_loss: 0.08232 test_loss: 0.09439 \n",
      "[268/300] train_loss: 0.07018 valid_loss: 0.08617 test_loss: 0.09376 \n",
      "[269/300] train_loss: 0.06939 valid_loss: 0.08295 test_loss: 0.09244 \n",
      "[270/300] train_loss: 0.06963 valid_loss: 0.08069 test_loss: 0.09374 \n",
      "Validation loss decreased (0.080729 --> 0.080689).  Saving model ...\n",
      "[271/300] train_loss: 0.07109 valid_loss: 0.09304 test_loss: 0.09390 \n",
      "[272/300] train_loss: 0.06677 valid_loss: 0.08421 test_loss: 0.09368 \n",
      "[273/300] train_loss: 0.06839 valid_loss: 0.08141 test_loss: 0.09375 \n",
      "[274/300] train_loss: 0.06844 valid_loss: 0.08312 test_loss: 0.09203 \n",
      "[275/300] train_loss: 0.06663 valid_loss: 0.08027 test_loss: 0.09324 \n",
      "Validation loss decreased (0.080689 --> 0.080273).  Saving model ...\n",
      "[276/300] train_loss: 0.06995 valid_loss: 0.07888 test_loss: 0.09231 \n",
      "Validation loss decreased (0.080273 --> 0.078881).  Saving model ...\n",
      "[277/300] train_loss: 0.06884 valid_loss: 0.07937 test_loss: 0.09234 \n",
      "[278/300] train_loss: 0.07241 valid_loss: 0.08519 test_loss: 0.09332 \n",
      "[279/300] train_loss: 0.06870 valid_loss: 0.07963 test_loss: 0.09180 \n",
      "[280/300] train_loss: 0.06864 valid_loss: 0.08206 test_loss: 0.09348 \n",
      "[281/300] train_loss: 0.07052 valid_loss: 0.07968 test_loss: 0.09139 \n",
      "[282/300] train_loss: 0.06908 valid_loss: 0.08034 test_loss: 0.09260 \n",
      "[283/300] train_loss: 0.06921 valid_loss: 0.08039 test_loss: 0.09235 \n",
      "[284/300] train_loss: 0.06971 valid_loss: 0.08003 test_loss: 0.09229 \n",
      "[285/300] train_loss: 0.06944 valid_loss: 0.07970 test_loss: 0.09186 \n",
      "[286/300] train_loss: 0.06839 valid_loss: 0.09227 test_loss: 0.09403 \n",
      "[287/300] train_loss: 0.07047 valid_loss: 0.08200 test_loss: 0.09276 \n",
      "[288/300] train_loss: 0.06885 valid_loss: 0.08221 test_loss: 0.09261 \n",
      "[289/300] train_loss: 0.06717 valid_loss: 0.08292 test_loss: 0.09225 \n",
      "[290/300] train_loss: 0.07096 valid_loss: 0.08535 test_loss: 0.09239 \n",
      "[291/300] train_loss: 0.06945 valid_loss: 0.08136 test_loss: 0.09418 \n",
      "[292/300] train_loss: 0.06799 valid_loss: 0.07885 test_loss: 0.09184 \n",
      "Validation loss decreased (0.078881 --> 0.078854).  Saving model ...\n",
      "[293/300] train_loss: 0.06809 valid_loss: 0.08232 test_loss: 0.09260 \n",
      "[294/300] train_loss: 0.06564 valid_loss: 0.08529 test_loss: 0.09230 \n",
      "[295/300] train_loss: 0.06734 valid_loss: 0.08304 test_loss: 0.09223 \n",
      "[296/300] train_loss: 0.06969 valid_loss: 0.08330 test_loss: 0.09260 \n",
      "[297/300] train_loss: 0.06883 valid_loss: 0.07937 test_loss: 0.09185 \n",
      "[298/300] train_loss: 0.06622 valid_loss: 0.08210 test_loss: 0.09227 \n",
      "[299/300] train_loss: 0.06732 valid_loss: 0.08275 test_loss: 0.09174 \n",
      "[300/300] train_loss: 0.06620 valid_loss: 0.08123 test_loss: 0.09246 \n",
      "TRAINING MODEL 7\n",
      "[  1/300] train_loss: 0.64146 valid_loss: 0.57691 test_loss: 0.57427 \n",
      "Validation loss decreased (inf --> 0.576915).  Saving model ...\n",
      "[  2/300] train_loss: 0.48947 valid_loss: 0.42793 test_loss: 0.43036 \n",
      "Validation loss decreased (0.576915 --> 0.427934).  Saving model ...\n",
      "[  3/300] train_loss: 0.37608 valid_loss: 0.35732 test_loss: 0.35941 \n",
      "Validation loss decreased (0.427934 --> 0.357324).  Saving model ...\n",
      "[  4/300] train_loss: 0.31717 valid_loss: 0.30901 test_loss: 0.32025 \n",
      "Validation loss decreased (0.357324 --> 0.309012).  Saving model ...\n",
      "[  5/300] train_loss: 0.27562 valid_loss: 0.27341 test_loss: 0.28720 \n",
      "Validation loss decreased (0.309012 --> 0.273415).  Saving model ...\n",
      "[  6/300] train_loss: 0.24595 valid_loss: 0.24185 test_loss: 0.25718 \n",
      "Validation loss decreased (0.273415 --> 0.241849).  Saving model ...\n",
      "[  7/300] train_loss: 0.22210 valid_loss: 0.22205 test_loss: 0.23611 \n",
      "Validation loss decreased (0.241849 --> 0.222054).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8/300] train_loss: 0.20417 valid_loss: 0.20190 test_loss: 0.21748 \n",
      "Validation loss decreased (0.222054 --> 0.201897).  Saving model ...\n",
      "[  9/300] train_loss: 0.18815 valid_loss: 0.19119 test_loss: 0.20421 \n",
      "Validation loss decreased (0.201897 --> 0.191189).  Saving model ...\n",
      "[ 10/300] train_loss: 0.18135 valid_loss: 0.17911 test_loss: 0.19449 \n",
      "Validation loss decreased (0.191189 --> 0.179106).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16827 valid_loss: 0.17441 test_loss: 0.18685 \n",
      "Validation loss decreased (0.179106 --> 0.174407).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16311 valid_loss: 0.16568 test_loss: 0.18012 \n",
      "Validation loss decreased (0.174407 --> 0.165679).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15615 valid_loss: 0.16102 test_loss: 0.17550 \n",
      "Validation loss decreased (0.165679 --> 0.161021).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15269 valid_loss: 0.15552 test_loss: 0.17179 \n",
      "Validation loss decreased (0.161021 --> 0.155523).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14622 valid_loss: 0.15635 test_loss: 0.16885 \n",
      "[ 16/300] train_loss: 0.14356 valid_loss: 0.15300 test_loss: 0.16553 \n",
      "Validation loss decreased (0.155523 --> 0.153000).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14155 valid_loss: 0.15146 test_loss: 0.16209 \n",
      "Validation loss decreased (0.153000 --> 0.151456).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13865 valid_loss: 0.14613 test_loss: 0.15840 \n",
      "Validation loss decreased (0.151456 --> 0.146126).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13540 valid_loss: 0.14635 test_loss: 0.15630 \n",
      "[ 20/300] train_loss: 0.13517 valid_loss: 0.14410 test_loss: 0.15604 \n",
      "Validation loss decreased (0.146126 --> 0.144100).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12916 valid_loss: 0.14391 test_loss: 0.15404 \n",
      "Validation loss decreased (0.144100 --> 0.143907).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13341 valid_loss: 0.14066 test_loss: 0.15263 \n",
      "Validation loss decreased (0.143907 --> 0.140661).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12715 valid_loss: 0.13827 test_loss: 0.14976 \n",
      "Validation loss decreased (0.140661 --> 0.138268).  Saving model ...\n",
      "[ 24/300] train_loss: 0.13019 valid_loss: 0.13248 test_loss: 0.14914 \n",
      "Validation loss decreased (0.138268 --> 0.132479).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12785 valid_loss: 0.13283 test_loss: 0.14709 \n",
      "[ 26/300] train_loss: 0.12387 valid_loss: 0.13294 test_loss: 0.14686 \n",
      "[ 27/300] train_loss: 0.12364 valid_loss: 0.13265 test_loss: 0.14761 \n",
      "[ 28/300] train_loss: 0.12122 valid_loss: 0.12959 test_loss: 0.14664 \n",
      "Validation loss decreased (0.132479 --> 0.129594).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11889 valid_loss: 0.13000 test_loss: 0.14271 \n",
      "[ 30/300] train_loss: 0.11760 valid_loss: 0.12765 test_loss: 0.14392 \n",
      "Validation loss decreased (0.129594 --> 0.127651).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11732 valid_loss: 0.13600 test_loss: 0.15480 \n",
      "[ 32/300] train_loss: 0.11991 valid_loss: 0.12721 test_loss: 0.14254 \n",
      "Validation loss decreased (0.127651 --> 0.127208).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11553 valid_loss: 0.12791 test_loss: 0.14005 \n",
      "[ 34/300] train_loss: 0.11414 valid_loss: 0.12560 test_loss: 0.13925 \n",
      "Validation loss decreased (0.127208 --> 0.125602).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11371 valid_loss: 0.12517 test_loss: 0.13757 \n",
      "Validation loss decreased (0.125602 --> 0.125171).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11612 valid_loss: 0.12698 test_loss: 0.14016 \n",
      "[ 37/300] train_loss: 0.11515 valid_loss: 0.12230 test_loss: 0.13560 \n",
      "Validation loss decreased (0.125171 --> 0.122304).  Saving model ...\n",
      "[ 38/300] train_loss: 0.10971 valid_loss: 0.12273 test_loss: 0.13518 \n",
      "[ 39/300] train_loss: 0.11295 valid_loss: 0.12406 test_loss: 0.13522 \n",
      "[ 40/300] train_loss: 0.11036 valid_loss: 0.11912 test_loss: 0.13500 \n",
      "Validation loss decreased (0.122304 --> 0.119123).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10831 valid_loss: 0.12239 test_loss: 0.13377 \n",
      "[ 42/300] train_loss: 0.11103 valid_loss: 0.11603 test_loss: 0.13101 \n",
      "Validation loss decreased (0.119123 --> 0.116032).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10930 valid_loss: 0.11526 test_loss: 0.13015 \n",
      "Validation loss decreased (0.116032 --> 0.115259).  Saving model ...\n",
      "[ 44/300] train_loss: 0.11091 valid_loss: 0.11535 test_loss: 0.13014 \n",
      "[ 45/300] train_loss: 0.10825 valid_loss: 0.11567 test_loss: 0.12952 \n",
      "[ 46/300] train_loss: 0.10492 valid_loss: 0.11881 test_loss: 0.12953 \n",
      "[ 47/300] train_loss: 0.10537 valid_loss: 0.11740 test_loss: 0.12997 \n",
      "[ 48/300] train_loss: 0.10616 valid_loss: 0.11535 test_loss: 0.12994 \n",
      "[ 49/300] train_loss: 0.10760 valid_loss: 0.11764 test_loss: 0.12712 \n",
      "[ 50/300] train_loss: 0.10503 valid_loss: 0.11679 test_loss: 0.12775 \n",
      "[ 51/300] train_loss: 0.10445 valid_loss: 0.11313 test_loss: 0.12841 \n",
      "Validation loss decreased (0.115259 --> 0.113127).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10497 valid_loss: 0.11414 test_loss: 0.12946 \n",
      "[ 53/300] train_loss: 0.10200 valid_loss: 0.11100 test_loss: 0.12620 \n",
      "Validation loss decreased (0.113127 --> 0.111001).  Saving model ...\n",
      "[ 54/300] train_loss: 0.10262 valid_loss: 0.11519 test_loss: 0.12604 \n",
      "[ 55/300] train_loss: 0.10088 valid_loss: 0.11293 test_loss: 0.12628 \n",
      "[ 56/300] train_loss: 0.10178 valid_loss: 0.11278 test_loss: 0.12581 \n",
      "[ 57/300] train_loss: 0.10165 valid_loss: 0.11559 test_loss: 0.12479 \n",
      "[ 58/300] train_loss: 0.09946 valid_loss: 0.11080 test_loss: 0.12513 \n",
      "Validation loss decreased (0.111001 --> 0.110804).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09845 valid_loss: 0.11144 test_loss: 0.12438 \n",
      "[ 60/300] train_loss: 0.10068 valid_loss: 0.11440 test_loss: 0.12306 \n",
      "[ 61/300] train_loss: 0.09914 valid_loss: 0.11010 test_loss: 0.12142 \n",
      "Validation loss decreased (0.110804 --> 0.110101).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09643 valid_loss: 0.10891 test_loss: 0.12157 \n",
      "Validation loss decreased (0.110101 --> 0.108909).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09868 valid_loss: 0.10964 test_loss: 0.12206 \n",
      "[ 64/300] train_loss: 0.09868 valid_loss: 0.10744 test_loss: 0.12228 \n",
      "Validation loss decreased (0.108909 --> 0.107438).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09873 valid_loss: 0.11203 test_loss: 0.12181 \n",
      "[ 66/300] train_loss: 0.10175 valid_loss: 0.10616 test_loss: 0.12041 \n",
      "Validation loss decreased (0.107438 --> 0.106162).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09840 valid_loss: 0.10745 test_loss: 0.12122 \n",
      "[ 68/300] train_loss: 0.09618 valid_loss: 0.10701 test_loss: 0.11968 \n",
      "[ 69/300] train_loss: 0.09682 valid_loss: 0.10879 test_loss: 0.11893 \n",
      "[ 70/300] train_loss: 0.09339 valid_loss: 0.10625 test_loss: 0.12054 \n",
      "[ 71/300] train_loss: 0.09714 valid_loss: 0.10340 test_loss: 0.11785 \n",
      "Validation loss decreased (0.106162 --> 0.103398).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09494 valid_loss: 0.10388 test_loss: 0.11886 \n",
      "[ 73/300] train_loss: 0.09497 valid_loss: 0.10453 test_loss: 0.11896 \n",
      "[ 74/300] train_loss: 0.09506 valid_loss: 0.10317 test_loss: 0.11782 \n",
      "Validation loss decreased (0.103398 --> 0.103171).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09520 valid_loss: 0.10239 test_loss: 0.11590 \n",
      "Validation loss decreased (0.103171 --> 0.102389).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09052 valid_loss: 0.10298 test_loss: 0.11665 \n",
      "[ 77/300] train_loss: 0.09338 valid_loss: 0.10074 test_loss: 0.11511 \n",
      "Validation loss decreased (0.102389 --> 0.100743).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09247 valid_loss: 0.10078 test_loss: 0.11574 \n",
      "[ 79/300] train_loss: 0.09643 valid_loss: 0.10478 test_loss: 0.11666 \n",
      "[ 80/300] train_loss: 0.09284 valid_loss: 0.10407 test_loss: 0.11846 \n",
      "[ 81/300] train_loss: 0.09104 valid_loss: 0.10677 test_loss: 0.11670 \n",
      "[ 82/300] train_loss: 0.09466 valid_loss: 0.10396 test_loss: 0.11485 \n",
      "[ 83/300] train_loss: 0.09072 valid_loss: 0.10184 test_loss: 0.11497 \n",
      "[ 84/300] train_loss: 0.09120 valid_loss: 0.10681 test_loss: 0.11563 \n",
      "[ 85/300] train_loss: 0.09328 valid_loss: 0.10813 test_loss: 0.11574 \n",
      "[ 86/300] train_loss: 0.08974 valid_loss: 0.10458 test_loss: 0.11614 \n",
      "[ 87/300] train_loss: 0.09321 valid_loss: 0.10020 test_loss: 0.11359 \n",
      "Validation loss decreased (0.100743 --> 0.100202).  Saving model ...\n",
      "[ 88/300] train_loss: 0.09085 valid_loss: 0.09834 test_loss: 0.11245 \n",
      "Validation loss decreased (0.100202 --> 0.098338).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89/300] train_loss: 0.09113 valid_loss: 0.10803 test_loss: 0.11364 \n",
      "[ 90/300] train_loss: 0.09067 valid_loss: 0.10151 test_loss: 0.11315 \n",
      "[ 91/300] train_loss: 0.09249 valid_loss: 0.10365 test_loss: 0.11620 \n",
      "[ 92/300] train_loss: 0.09039 valid_loss: 0.10114 test_loss: 0.11435 \n",
      "[ 93/300] train_loss: 0.08777 valid_loss: 0.09908 test_loss: 0.11196 \n",
      "[ 94/300] train_loss: 0.08775 valid_loss: 0.10377 test_loss: 0.11154 \n",
      "[ 95/300] train_loss: 0.08849 valid_loss: 0.10028 test_loss: 0.11290 \n",
      "[ 96/300] train_loss: 0.09195 valid_loss: 0.09767 test_loss: 0.11090 \n",
      "Validation loss decreased (0.098338 --> 0.097668).  Saving model ...\n",
      "[ 97/300] train_loss: 0.08978 valid_loss: 0.09807 test_loss: 0.11255 \n",
      "[ 98/300] train_loss: 0.08914 valid_loss: 0.10334 test_loss: 0.11312 \n",
      "[ 99/300] train_loss: 0.08916 valid_loss: 0.09718 test_loss: 0.11132 \n",
      "Validation loss decreased (0.097668 --> 0.097176).  Saving model ...\n",
      "[100/300] train_loss: 0.08786 valid_loss: 0.09782 test_loss: 0.11095 \n",
      "[101/300] train_loss: 0.08554 valid_loss: 0.09624 test_loss: 0.11032 \n",
      "Validation loss decreased (0.097176 --> 0.096237).  Saving model ...\n",
      "[102/300] train_loss: 0.08779 valid_loss: 0.09974 test_loss: 0.10971 \n",
      "[103/300] train_loss: 0.08584 valid_loss: 0.09898 test_loss: 0.11137 \n",
      "[104/300] train_loss: 0.08582 valid_loss: 0.09682 test_loss: 0.11012 \n",
      "[105/300] train_loss: 0.08736 valid_loss: 0.09757 test_loss: 0.10995 \n",
      "[106/300] train_loss: 0.08704 valid_loss: 0.10216 test_loss: 0.11207 \n",
      "[107/300] train_loss: 0.08456 valid_loss: 0.09583 test_loss: 0.10917 \n",
      "Validation loss decreased (0.096237 --> 0.095832).  Saving model ...\n",
      "[108/300] train_loss: 0.08539 valid_loss: 0.09879 test_loss: 0.11147 \n",
      "[109/300] train_loss: 0.08614 valid_loss: 0.09734 test_loss: 0.11000 \n",
      "[110/300] train_loss: 0.08683 valid_loss: 0.10144 test_loss: 0.11138 \n",
      "[111/300] train_loss: 0.08271 valid_loss: 0.09905 test_loss: 0.10919 \n",
      "[112/300] train_loss: 0.08487 valid_loss: 0.09626 test_loss: 0.10967 \n",
      "[113/300] train_loss: 0.08616 valid_loss: 0.10009 test_loss: 0.10923 \n",
      "[114/300] train_loss: 0.08517 valid_loss: 0.09936 test_loss: 0.10910 \n",
      "[115/300] train_loss: 0.08608 valid_loss: 0.09677 test_loss: 0.10756 \n",
      "[116/300] train_loss: 0.08375 valid_loss: 0.10105 test_loss: 0.10842 \n",
      "[117/300] train_loss: 0.08542 valid_loss: 0.11123 test_loss: 0.10717 \n",
      "[118/300] train_loss: 0.08551 valid_loss: 0.10565 test_loss: 0.10760 \n",
      "[119/300] train_loss: 0.08580 valid_loss: 0.09850 test_loss: 0.10744 \n",
      "[120/300] train_loss: 0.08434 valid_loss: 0.09417 test_loss: 0.10781 \n",
      "Validation loss decreased (0.095832 --> 0.094172).  Saving model ...\n",
      "[121/300] train_loss: 0.08608 valid_loss: 0.09565 test_loss: 0.10849 \n",
      "[122/300] train_loss: 0.08430 valid_loss: 0.10779 test_loss: 0.10735 \n",
      "[123/300] train_loss: 0.08333 valid_loss: 0.10041 test_loss: 0.10687 \n",
      "[124/300] train_loss: 0.08143 valid_loss: 0.10027 test_loss: 0.10835 \n",
      "[125/300] train_loss: 0.08537 valid_loss: 0.10874 test_loss: 0.10701 \n",
      "[126/300] train_loss: 0.08746 valid_loss: 0.09444 test_loss: 0.10525 \n",
      "[127/300] train_loss: 0.08285 valid_loss: 0.09829 test_loss: 0.10607 \n",
      "[128/300] train_loss: 0.08571 valid_loss: 0.10149 test_loss: 0.10761 \n",
      "[129/300] train_loss: 0.08379 valid_loss: 0.09666 test_loss: 0.10538 \n",
      "[130/300] train_loss: 0.08179 valid_loss: 0.09498 test_loss: 0.10543 \n",
      "[131/300] train_loss: 0.08152 valid_loss: 0.09279 test_loss: 0.10445 \n",
      "Validation loss decreased (0.094172 --> 0.092790).  Saving model ...\n",
      "[132/300] train_loss: 0.08269 valid_loss: 0.09398 test_loss: 0.10551 \n",
      "[133/300] train_loss: 0.08235 valid_loss: 0.10120 test_loss: 0.10660 \n",
      "[134/300] train_loss: 0.08274 valid_loss: 0.10102 test_loss: 0.10572 \n",
      "[135/300] train_loss: 0.08135 valid_loss: 0.09678 test_loss: 0.10381 \n",
      "[136/300] train_loss: 0.08036 valid_loss: 0.09670 test_loss: 0.10415 \n",
      "[137/300] train_loss: 0.07974 valid_loss: 0.09778 test_loss: 0.10517 \n",
      "[138/300] train_loss: 0.08079 valid_loss: 0.09387 test_loss: 0.10572 \n",
      "[139/300] train_loss: 0.08233 valid_loss: 0.09980 test_loss: 0.10557 \n",
      "[140/300] train_loss: 0.08295 valid_loss: 0.09848 test_loss: 0.10544 \n",
      "[141/300] train_loss: 0.08240 valid_loss: 0.10075 test_loss: 0.10474 \n",
      "[142/300] train_loss: 0.08063 valid_loss: 0.09507 test_loss: 0.10432 \n",
      "[143/300] train_loss: 0.08053 valid_loss: 0.10181 test_loss: 0.10447 \n",
      "[144/300] train_loss: 0.08353 valid_loss: 0.09770 test_loss: 0.10471 \n",
      "[145/300] train_loss: 0.08186 valid_loss: 0.10635 test_loss: 0.10513 \n",
      "[146/300] train_loss: 0.08159 valid_loss: 0.10055 test_loss: 0.10390 \n",
      "[147/300] train_loss: 0.08034 valid_loss: 0.09493 test_loss: 0.10381 \n",
      "[148/300] train_loss: 0.08013 valid_loss: 0.09297 test_loss: 0.10318 \n",
      "[149/300] train_loss: 0.08258 valid_loss: 0.10174 test_loss: 0.10503 \n",
      "[150/300] train_loss: 0.07949 valid_loss: 0.09547 test_loss: 0.10380 \n",
      "[151/300] train_loss: 0.08126 valid_loss: 0.09621 test_loss: 0.10403 \n",
      "[152/300] train_loss: 0.08228 valid_loss: 0.10083 test_loss: 0.10288 \n",
      "[153/300] train_loss: 0.07867 valid_loss: 0.09926 test_loss: 0.10493 \n",
      "[154/300] train_loss: 0.08163 valid_loss: 0.09127 test_loss: 0.10272 \n",
      "Validation loss decreased (0.092790 --> 0.091266).  Saving model ...\n",
      "[155/300] train_loss: 0.07850 valid_loss: 0.09730 test_loss: 0.10353 \n",
      "[156/300] train_loss: 0.08065 valid_loss: 0.09369 test_loss: 0.10300 \n",
      "[157/300] train_loss: 0.08053 valid_loss: 0.09663 test_loss: 0.10253 \n",
      "[158/300] train_loss: 0.07658 valid_loss: 0.10525 test_loss: 0.10396 \n",
      "[159/300] train_loss: 0.08052 valid_loss: 0.09165 test_loss: 0.10181 \n",
      "[160/300] train_loss: 0.07900 valid_loss: 0.09342 test_loss: 0.10221 \n",
      "[161/300] train_loss: 0.07660 valid_loss: 0.09764 test_loss: 0.10317 \n",
      "[162/300] train_loss: 0.07930 valid_loss: 0.09161 test_loss: 0.10088 \n",
      "[163/300] train_loss: 0.07687 valid_loss: 0.08918 test_loss: 0.10285 \n",
      "Validation loss decreased (0.091266 --> 0.089177).  Saving model ...\n",
      "[164/300] train_loss: 0.07581 valid_loss: 0.09232 test_loss: 0.10343 \n",
      "[165/300] train_loss: 0.07711 valid_loss: 0.08988 test_loss: 0.10228 \n",
      "[166/300] train_loss: 0.07784 valid_loss: 0.09002 test_loss: 0.10130 \n",
      "[167/300] train_loss: 0.07509 valid_loss: 0.09108 test_loss: 0.10179 \n",
      "[168/300] train_loss: 0.07753 valid_loss: 0.08967 test_loss: 0.10204 \n",
      "[169/300] train_loss: 0.08052 valid_loss: 0.08974 test_loss: 0.10167 \n",
      "[170/300] train_loss: 0.07983 valid_loss: 0.09054 test_loss: 0.10210 \n",
      "[171/300] train_loss: 0.07695 valid_loss: 0.09091 test_loss: 0.10137 \n",
      "[172/300] train_loss: 0.07904 valid_loss: 0.09237 test_loss: 0.10060 \n",
      "[173/300] train_loss: 0.07571 valid_loss: 0.09147 test_loss: 0.10139 \n",
      "[174/300] train_loss: 0.07725 valid_loss: 0.09427 test_loss: 0.10187 \n",
      "[175/300] train_loss: 0.07593 valid_loss: 0.09044 test_loss: 0.10136 \n",
      "[176/300] train_loss: 0.07682 valid_loss: 0.08959 test_loss: 0.10136 \n",
      "[177/300] train_loss: 0.07825 valid_loss: 0.09002 test_loss: 0.10041 \n",
      "[178/300] train_loss: 0.07701 valid_loss: 0.08713 test_loss: 0.09912 \n",
      "Validation loss decreased (0.089177 --> 0.087128).  Saving model ...\n",
      "[179/300] train_loss: 0.07677 valid_loss: 0.08806 test_loss: 0.10063 \n",
      "[180/300] train_loss: 0.07506 valid_loss: 0.08812 test_loss: 0.10012 \n",
      "[181/300] train_loss: 0.07673 valid_loss: 0.09053 test_loss: 0.10078 \n",
      "[182/300] train_loss: 0.07476 valid_loss: 0.08904 test_loss: 0.10018 \n",
      "[183/300] train_loss: 0.07734 valid_loss: 0.09098 test_loss: 0.10023 \n",
      "[184/300] train_loss: 0.07303 valid_loss: 0.08778 test_loss: 0.09956 \n",
      "[185/300] train_loss: 0.07595 valid_loss: 0.09459 test_loss: 0.09951 \n",
      "[186/300] train_loss: 0.07666 valid_loss: 0.08954 test_loss: 0.09969 \n",
      "[187/300] train_loss: 0.07683 valid_loss: 0.08986 test_loss: 0.10086 \n",
      "[188/300] train_loss: 0.07439 valid_loss: 0.08811 test_loss: 0.09977 \n",
      "[189/300] train_loss: 0.07341 valid_loss: 0.09384 test_loss: 0.10076 \n",
      "[190/300] train_loss: 0.07408 valid_loss: 0.09219 test_loss: 0.10050 \n",
      "[191/300] train_loss: 0.07529 valid_loss: 0.09046 test_loss: 0.10062 \n",
      "[192/300] train_loss: 0.07651 valid_loss: 0.08861 test_loss: 0.09944 \n",
      "[193/300] train_loss: 0.07255 valid_loss: 0.09141 test_loss: 0.10017 \n",
      "[194/300] train_loss: 0.07343 valid_loss: 0.08664 test_loss: 0.09910 \n",
      "Validation loss decreased (0.087128 --> 0.086636).  Saving model ...\n",
      "[195/300] train_loss: 0.07532 valid_loss: 0.09108 test_loss: 0.09895 \n",
      "[196/300] train_loss: 0.07465 valid_loss: 0.08736 test_loss: 0.09979 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197/300] train_loss: 0.07505 valid_loss: 0.08643 test_loss: 0.09901 \n",
      "Validation loss decreased (0.086636 --> 0.086432).  Saving model ...\n",
      "[198/300] train_loss: 0.07494 valid_loss: 0.08871 test_loss: 0.09866 \n",
      "[199/300] train_loss: 0.07634 valid_loss: 0.08806 test_loss: 0.09841 \n",
      "[200/300] train_loss: 0.07394 valid_loss: 0.08820 test_loss: 0.09928 \n",
      "[201/300] train_loss: 0.07493 valid_loss: 0.08735 test_loss: 0.09940 \n",
      "[202/300] train_loss: 0.07416 valid_loss: 0.08849 test_loss: 0.09916 \n",
      "[203/300] train_loss: 0.07380 valid_loss: 0.08524 test_loss: 0.09768 \n",
      "Validation loss decreased (0.086432 --> 0.085244).  Saving model ...\n",
      "[204/300] train_loss: 0.07413 valid_loss: 0.08568 test_loss: 0.09853 \n",
      "[205/300] train_loss: 0.07340 valid_loss: 0.08731 test_loss: 0.09958 \n",
      "[206/300] train_loss: 0.07535 valid_loss: 0.08722 test_loss: 0.09864 \n",
      "[207/300] train_loss: 0.07311 valid_loss: 0.08553 test_loss: 0.09715 \n",
      "[208/300] train_loss: 0.07473 valid_loss: 0.08738 test_loss: 0.09832 \n",
      "[209/300] train_loss: 0.07332 valid_loss: 0.08789 test_loss: 0.09844 \n",
      "[210/300] train_loss: 0.07423 valid_loss: 0.08837 test_loss: 0.09951 \n",
      "[211/300] train_loss: 0.07285 valid_loss: 0.10985 test_loss: 0.09840 \n",
      "[212/300] train_loss: 0.07466 valid_loss: 0.08737 test_loss: 0.09788 \n",
      "[213/300] train_loss: 0.07361 valid_loss: 0.08654 test_loss: 0.09879 \n",
      "[214/300] train_loss: 0.07348 valid_loss: 0.08772 test_loss: 0.09705 \n",
      "[215/300] train_loss: 0.07606 valid_loss: 0.08941 test_loss: 0.09787 \n",
      "[216/300] train_loss: 0.07364 valid_loss: 0.08782 test_loss: 0.09746 \n",
      "[217/300] train_loss: 0.07418 valid_loss: 0.08996 test_loss: 0.09693 \n",
      "[218/300] train_loss: 0.07273 valid_loss: 0.09126 test_loss: 0.09632 \n",
      "[219/300] train_loss: 0.07295 valid_loss: 0.09067 test_loss: 0.09659 \n",
      "[220/300] train_loss: 0.07325 valid_loss: 0.08953 test_loss: 0.09692 \n",
      "[221/300] train_loss: 0.07328 valid_loss: 0.08774 test_loss: 0.09603 \n",
      "[222/300] train_loss: 0.07244 valid_loss: 0.08569 test_loss: 0.09639 \n",
      "[223/300] train_loss: 0.07292 valid_loss: 0.08699 test_loss: 0.09578 \n",
      "[224/300] train_loss: 0.07205 valid_loss: 0.09333 test_loss: 0.09744 \n",
      "[225/300] train_loss: 0.07365 valid_loss: 0.08759 test_loss: 0.09634 \n",
      "[226/300] train_loss: 0.07334 valid_loss: 0.08603 test_loss: 0.09766 \n",
      "[227/300] train_loss: 0.07208 valid_loss: 0.09120 test_loss: 0.09734 \n",
      "[228/300] train_loss: 0.07139 valid_loss: 0.09097 test_loss: 0.09692 \n",
      "[229/300] train_loss: 0.07305 valid_loss: 0.08680 test_loss: 0.09679 \n",
      "[230/300] train_loss: 0.07252 valid_loss: 0.09363 test_loss: 0.09675 \n",
      "[231/300] train_loss: 0.07069 valid_loss: 0.08929 test_loss: 0.09770 \n",
      "[232/300] train_loss: 0.06976 valid_loss: 0.09716 test_loss: 0.09722 \n",
      "[233/300] train_loss: 0.07201 valid_loss: 0.09169 test_loss: 0.09585 \n",
      "[234/300] train_loss: 0.07198 valid_loss: 0.09538 test_loss: 0.09632 \n",
      "[235/300] train_loss: 0.07085 valid_loss: 0.08697 test_loss: 0.09520 \n",
      "[236/300] train_loss: 0.07402 valid_loss: 0.09514 test_loss: 0.09630 \n",
      "[237/300] train_loss: 0.07029 valid_loss: 0.09716 test_loss: 0.09578 \n",
      "[238/300] train_loss: 0.07197 valid_loss: 0.09402 test_loss: 0.09568 \n",
      "[239/300] train_loss: 0.06954 valid_loss: 0.09206 test_loss: 0.09674 \n",
      "[240/300] train_loss: 0.07057 valid_loss: 0.09289 test_loss: 0.09574 \n",
      "[241/300] train_loss: 0.06986 valid_loss: 0.09295 test_loss: 0.09637 \n",
      "[242/300] train_loss: 0.06997 valid_loss: 0.09667 test_loss: 0.09683 \n",
      "[243/300] train_loss: 0.07004 valid_loss: 0.09690 test_loss: 0.09602 \n",
      "[244/300] train_loss: 0.06955 valid_loss: 0.08641 test_loss: 0.09579 \n",
      "[245/300] train_loss: 0.07107 valid_loss: 0.08803 test_loss: 0.09639 \n",
      "[246/300] train_loss: 0.07362 valid_loss: 0.08990 test_loss: 0.09534 \n",
      "[247/300] train_loss: 0.07376 valid_loss: 0.08669 test_loss: 0.09499 \n",
      "[248/300] train_loss: 0.06973 valid_loss: 0.09254 test_loss: 0.09582 \n",
      "[249/300] train_loss: 0.06861 valid_loss: 0.10182 test_loss: 0.09429 \n",
      "[250/300] train_loss: 0.07233 valid_loss: 0.08743 test_loss: 0.09634 \n",
      "[251/300] train_loss: 0.07115 valid_loss: 0.08922 test_loss: 0.09562 \n",
      "[252/300] train_loss: 0.07190 valid_loss: 0.08457 test_loss: 0.09547 \n",
      "Validation loss decreased (0.085244 --> 0.084573).  Saving model ...\n",
      "[253/300] train_loss: 0.07017 valid_loss: 0.08466 test_loss: 0.09482 \n",
      "[254/300] train_loss: 0.07035 valid_loss: 0.08527 test_loss: 0.09436 \n",
      "[255/300] train_loss: 0.07134 valid_loss: 0.08689 test_loss: 0.09489 \n",
      "[256/300] train_loss: 0.07018 valid_loss: 0.08533 test_loss: 0.09563 \n",
      "[257/300] train_loss: 0.07103 valid_loss: 0.08358 test_loss: 0.09566 \n",
      "Validation loss decreased (0.084573 --> 0.083581).  Saving model ...\n",
      "[258/300] train_loss: 0.07090 valid_loss: 0.08722 test_loss: 0.09596 \n",
      "[259/300] train_loss: 0.06892 valid_loss: 0.08405 test_loss: 0.09516 \n",
      "[260/300] train_loss: 0.06985 valid_loss: 0.08577 test_loss: 0.09454 \n",
      "[261/300] train_loss: 0.07222 valid_loss: 0.08867 test_loss: 0.09441 \n",
      "[262/300] train_loss: 0.06948 valid_loss: 0.08456 test_loss: 0.09637 \n",
      "[263/300] train_loss: 0.06898 valid_loss: 0.08337 test_loss: 0.09359 \n",
      "Validation loss decreased (0.083581 --> 0.083371).  Saving model ...\n",
      "[264/300] train_loss: 0.06921 valid_loss: 0.08400 test_loss: 0.09420 \n",
      "[265/300] train_loss: 0.06841 valid_loss: 0.09320 test_loss: 0.09476 \n",
      "[266/300] train_loss: 0.06686 valid_loss: 0.08784 test_loss: 0.09392 \n",
      "[267/300] train_loss: 0.07043 valid_loss: 0.08307 test_loss: 0.09395 \n",
      "Validation loss decreased (0.083371 --> 0.083072).  Saving model ...\n",
      "[268/300] train_loss: 0.07215 valid_loss: 0.08390 test_loss: 0.09369 \n",
      "[269/300] train_loss: 0.06783 valid_loss: 0.08509 test_loss: 0.09310 \n",
      "[270/300] train_loss: 0.06917 valid_loss: 0.08512 test_loss: 0.09378 \n",
      "[271/300] train_loss: 0.06903 valid_loss: 0.08574 test_loss: 0.09340 \n",
      "[272/300] train_loss: 0.06943 valid_loss: 0.08452 test_loss: 0.09317 \n",
      "[273/300] train_loss: 0.07017 valid_loss: 0.08429 test_loss: 0.09429 \n",
      "[274/300] train_loss: 0.06887 valid_loss: 0.08299 test_loss: 0.09388 \n",
      "Validation loss decreased (0.083072 --> 0.082990).  Saving model ...\n",
      "[275/300] train_loss: 0.06782 valid_loss: 0.08138 test_loss: 0.09299 \n",
      "Validation loss decreased (0.082990 --> 0.081376).  Saving model ...\n",
      "[276/300] train_loss: 0.06734 valid_loss: 0.08512 test_loss: 0.09587 \n",
      "[277/300] train_loss: 0.06730 valid_loss: 0.08481 test_loss: 0.09446 \n",
      "[278/300] train_loss: 0.06755 valid_loss: 0.08399 test_loss: 0.09549 \n",
      "[279/300] train_loss: 0.06780 valid_loss: 0.08211 test_loss: 0.09335 \n",
      "[280/300] train_loss: 0.06662 valid_loss: 0.08207 test_loss: 0.09193 \n",
      "[281/300] train_loss: 0.07068 valid_loss: 0.08249 test_loss: 0.09170 \n",
      "[282/300] train_loss: 0.06948 valid_loss: 0.08225 test_loss: 0.09232 \n",
      "[283/300] train_loss: 0.06724 valid_loss: 0.08189 test_loss: 0.09307 \n",
      "[284/300] train_loss: 0.06921 valid_loss: 0.08232 test_loss: 0.09293 \n",
      "[285/300] train_loss: 0.06979 valid_loss: 0.08334 test_loss: 0.09381 \n",
      "[286/300] train_loss: 0.07091 valid_loss: 0.08228 test_loss: 0.09243 \n",
      "[287/300] train_loss: 0.06807 valid_loss: 0.08342 test_loss: 0.09283 \n",
      "[288/300] train_loss: 0.06739 valid_loss: 0.08169 test_loss: 0.09176 \n",
      "[289/300] train_loss: 0.06670 valid_loss: 0.08321 test_loss: 0.09331 \n",
      "[290/300] train_loss: 0.06681 valid_loss: 0.08279 test_loss: 0.09276 \n",
      "[291/300] train_loss: 0.06885 valid_loss: 0.08488 test_loss: 0.09347 \n",
      "[292/300] train_loss: 0.06731 valid_loss: 0.08127 test_loss: 0.09240 \n",
      "Validation loss decreased (0.081376 --> 0.081268).  Saving model ...\n",
      "[293/300] train_loss: 0.06615 valid_loss: 0.08318 test_loss: 0.09328 \n",
      "[294/300] train_loss: 0.06600 valid_loss: 0.08344 test_loss: 0.09227 \n",
      "[295/300] train_loss: 0.06750 valid_loss: 0.08140 test_loss: 0.09258 \n",
      "[296/300] train_loss: 0.06420 valid_loss: 0.08252 test_loss: 0.09356 \n",
      "[297/300] train_loss: 0.06822 valid_loss: 0.08295 test_loss: 0.09294 \n",
      "[298/300] train_loss: 0.06794 valid_loss: 0.08249 test_loss: 0.09220 \n",
      "[299/300] train_loss: 0.06607 valid_loss: 0.08009 test_loss: 0.09163 \n",
      "Validation loss decreased (0.081268 --> 0.080087).  Saving model ...\n",
      "[300/300] train_loss: 0.06680 valid_loss: 0.08295 test_loss: 0.09287 \n",
      "TRAINING MODEL 8\n",
      "[  1/300] train_loss: 0.59944 valid_loss: 0.52764 test_loss: 0.52162 \n",
      "Validation loss decreased (inf --> 0.527636).  Saving model ...\n",
      "[  2/300] train_loss: 0.43791 valid_loss: 0.39481 test_loss: 0.39327 \n",
      "Validation loss decreased (0.527636 --> 0.394811).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3/300] train_loss: 0.34496 valid_loss: 0.34029 test_loss: 0.34093 \n",
      "Validation loss decreased (0.394811 --> 0.340286).  Saving model ...\n",
      "[  4/300] train_loss: 0.29611 valid_loss: 0.29544 test_loss: 0.30213 \n",
      "Validation loss decreased (0.340286 --> 0.295439).  Saving model ...\n",
      "[  5/300] train_loss: 0.25782 valid_loss: 0.26224 test_loss: 0.27233 \n",
      "Validation loss decreased (0.295439 --> 0.262240).  Saving model ...\n",
      "[  6/300] train_loss: 0.23070 valid_loss: 0.23654 test_loss: 0.24997 \n",
      "Validation loss decreased (0.262240 --> 0.236541).  Saving model ...\n",
      "[  7/300] train_loss: 0.21229 valid_loss: 0.21437 test_loss: 0.22889 \n",
      "Validation loss decreased (0.236541 --> 0.214370).  Saving model ...\n",
      "[  8/300] train_loss: 0.19363 valid_loss: 0.20020 test_loss: 0.21563 \n",
      "Validation loss decreased (0.214370 --> 0.200196).  Saving model ...\n",
      "[  9/300] train_loss: 0.18298 valid_loss: 0.18697 test_loss: 0.20226 \n",
      "Validation loss decreased (0.200196 --> 0.186973).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17149 valid_loss: 0.18020 test_loss: 0.19482 \n",
      "Validation loss decreased (0.186973 --> 0.180198).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16348 valid_loss: 0.17166 test_loss: 0.18554 \n",
      "Validation loss decreased (0.180198 --> 0.171659).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15764 valid_loss: 0.16663 test_loss: 0.17961 \n",
      "Validation loss decreased (0.171659 --> 0.166627).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15339 valid_loss: 0.16122 test_loss: 0.17537 \n",
      "Validation loss decreased (0.166627 --> 0.161215).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15123 valid_loss: 0.15744 test_loss: 0.17166 \n",
      "Validation loss decreased (0.161215 --> 0.157438).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14372 valid_loss: 0.15313 test_loss: 0.16708 \n",
      "Validation loss decreased (0.157438 --> 0.153134).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14044 valid_loss: 0.15264 test_loss: 0.16693 \n",
      "Validation loss decreased (0.153134 --> 0.152639).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13902 valid_loss: 0.14684 test_loss: 0.16280 \n",
      "Validation loss decreased (0.152639 --> 0.146843).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13827 valid_loss: 0.14449 test_loss: 0.15905 \n",
      "Validation loss decreased (0.146843 --> 0.144491).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13221 valid_loss: 0.14093 test_loss: 0.15848 \n",
      "Validation loss decreased (0.144491 --> 0.140927).  Saving model ...\n",
      "[ 20/300] train_loss: 0.13434 valid_loss: 0.14167 test_loss: 0.15643 \n",
      "[ 21/300] train_loss: 0.13090 valid_loss: 0.14078 test_loss: 0.15333 \n",
      "Validation loss decreased (0.140927 --> 0.140779).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13092 valid_loss: 0.14206 test_loss: 0.15252 \n",
      "[ 23/300] train_loss: 0.13010 valid_loss: 0.13595 test_loss: 0.15108 \n",
      "Validation loss decreased (0.140779 --> 0.135946).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12644 valid_loss: 0.13609 test_loss: 0.14801 \n",
      "[ 25/300] train_loss: 0.12358 valid_loss: 0.13590 test_loss: 0.14914 \n",
      "Validation loss decreased (0.135946 --> 0.135896).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12352 valid_loss: 0.13273 test_loss: 0.14735 \n",
      "Validation loss decreased (0.135896 --> 0.132729).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12306 valid_loss: 0.13125 test_loss: 0.14469 \n",
      "Validation loss decreased (0.132729 --> 0.131252).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12138 valid_loss: 0.12921 test_loss: 0.14472 \n",
      "Validation loss decreased (0.131252 --> 0.129207).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12494 valid_loss: 0.12789 test_loss: 0.14241 \n",
      "Validation loss decreased (0.129207 --> 0.127888).  Saving model ...\n",
      "[ 30/300] train_loss: 0.12025 valid_loss: 0.13228 test_loss: 0.14309 \n",
      "[ 31/300] train_loss: 0.11728 valid_loss: 0.12551 test_loss: 0.13972 \n",
      "Validation loss decreased (0.127888 --> 0.125508).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11713 valid_loss: 0.12342 test_loss: 0.13969 \n",
      "Validation loss decreased (0.125508 --> 0.123425).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11358 valid_loss: 0.12359 test_loss: 0.13871 \n",
      "[ 34/300] train_loss: 0.11583 valid_loss: 0.12303 test_loss: 0.13915 \n",
      "Validation loss decreased (0.123425 --> 0.123028).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11327 valid_loss: 0.12196 test_loss: 0.13599 \n",
      "Validation loss decreased (0.123028 --> 0.121962).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11361 valid_loss: 0.12142 test_loss: 0.13532 \n",
      "Validation loss decreased (0.121962 --> 0.121422).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11481 valid_loss: 0.12165 test_loss: 0.13573 \n",
      "[ 38/300] train_loss: 0.11272 valid_loss: 0.12106 test_loss: 0.13750 \n",
      "Validation loss decreased (0.121422 --> 0.121062).  Saving model ...\n",
      "[ 39/300] train_loss: 0.10987 valid_loss: 0.12109 test_loss: 0.13433 \n",
      "[ 40/300] train_loss: 0.11126 valid_loss: 0.11994 test_loss: 0.13341 \n",
      "Validation loss decreased (0.121062 --> 0.119939).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10888 valid_loss: 0.11712 test_loss: 0.13261 \n",
      "Validation loss decreased (0.119939 --> 0.117121).  Saving model ...\n",
      "[ 42/300] train_loss: 0.10811 valid_loss: 0.11825 test_loss: 0.13156 \n",
      "[ 43/300] train_loss: 0.10751 valid_loss: 0.11723 test_loss: 0.13172 \n",
      "[ 44/300] train_loss: 0.11188 valid_loss: 0.11585 test_loss: 0.13254 \n",
      "Validation loss decreased (0.117121 --> 0.115853).  Saving model ...\n",
      "[ 45/300] train_loss: 0.10745 valid_loss: 0.11438 test_loss: 0.13097 \n",
      "Validation loss decreased (0.115853 --> 0.114384).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10627 valid_loss: 0.11178 test_loss: 0.12758 \n",
      "Validation loss decreased (0.114384 --> 0.111776).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10838 valid_loss: 0.11321 test_loss: 0.12893 \n",
      "[ 48/300] train_loss: 0.10702 valid_loss: 0.11582 test_loss: 0.12967 \n",
      "[ 49/300] train_loss: 0.10273 valid_loss: 0.11399 test_loss: 0.12757 \n",
      "[ 50/300] train_loss: 0.10382 valid_loss: 0.11468 test_loss: 0.12895 \n",
      "[ 51/300] train_loss: 0.10510 valid_loss: 0.10952 test_loss: 0.12529 \n",
      "Validation loss decreased (0.111776 --> 0.109522).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10250 valid_loss: 0.11156 test_loss: 0.12504 \n",
      "[ 53/300] train_loss: 0.10234 valid_loss: 0.11030 test_loss: 0.12517 \n",
      "[ 54/300] train_loss: 0.10388 valid_loss: 0.11398 test_loss: 0.12831 \n",
      "[ 55/300] train_loss: 0.10382 valid_loss: 0.11340 test_loss: 0.12707 \n",
      "[ 56/300] train_loss: 0.10064 valid_loss: 0.11228 test_loss: 0.12332 \n",
      "[ 57/300] train_loss: 0.10121 valid_loss: 0.11001 test_loss: 0.12334 \n",
      "[ 58/300] train_loss: 0.09991 valid_loss: 0.11115 test_loss: 0.12281 \n",
      "[ 59/300] train_loss: 0.10350 valid_loss: 0.11262 test_loss: 0.12761 \n",
      "[ 60/300] train_loss: 0.10107 valid_loss: 0.10768 test_loss: 0.12282 \n",
      "Validation loss decreased (0.109522 --> 0.107677).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09908 valid_loss: 0.10634 test_loss: 0.12054 \n",
      "Validation loss decreased (0.107677 --> 0.106336).  Saving model ...\n",
      "[ 62/300] train_loss: 0.10171 valid_loss: 0.11107 test_loss: 0.12066 \n",
      "[ 63/300] train_loss: 0.09988 valid_loss: 0.10555 test_loss: 0.12061 \n",
      "Validation loss decreased (0.106336 --> 0.105554).  Saving model ...\n",
      "[ 64/300] train_loss: 0.10114 valid_loss: 0.10436 test_loss: 0.11840 \n",
      "Validation loss decreased (0.105554 --> 0.104362).  Saving model ...\n",
      "[ 65/300] train_loss: 0.10151 valid_loss: 0.10520 test_loss: 0.12068 \n",
      "[ 66/300] train_loss: 0.09701 valid_loss: 0.10845 test_loss: 0.11977 \n",
      "[ 67/300] train_loss: 0.09672 valid_loss: 0.10556 test_loss: 0.11829 \n",
      "[ 68/300] train_loss: 0.10007 valid_loss: 0.10570 test_loss: 0.11700 \n",
      "[ 69/300] train_loss: 0.09739 valid_loss: 0.10560 test_loss: 0.11899 \n",
      "[ 70/300] train_loss: 0.09765 valid_loss: 0.10351 test_loss: 0.11767 \n",
      "Validation loss decreased (0.104362 --> 0.103505).  Saving model ...\n",
      "[ 71/300] train_loss: 0.09595 valid_loss: 0.10300 test_loss: 0.11716 \n",
      "Validation loss decreased (0.103505 --> 0.102996).  Saving model ...\n",
      "[ 72/300] train_loss: 0.09603 valid_loss: 0.10228 test_loss: 0.11717 \n",
      "Validation loss decreased (0.102996 --> 0.102278).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09403 valid_loss: 0.10196 test_loss: 0.11544 \n",
      "Validation loss decreased (0.102278 --> 0.101956).  Saving model ...\n",
      "[ 74/300] train_loss: 0.09379 valid_loss: 0.10260 test_loss: 0.11484 \n",
      "[ 75/300] train_loss: 0.09522 valid_loss: 0.10332 test_loss: 0.11487 \n",
      "[ 76/300] train_loss: 0.09640 valid_loss: 0.10468 test_loss: 0.11723 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 77/300] train_loss: 0.09700 valid_loss: 0.09972 test_loss: 0.11409 \n",
      "Validation loss decreased (0.101956 --> 0.099722).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09299 valid_loss: 0.10122 test_loss: 0.11433 \n",
      "[ 79/300] train_loss: 0.09506 valid_loss: 0.10127 test_loss: 0.11467 \n",
      "[ 80/300] train_loss: 0.09682 valid_loss: 0.10101 test_loss: 0.11360 \n",
      "[ 81/300] train_loss: 0.09040 valid_loss: 0.10291 test_loss: 0.11401 \n",
      "[ 82/300] train_loss: 0.09345 valid_loss: 0.10364 test_loss: 0.11434 \n",
      "[ 83/300] train_loss: 0.09153 valid_loss: 0.10115 test_loss: 0.11368 \n",
      "[ 84/300] train_loss: 0.09392 valid_loss: 0.10415 test_loss: 0.11438 \n",
      "[ 85/300] train_loss: 0.09413 valid_loss: 0.10236 test_loss: 0.11547 \n",
      "[ 86/300] train_loss: 0.09265 valid_loss: 0.10005 test_loss: 0.11236 \n",
      "[ 87/300] train_loss: 0.09021 valid_loss: 0.09895 test_loss: 0.11237 \n",
      "Validation loss decreased (0.099722 --> 0.098951).  Saving model ...\n",
      "[ 88/300] train_loss: 0.09056 valid_loss: 0.09776 test_loss: 0.11219 \n",
      "Validation loss decreased (0.098951 --> 0.097758).  Saving model ...\n",
      "[ 89/300] train_loss: 0.09252 valid_loss: 0.10506 test_loss: 0.11300 \n",
      "[ 90/300] train_loss: 0.08947 valid_loss: 0.09708 test_loss: 0.11126 \n",
      "Validation loss decreased (0.097758 --> 0.097079).  Saving model ...\n",
      "[ 91/300] train_loss: 0.08724 valid_loss: 0.09988 test_loss: 0.11184 \n",
      "[ 92/300] train_loss: 0.09283 valid_loss: 0.09651 test_loss: 0.11080 \n",
      "Validation loss decreased (0.097079 --> 0.096508).  Saving model ...\n",
      "[ 93/300] train_loss: 0.08970 valid_loss: 0.09892 test_loss: 0.11093 \n",
      "[ 94/300] train_loss: 0.09050 valid_loss: 0.09716 test_loss: 0.11034 \n",
      "[ 95/300] train_loss: 0.09004 valid_loss: 0.10027 test_loss: 0.11034 \n",
      "[ 96/300] train_loss: 0.08905 valid_loss: 0.09748 test_loss: 0.10980 \n",
      "[ 97/300] train_loss: 0.08657 valid_loss: 0.09881 test_loss: 0.11250 \n",
      "[ 98/300] train_loss: 0.08864 valid_loss: 0.09799 test_loss: 0.11095 \n",
      "[ 99/300] train_loss: 0.08691 valid_loss: 0.09788 test_loss: 0.10871 \n",
      "[100/300] train_loss: 0.08788 valid_loss: 0.09608 test_loss: 0.10945 \n",
      "Validation loss decreased (0.096508 --> 0.096077).  Saving model ...\n",
      "[101/300] train_loss: 0.08736 valid_loss: 0.09652 test_loss: 0.11144 \n",
      "[102/300] train_loss: 0.08699 valid_loss: 0.09702 test_loss: 0.11031 \n",
      "[103/300] train_loss: 0.08711 valid_loss: 0.09683 test_loss: 0.11035 \n",
      "[104/300] train_loss: 0.08930 valid_loss: 0.09652 test_loss: 0.10938 \n",
      "[105/300] train_loss: 0.08782 valid_loss: 0.09477 test_loss: 0.10968 \n",
      "Validation loss decreased (0.096077 --> 0.094773).  Saving model ...\n",
      "[106/300] train_loss: 0.09014 valid_loss: 0.09373 test_loss: 0.10812 \n",
      "Validation loss decreased (0.094773 --> 0.093727).  Saving model ...\n",
      "[107/300] train_loss: 0.08670 valid_loss: 0.09487 test_loss: 0.10946 \n",
      "[108/300] train_loss: 0.08679 valid_loss: 0.09503 test_loss: 0.10898 \n",
      "[109/300] train_loss: 0.08579 valid_loss: 0.10067 test_loss: 0.10933 \n",
      "[110/300] train_loss: 0.08923 valid_loss: 0.09641 test_loss: 0.10864 \n",
      "[111/300] train_loss: 0.08772 valid_loss: 0.09524 test_loss: 0.10899 \n",
      "[112/300] train_loss: 0.08652 valid_loss: 0.09516 test_loss: 0.10699 \n",
      "[113/300] train_loss: 0.08616 valid_loss: 0.09677 test_loss: 0.10804 \n",
      "[114/300] train_loss: 0.08661 valid_loss: 0.09815 test_loss: 0.10700 \n",
      "[115/300] train_loss: 0.08591 valid_loss: 0.09877 test_loss: 0.10712 \n",
      "[116/300] train_loss: 0.08712 valid_loss: 0.09673 test_loss: 0.10818 \n",
      "[117/300] train_loss: 0.08557 valid_loss: 0.09402 test_loss: 0.10561 \n",
      "[118/300] train_loss: 0.08609 valid_loss: 0.09493 test_loss: 0.10683 \n",
      "[119/300] train_loss: 0.08476 valid_loss: 0.09285 test_loss: 0.10553 \n",
      "Validation loss decreased (0.093727 --> 0.092851).  Saving model ...\n",
      "[120/300] train_loss: 0.08559 valid_loss: 0.09563 test_loss: 0.10602 \n",
      "[121/300] train_loss: 0.08524 valid_loss: 0.09095 test_loss: 0.10484 \n",
      "Validation loss decreased (0.092851 --> 0.090952).  Saving model ...\n",
      "[122/300] train_loss: 0.08263 valid_loss: 0.09372 test_loss: 0.10697 \n",
      "[123/300] train_loss: 0.08518 valid_loss: 0.09372 test_loss: 0.10556 \n",
      "[124/300] train_loss: 0.08329 valid_loss: 0.09543 test_loss: 0.10570 \n",
      "[125/300] train_loss: 0.08443 valid_loss: 0.09724 test_loss: 0.10851 \n",
      "[126/300] train_loss: 0.08293 valid_loss: 0.09187 test_loss: 0.10551 \n",
      "[127/300] train_loss: 0.08524 valid_loss: 0.09639 test_loss: 0.10413 \n",
      "[128/300] train_loss: 0.08253 valid_loss: 0.09481 test_loss: 0.10622 \n",
      "[129/300] train_loss: 0.08404 valid_loss: 0.09638 test_loss: 0.10423 \n",
      "[130/300] train_loss: 0.08395 valid_loss: 0.09662 test_loss: 0.10395 \n",
      "[131/300] train_loss: 0.08463 valid_loss: 0.09412 test_loss: 0.10334 \n",
      "[132/300] train_loss: 0.08277 valid_loss: 0.09065 test_loss: 0.10331 \n",
      "Validation loss decreased (0.090952 --> 0.090647).  Saving model ...\n",
      "[133/300] train_loss: 0.08220 valid_loss: 0.09166 test_loss: 0.10393 \n",
      "[134/300] train_loss: 0.08470 valid_loss: 0.09326 test_loss: 0.10404 \n",
      "[135/300] train_loss: 0.08661 valid_loss: 0.09164 test_loss: 0.10386 \n",
      "[136/300] train_loss: 0.08440 valid_loss: 0.09420 test_loss: 0.10503 \n",
      "[137/300] train_loss: 0.08336 valid_loss: 0.09217 test_loss: 0.10355 \n",
      "[138/300] train_loss: 0.08243 valid_loss: 0.09039 test_loss: 0.10348 \n",
      "Validation loss decreased (0.090647 --> 0.090388).  Saving model ...\n",
      "[139/300] train_loss: 0.08309 valid_loss: 0.08977 test_loss: 0.10287 \n",
      "Validation loss decreased (0.090388 --> 0.089770).  Saving model ...\n",
      "[140/300] train_loss: 0.08221 valid_loss: 0.08973 test_loss: 0.10337 \n",
      "Validation loss decreased (0.089770 --> 0.089734).  Saving model ...\n",
      "[141/300] train_loss: 0.08320 valid_loss: 0.09452 test_loss: 0.10282 \n",
      "[142/300] train_loss: 0.08132 valid_loss: 0.09181 test_loss: 0.10346 \n",
      "[143/300] train_loss: 0.08245 valid_loss: 0.09157 test_loss: 0.10201 \n",
      "[144/300] train_loss: 0.08163 valid_loss: 0.09240 test_loss: 0.10342 \n",
      "[145/300] train_loss: 0.08219 valid_loss: 0.09099 test_loss: 0.10284 \n",
      "[146/300] train_loss: 0.08173 valid_loss: 0.09151 test_loss: 0.10335 \n",
      "[147/300] train_loss: 0.08262 valid_loss: 0.09168 test_loss: 0.10238 \n",
      "[148/300] train_loss: 0.07892 valid_loss: 0.09390 test_loss: 0.10362 \n",
      "[149/300] train_loss: 0.08026 valid_loss: 0.09149 test_loss: 0.10256 \n",
      "[150/300] train_loss: 0.08216 valid_loss: 0.08822 test_loss: 0.10153 \n",
      "Validation loss decreased (0.089734 --> 0.088215).  Saving model ...\n",
      "[151/300] train_loss: 0.07790 valid_loss: 0.08940 test_loss: 0.10125 \n",
      "[152/300] train_loss: 0.07891 valid_loss: 0.09228 test_loss: 0.10199 \n",
      "[153/300] train_loss: 0.07947 valid_loss: 0.08958 test_loss: 0.10248 \n",
      "[154/300] train_loss: 0.07845 valid_loss: 0.09215 test_loss: 0.10332 \n",
      "[155/300] train_loss: 0.08190 valid_loss: 0.09154 test_loss: 0.10288 \n",
      "[156/300] train_loss: 0.08069 valid_loss: 0.08939 test_loss: 0.10217 \n",
      "[157/300] train_loss: 0.07599 valid_loss: 0.08993 test_loss: 0.10271 \n",
      "[158/300] train_loss: 0.08217 valid_loss: 0.08895 test_loss: 0.10072 \n",
      "[159/300] train_loss: 0.07932 valid_loss: 0.09036 test_loss: 0.10289 \n",
      "[160/300] train_loss: 0.07880 valid_loss: 0.08858 test_loss: 0.10104 \n",
      "[161/300] train_loss: 0.08079 valid_loss: 0.08820 test_loss: 0.10078 \n",
      "Validation loss decreased (0.088215 --> 0.088204).  Saving model ...\n",
      "[162/300] train_loss: 0.08003 valid_loss: 0.08769 test_loss: 0.10074 \n",
      "Validation loss decreased (0.088204 --> 0.087692).  Saving model ...\n",
      "[163/300] train_loss: 0.07860 valid_loss: 0.08753 test_loss: 0.10037 \n",
      "Validation loss decreased (0.087692 --> 0.087532).  Saving model ...\n",
      "[164/300] train_loss: 0.08318 valid_loss: 0.08875 test_loss: 0.10083 \n",
      "[165/300] train_loss: 0.07999 valid_loss: 0.09114 test_loss: 0.10212 \n",
      "[166/300] train_loss: 0.07734 valid_loss: 0.08780 test_loss: 0.10119 \n",
      "[167/300] train_loss: 0.07661 valid_loss: 0.08902 test_loss: 0.10025 \n",
      "[168/300] train_loss: 0.07613 valid_loss: 0.09007 test_loss: 0.10161 \n",
      "[169/300] train_loss: 0.08033 valid_loss: 0.08977 test_loss: 0.10039 \n",
      "[170/300] train_loss: 0.07857 valid_loss: 0.08896 test_loss: 0.10106 \n",
      "[171/300] train_loss: 0.07747 valid_loss: 0.08797 test_loss: 0.10069 \n",
      "[172/300] train_loss: 0.07669 valid_loss: 0.08646 test_loss: 0.09960 \n",
      "Validation loss decreased (0.087532 --> 0.086458).  Saving model ...\n",
      "[173/300] train_loss: 0.07704 valid_loss: 0.09118 test_loss: 0.10093 \n",
      "[174/300] train_loss: 0.07745 valid_loss: 0.08683 test_loss: 0.09967 \n",
      "[175/300] train_loss: 0.07890 valid_loss: 0.08835 test_loss: 0.10002 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[176/300] train_loss: 0.07963 valid_loss: 0.08693 test_loss: 0.09998 \n",
      "[177/300] train_loss: 0.07828 valid_loss: 0.08792 test_loss: 0.09995 \n",
      "[178/300] train_loss: 0.07675 valid_loss: 0.08729 test_loss: 0.09979 \n",
      "[179/300] train_loss: 0.08004 valid_loss: 0.08637 test_loss: 0.09843 \n",
      "Validation loss decreased (0.086458 --> 0.086370).  Saving model ...\n",
      "[180/300] train_loss: 0.07784 valid_loss: 0.08856 test_loss: 0.10058 \n",
      "[181/300] train_loss: 0.07895 valid_loss: 0.08700 test_loss: 0.09955 \n",
      "[182/300] train_loss: 0.07700 valid_loss: 0.08843 test_loss: 0.09919 \n",
      "[183/300] train_loss: 0.07797 valid_loss: 0.08680 test_loss: 0.09893 \n",
      "[184/300] train_loss: 0.07486 valid_loss: 0.08687 test_loss: 0.09878 \n",
      "[185/300] train_loss: 0.07595 valid_loss: 0.08747 test_loss: 0.09859 \n",
      "[186/300] train_loss: 0.07726 valid_loss: 0.09596 test_loss: 0.09947 \n",
      "[187/300] train_loss: 0.07756 valid_loss: 0.08696 test_loss: 0.09833 \n",
      "[188/300] train_loss: 0.07694 valid_loss: 0.08602 test_loss: 0.09833 \n",
      "Validation loss decreased (0.086370 --> 0.086024).  Saving model ...\n",
      "[189/300] train_loss: 0.07533 valid_loss: 0.08669 test_loss: 0.09786 \n",
      "[190/300] train_loss: 0.07576 valid_loss: 0.08909 test_loss: 0.09867 \n",
      "[191/300] train_loss: 0.07662 valid_loss: 0.08839 test_loss: 0.09903 \n",
      "[192/300] train_loss: 0.07801 valid_loss: 0.08649 test_loss: 0.09858 \n",
      "[193/300] train_loss: 0.07579 valid_loss: 0.08535 test_loss: 0.09670 \n",
      "Validation loss decreased (0.086024 --> 0.085353).  Saving model ...\n",
      "[194/300] train_loss: 0.07689 valid_loss: 0.08554 test_loss: 0.09682 \n",
      "[195/300] train_loss: 0.07396 valid_loss: 0.09047 test_loss: 0.09702 \n",
      "[196/300] train_loss: 0.07450 valid_loss: 0.08753 test_loss: 0.09771 \n",
      "[197/300] train_loss: 0.07402 valid_loss: 0.08902 test_loss: 0.09789 \n",
      "[198/300] train_loss: 0.07339 valid_loss: 0.08689 test_loss: 0.09939 \n",
      "[199/300] train_loss: 0.07434 valid_loss: 0.08621 test_loss: 0.09844 \n",
      "[200/300] train_loss: 0.07486 valid_loss: 0.08710 test_loss: 0.09712 \n",
      "[201/300] train_loss: 0.07545 valid_loss: 0.09069 test_loss: 0.09801 \n",
      "[202/300] train_loss: 0.07603 valid_loss: 0.08867 test_loss: 0.09887 \n",
      "[203/300] train_loss: 0.07496 valid_loss: 0.08690 test_loss: 0.09738 \n",
      "[204/300] train_loss: 0.07449 valid_loss: 0.08496 test_loss: 0.09825 \n",
      "Validation loss decreased (0.085353 --> 0.084955).  Saving model ...\n",
      "[205/300] train_loss: 0.07368 valid_loss: 0.08592 test_loss: 0.09819 \n",
      "[206/300] train_loss: 0.07370 valid_loss: 0.08554 test_loss: 0.09746 \n",
      "[207/300] train_loss: 0.07199 valid_loss: 0.08562 test_loss: 0.09687 \n",
      "[208/300] train_loss: 0.07340 valid_loss: 0.08724 test_loss: 0.09690 \n",
      "[209/300] train_loss: 0.07660 valid_loss: 0.09896 test_loss: 0.09827 \n",
      "[210/300] train_loss: 0.07386 valid_loss: 0.08516 test_loss: 0.09752 \n",
      "[211/300] train_loss: 0.07529 valid_loss: 0.08478 test_loss: 0.09752 \n",
      "Validation loss decreased (0.084955 --> 0.084778).  Saving model ...\n",
      "[212/300] train_loss: 0.07278 valid_loss: 0.08526 test_loss: 0.09626 \n",
      "[213/300] train_loss: 0.07494 valid_loss: 0.08577 test_loss: 0.09684 \n",
      "[214/300] train_loss: 0.07611 valid_loss: 0.08394 test_loss: 0.09654 \n",
      "Validation loss decreased (0.084778 --> 0.083944).  Saving model ...\n",
      "[215/300] train_loss: 0.07234 valid_loss: 0.08413 test_loss: 0.09764 \n",
      "[216/300] train_loss: 0.07236 valid_loss: 0.08485 test_loss: 0.09647 \n",
      "[217/300] train_loss: 0.07362 valid_loss: 0.08453 test_loss: 0.09725 \n",
      "[218/300] train_loss: 0.07411 valid_loss: 0.08423 test_loss: 0.09584 \n",
      "[219/300] train_loss: 0.07512 valid_loss: 0.08375 test_loss: 0.09646 \n",
      "Validation loss decreased (0.083944 --> 0.083746).  Saving model ...\n",
      "[220/300] train_loss: 0.07244 valid_loss: 0.08357 test_loss: 0.09587 \n",
      "Validation loss decreased (0.083746 --> 0.083568).  Saving model ...\n",
      "[221/300] train_loss: 0.07415 valid_loss: 0.08393 test_loss: 0.09606 \n",
      "[222/300] train_loss: 0.07568 valid_loss: 0.08415 test_loss: 0.09629 \n",
      "[223/300] train_loss: 0.07222 valid_loss: 0.08407 test_loss: 0.09711 \n",
      "[224/300] train_loss: 0.07240 valid_loss: 0.08313 test_loss: 0.09604 \n",
      "Validation loss decreased (0.083568 --> 0.083126).  Saving model ...\n",
      "[225/300] train_loss: 0.07319 valid_loss: 0.08290 test_loss: 0.09564 \n",
      "Validation loss decreased (0.083126 --> 0.082900).  Saving model ...\n",
      "[226/300] train_loss: 0.07255 valid_loss: 0.08302 test_loss: 0.09604 \n",
      "[227/300] train_loss: 0.07376 valid_loss: 0.08328 test_loss: 0.09667 \n",
      "[228/300] train_loss: 0.07141 valid_loss: 0.08178 test_loss: 0.09562 \n",
      "Validation loss decreased (0.082900 --> 0.081780).  Saving model ...\n",
      "[229/300] train_loss: 0.07166 valid_loss: 0.08362 test_loss: 0.09649 \n",
      "[230/300] train_loss: 0.07422 valid_loss: 0.08508 test_loss: 0.09646 \n",
      "[231/300] train_loss: 0.07234 valid_loss: 0.08321 test_loss: 0.09600 \n",
      "[232/300] train_loss: 0.07216 valid_loss: 0.08260 test_loss: 0.09586 \n",
      "[233/300] train_loss: 0.07303 valid_loss: 0.08278 test_loss: 0.09561 \n",
      "[234/300] train_loss: 0.07060 valid_loss: 0.08283 test_loss: 0.09515 \n",
      "[235/300] train_loss: 0.07434 valid_loss: 0.08283 test_loss: 0.09569 \n",
      "[236/300] train_loss: 0.07358 valid_loss: 0.08564 test_loss: 0.09569 \n",
      "[237/300] train_loss: 0.07178 valid_loss: 0.08311 test_loss: 0.09681 \n",
      "[238/300] train_loss: 0.07248 valid_loss: 0.08394 test_loss: 0.09675 \n",
      "[239/300] train_loss: 0.07316 valid_loss: 0.08235 test_loss: 0.09548 \n",
      "[240/300] train_loss: 0.07061 valid_loss: 0.08372 test_loss: 0.09717 \n",
      "[241/300] train_loss: 0.07165 valid_loss: 0.08211 test_loss: 0.09467 \n",
      "[242/300] train_loss: 0.07149 valid_loss: 0.08250 test_loss: 0.09475 \n",
      "[243/300] train_loss: 0.07166 valid_loss: 0.08416 test_loss: 0.09485 \n",
      "[244/300] train_loss: 0.07146 valid_loss: 0.08338 test_loss: 0.09567 \n",
      "[245/300] train_loss: 0.07289 valid_loss: 0.08296 test_loss: 0.09534 \n",
      "[246/300] train_loss: 0.07000 valid_loss: 0.08220 test_loss: 0.09542 \n",
      "[247/300] train_loss: 0.07302 valid_loss: 0.08287 test_loss: 0.09554 \n",
      "[248/300] train_loss: 0.07297 valid_loss: 0.08147 test_loss: 0.09447 \n",
      "Validation loss decreased (0.081780 --> 0.081475).  Saving model ...\n",
      "[249/300] train_loss: 0.07291 valid_loss: 0.08255 test_loss: 0.09501 \n",
      "[250/300] train_loss: 0.07161 valid_loss: 0.08370 test_loss: 0.09508 \n",
      "[251/300] train_loss: 0.07313 valid_loss: 0.08317 test_loss: 0.09532 \n",
      "[252/300] train_loss: 0.07118 valid_loss: 0.08162 test_loss: 0.09498 \n",
      "[253/300] train_loss: 0.07153 valid_loss: 0.08374 test_loss: 0.09550 \n",
      "[254/300] train_loss: 0.06949 valid_loss: 0.08249 test_loss: 0.09509 \n",
      "[255/300] train_loss: 0.07174 valid_loss: 0.08261 test_loss: 0.09464 \n",
      "[256/300] train_loss: 0.07107 valid_loss: 0.08136 test_loss: 0.09418 \n",
      "Validation loss decreased (0.081475 --> 0.081364).  Saving model ...\n",
      "[257/300] train_loss: 0.06917 valid_loss: 0.08151 test_loss: 0.09500 \n",
      "[258/300] train_loss: 0.07067 valid_loss: 0.08208 test_loss: 0.09554 \n",
      "[259/300] train_loss: 0.06943 valid_loss: 0.08227 test_loss: 0.09459 \n",
      "[260/300] train_loss: 0.06975 valid_loss: 0.08205 test_loss: 0.09476 \n",
      "[261/300] train_loss: 0.07019 valid_loss: 0.08261 test_loss: 0.09545 \n",
      "[262/300] train_loss: 0.06955 valid_loss: 0.08113 test_loss: 0.09493 \n",
      "Validation loss decreased (0.081364 --> 0.081131).  Saving model ...\n",
      "[263/300] train_loss: 0.07090 valid_loss: 0.08167 test_loss: 0.09430 \n",
      "[264/300] train_loss: 0.07067 valid_loss: 0.08290 test_loss: 0.09501 \n",
      "[265/300] train_loss: 0.07235 valid_loss: 0.08353 test_loss: 0.09400 \n",
      "[266/300] train_loss: 0.07053 valid_loss: 0.08199 test_loss: 0.09481 \n",
      "[267/300] train_loss: 0.07129 valid_loss: 0.08171 test_loss: 0.09396 \n",
      "[268/300] train_loss: 0.06892 valid_loss: 0.08177 test_loss: 0.09423 \n",
      "[269/300] train_loss: 0.07002 valid_loss: 0.08133 test_loss: 0.09458 \n",
      "[270/300] train_loss: 0.06952 valid_loss: 0.08055 test_loss: 0.09358 \n",
      "Validation loss decreased (0.081131 --> 0.080551).  Saving model ...\n",
      "[271/300] train_loss: 0.06943 valid_loss: 0.08194 test_loss: 0.09375 \n",
      "[272/300] train_loss: 0.06931 valid_loss: 0.08158 test_loss: 0.09368 \n",
      "[273/300] train_loss: 0.06922 valid_loss: 0.08092 test_loss: 0.09362 \n",
      "[274/300] train_loss: 0.06978 valid_loss: 0.08311 test_loss: 0.09369 \n",
      "[275/300] train_loss: 0.06955 valid_loss: 0.08400 test_loss: 0.09380 \n",
      "[276/300] train_loss: 0.06996 valid_loss: 0.08217 test_loss: 0.09453 \n",
      "[277/300] train_loss: 0.06728 valid_loss: 0.08091 test_loss: 0.09364 \n",
      "[278/300] train_loss: 0.06858 valid_loss: 0.08193 test_loss: 0.09481 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[279/300] train_loss: 0.06944 valid_loss: 0.08321 test_loss: 0.09658 \n",
      "[280/300] train_loss: 0.06909 valid_loss: 0.08060 test_loss: 0.09383 \n",
      "[281/300] train_loss: 0.06914 valid_loss: 0.08030 test_loss: 0.09392 \n",
      "Validation loss decreased (0.080551 --> 0.080298).  Saving model ...\n",
      "[282/300] train_loss: 0.07138 valid_loss: 0.08097 test_loss: 0.09370 \n",
      "[283/300] train_loss: 0.06723 valid_loss: 0.07984 test_loss: 0.09274 \n",
      "Validation loss decreased (0.080298 --> 0.079839).  Saving model ...\n",
      "[284/300] train_loss: 0.06801 valid_loss: 0.08106 test_loss: 0.09395 \n",
      "[285/300] train_loss: 0.06866 valid_loss: 0.07917 test_loss: 0.09328 \n",
      "Validation loss decreased (0.079839 --> 0.079172).  Saving model ...\n",
      "[286/300] train_loss: 0.06634 valid_loss: 0.07954 test_loss: 0.09329 \n",
      "[287/300] train_loss: 0.06890 valid_loss: 0.08136 test_loss: 0.09304 \n",
      "[288/300] train_loss: 0.06868 valid_loss: 0.08283 test_loss: 0.09417 \n",
      "[289/300] train_loss: 0.06879 valid_loss: 0.07938 test_loss: 0.09208 \n",
      "[290/300] train_loss: 0.06924 valid_loss: 0.08052 test_loss: 0.09344 \n",
      "[291/300] train_loss: 0.06886 valid_loss: 0.08005 test_loss: 0.09277 \n",
      "[292/300] train_loss: 0.06843 valid_loss: 0.08058 test_loss: 0.09265 \n",
      "[293/300] train_loss: 0.06714 valid_loss: 0.08091 test_loss: 0.09361 \n",
      "[294/300] train_loss: 0.06906 valid_loss: 0.08032 test_loss: 0.09377 \n",
      "[295/300] train_loss: 0.06688 valid_loss: 0.08142 test_loss: 0.09396 \n",
      "[296/300] train_loss: 0.06726 valid_loss: 0.07943 test_loss: 0.09264 \n",
      "[297/300] train_loss: 0.06691 valid_loss: 0.07916 test_loss: 0.09322 \n",
      "Validation loss decreased (0.079172 --> 0.079161).  Saving model ...\n",
      "[298/300] train_loss: 0.06831 valid_loss: 0.08045 test_loss: 0.09405 \n",
      "[299/300] train_loss: 0.06845 valid_loss: 0.08161 test_loss: 0.09425 \n",
      "[300/300] train_loss: 0.06689 valid_loss: 0.07992 test_loss: 0.09334 \n",
      "TRAINING MODEL 9\n",
      "[  1/300] train_loss: 0.62825 valid_loss: 0.56334 test_loss: 0.57512 \n",
      "Validation loss decreased (inf --> 0.563343).  Saving model ...\n",
      "[  2/300] train_loss: 0.48981 valid_loss: 0.44436 test_loss: 0.46113 \n",
      "Validation loss decreased (0.563343 --> 0.444365).  Saving model ...\n",
      "[  3/300] train_loss: 0.39038 valid_loss: 0.37145 test_loss: 0.38700 \n",
      "Validation loss decreased (0.444365 --> 0.371454).  Saving model ...\n",
      "[  4/300] train_loss: 0.32893 valid_loss: 0.32422 test_loss: 0.34085 \n",
      "Validation loss decreased (0.371454 --> 0.324219).  Saving model ...\n",
      "[  5/300] train_loss: 0.28855 valid_loss: 0.28061 test_loss: 0.30384 \n",
      "Validation loss decreased (0.324219 --> 0.280610).  Saving model ...\n",
      "[  6/300] train_loss: 0.25739 valid_loss: 0.25648 test_loss: 0.27905 \n",
      "Validation loss decreased (0.280610 --> 0.256480).  Saving model ...\n",
      "[  7/300] train_loss: 0.23197 valid_loss: 0.23312 test_loss: 0.25696 \n",
      "Validation loss decreased (0.256480 --> 0.233123).  Saving model ...\n",
      "[  8/300] train_loss: 0.21513 valid_loss: 0.21701 test_loss: 0.23746 \n",
      "Validation loss decreased (0.233123 --> 0.217014).  Saving model ...\n",
      "[  9/300] train_loss: 0.19911 valid_loss: 0.20019 test_loss: 0.21893 \n",
      "Validation loss decreased (0.217014 --> 0.200188).  Saving model ...\n",
      "[ 10/300] train_loss: 0.18325 valid_loss: 0.18703 test_loss: 0.20368 \n",
      "Validation loss decreased (0.200188 --> 0.187033).  Saving model ...\n",
      "[ 11/300] train_loss: 0.17399 valid_loss: 0.18247 test_loss: 0.19697 \n",
      "Validation loss decreased (0.187033 --> 0.182475).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16869 valid_loss: 0.17863 test_loss: 0.19202 \n",
      "Validation loss decreased (0.182475 --> 0.178631).  Saving model ...\n",
      "[ 13/300] train_loss: 0.16083 valid_loss: 0.16878 test_loss: 0.18105 \n",
      "Validation loss decreased (0.178631 --> 0.168776).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15769 valid_loss: 0.16338 test_loss: 0.17423 \n",
      "Validation loss decreased (0.168776 --> 0.163384).  Saving model ...\n",
      "[ 15/300] train_loss: 0.15169 valid_loss: 0.15617 test_loss: 0.17066 \n",
      "Validation loss decreased (0.163384 --> 0.156168).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14521 valid_loss: 0.15612 test_loss: 0.16713 \n",
      "Validation loss decreased (0.156168 --> 0.156115).  Saving model ...\n",
      "[ 17/300] train_loss: 0.15109 valid_loss: 0.15347 test_loss: 0.16647 \n",
      "Validation loss decreased (0.156115 --> 0.153473).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14167 valid_loss: 0.14803 test_loss: 0.16035 \n",
      "Validation loss decreased (0.153473 --> 0.148032).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13950 valid_loss: 0.14848 test_loss: 0.16143 \n",
      "[ 20/300] train_loss: 0.13654 valid_loss: 0.14676 test_loss: 0.15649 \n",
      "Validation loss decreased (0.148032 --> 0.146761).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13432 valid_loss: 0.14166 test_loss: 0.15500 \n",
      "Validation loss decreased (0.146761 --> 0.141658).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13200 valid_loss: 0.13822 test_loss: 0.15338 \n",
      "Validation loss decreased (0.141658 --> 0.138221).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12960 valid_loss: 0.13699 test_loss: 0.15110 \n",
      "Validation loss decreased (0.138221 --> 0.136988).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12850 valid_loss: 0.13501 test_loss: 0.15102 \n",
      "Validation loss decreased (0.136988 --> 0.135013).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12766 valid_loss: 0.13277 test_loss: 0.14761 \n",
      "Validation loss decreased (0.135013 --> 0.132772).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12713 valid_loss: 0.13205 test_loss: 0.14636 \n",
      "Validation loss decreased (0.132772 --> 0.132053).  Saving model ...\n",
      "[ 27/300] train_loss: 0.12183 valid_loss: 0.13022 test_loss: 0.14595 \n",
      "Validation loss decreased (0.132053 --> 0.130221).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12765 valid_loss: 0.12863 test_loss: 0.14423 \n",
      "Validation loss decreased (0.130221 --> 0.128631).  Saving model ...\n",
      "[ 29/300] train_loss: 0.12547 valid_loss: 0.12968 test_loss: 0.14390 \n",
      "[ 30/300] train_loss: 0.11982 valid_loss: 0.13087 test_loss: 0.14676 \n",
      "[ 31/300] train_loss: 0.12238 valid_loss: 0.12433 test_loss: 0.13923 \n",
      "Validation loss decreased (0.128631 --> 0.124330).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11892 valid_loss: 0.12673 test_loss: 0.14324 \n",
      "[ 33/300] train_loss: 0.11992 valid_loss: 0.12392 test_loss: 0.14136 \n",
      "Validation loss decreased (0.124330 --> 0.123920).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11637 valid_loss: 0.12462 test_loss: 0.13953 \n",
      "[ 35/300] train_loss: 0.11607 valid_loss: 0.12350 test_loss: 0.13773 \n",
      "Validation loss decreased (0.123920 --> 0.123496).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11439 valid_loss: 0.12354 test_loss: 0.13997 \n",
      "[ 37/300] train_loss: 0.11322 valid_loss: 0.12158 test_loss: 0.13801 \n",
      "Validation loss decreased (0.123496 --> 0.121584).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11511 valid_loss: 0.12156 test_loss: 0.13624 \n",
      "Validation loss decreased (0.121584 --> 0.121558).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11474 valid_loss: 0.11717 test_loss: 0.13392 \n",
      "Validation loss decreased (0.121558 --> 0.117171).  Saving model ...\n",
      "[ 40/300] train_loss: 0.11326 valid_loss: 0.11708 test_loss: 0.13199 \n",
      "Validation loss decreased (0.117171 --> 0.117075).  Saving model ...\n",
      "[ 41/300] train_loss: 0.11246 valid_loss: 0.11782 test_loss: 0.13223 \n",
      "[ 42/300] train_loss: 0.10803 valid_loss: 0.12052 test_loss: 0.13733 \n",
      "[ 43/300] train_loss: 0.11038 valid_loss: 0.11597 test_loss: 0.13227 \n",
      "Validation loss decreased (0.117075 --> 0.115970).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10759 valid_loss: 0.11632 test_loss: 0.13150 \n",
      "[ 45/300] train_loss: 0.10870 valid_loss: 0.11444 test_loss: 0.13129 \n",
      "Validation loss decreased (0.115970 --> 0.114441).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10595 valid_loss: 0.11323 test_loss: 0.13001 \n",
      "Validation loss decreased (0.114441 --> 0.113235).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10413 valid_loss: 0.11543 test_loss: 0.12921 \n",
      "[ 48/300] train_loss: 0.10464 valid_loss: 0.11398 test_loss: 0.12776 \n",
      "[ 49/300] train_loss: 0.10535 valid_loss: 0.11296 test_loss: 0.12672 \n",
      "Validation loss decreased (0.113235 --> 0.112961).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10641 valid_loss: 0.11479 test_loss: 0.12856 \n",
      "[ 51/300] train_loss: 0.10692 valid_loss: 0.11152 test_loss: 0.12627 \n",
      "Validation loss decreased (0.112961 --> 0.111519).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10452 valid_loss: 0.11577 test_loss: 0.12748 \n",
      "[ 53/300] train_loss: 0.10435 valid_loss: 0.11722 test_loss: 0.12763 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54/300] train_loss: 0.10249 valid_loss: 0.11359 test_loss: 0.12433 \n",
      "[ 55/300] train_loss: 0.10111 valid_loss: 0.11344 test_loss: 0.12586 \n",
      "[ 93/300] train_loss: 0.08912 valid_loss: 0.10271 test_loss: 0.11103 \n",
      "[ 94/300] train_loss: 0.08978 valid_loss: 0.09842 test_loss: 0.11008 \n",
      "Validation loss decreased (0.100089 --> 0.098425).  Saving model ...\n",
      "[ 95/300] train_loss: 0.08714 valid_loss: 0.09914 test_loss: 0.11116 \n",
      "[ 96/300] train_loss: 0.09106 valid_loss: 0.10218 test_loss: 0.11074 \n",
      "[ 97/300] train_loss: 0.09001 valid_loss: 0.09832 test_loss: 0.11155 \n",
      "Validation loss decreased (0.098425 --> 0.098317).  Saving model ...\n",
      "[ 98/300] train_loss: 0.08846 valid_loss: 0.09871 test_loss: 0.11064 \n",
      "[ 99/300] train_loss: 0.08932 valid_loss: 0.10036 test_loss: 0.11002 \n",
      "[100/300] train_loss: 0.08934 valid_loss: 0.09960 test_loss: 0.11132 \n",
      "[101/300] train_loss: 0.08762 valid_loss: 0.10105 test_loss: 0.11073 \n",
      "[102/300] train_loss: 0.08539 valid_loss: 0.10024 test_loss: 0.10958 \n",
      "[103/300] train_loss: 0.08531 valid_loss: 0.10197 test_loss: 0.10983 \n",
      "[104/300] train_loss: 0.08825 valid_loss: 0.09662 test_loss: 0.10881 \n",
      "Validation loss decreased (0.098317 --> 0.096617).  Saving model ...\n",
      "[105/300] train_loss: 0.08339 valid_loss: 0.09501 test_loss: 0.10906 \n",
      "Validation loss decreased (0.096617 --> 0.095011).  Saving model ...\n",
      "[106/300] train_loss: 0.08616 valid_loss: 0.09817 test_loss: 0.10838 \n",
      "[107/300] train_loss: 0.08545 valid_loss: 0.09681 test_loss: 0.10872 \n",
      "[108/300] train_loss: 0.08519 valid_loss: 0.09956 test_loss: 0.10853 \n",
      "[109/300] train_loss: 0.08780 valid_loss: 0.10024 test_loss: 0.10741 \n",
      "[110/300] train_loss: 0.08762 valid_loss: 0.09762 test_loss: 0.10700 \n",
      "[111/300] train_loss: 0.08555 valid_loss: 0.09788 test_loss: 0.10743 \n",
      "[112/300] train_loss: 0.08667 valid_loss: 0.09608 test_loss: 0.10907 \n",
      "[113/300] train_loss: 0.08428 valid_loss: 0.09688 test_loss: 0.10708 \n",
      "[114/300] train_loss: 0.08600 valid_loss: 0.09593 test_loss: 0.10866 \n",
      "[115/300] train_loss: 0.08563 valid_loss: 0.09640 test_loss: 0.10770 \n",
      "[116/300] train_loss: 0.08262 valid_loss: 0.09607 test_loss: 0.10580 \n",
      "[117/300] train_loss: 0.08506 valid_loss: 0.09653 test_loss: 0.10618 \n",
      "[118/300] train_loss: 0.08502 valid_loss: 0.09718 test_loss: 0.10723 \n",
      "[119/300] train_loss: 0.08324 valid_loss: 0.09447 test_loss: 0.10610 \n",
      "Validation loss decreased (0.095011 --> 0.094466).  Saving model ...\n",
      "[120/300] train_loss: 0.08540 valid_loss: 0.09663 test_loss: 0.10679 \n",
      "[121/300] train_loss: 0.08469 valid_loss: 0.09419 test_loss: 0.10593 \n",
      "Validation loss decreased (0.094466 --> 0.094186).  Saving model ...\n",
      "[122/300] train_loss: 0.08201 valid_loss: 0.09856 test_loss: 0.10708 \n",
      "[123/300] train_loss: 0.08572 valid_loss: 0.09349 test_loss: 0.10463 \n",
      "Validation loss decreased (0.094186 --> 0.093492).  Saving model ...\n",
      "[124/300] train_loss: 0.08126 valid_loss: 0.09269 test_loss: 0.10514 \n",
      "Validation loss decreased (0.093492 --> 0.092689).  Saving model ...\n",
      "[125/300] train_loss: 0.08215 valid_loss: 0.09609 test_loss: 0.10743 \n",
      "[126/300] train_loss: 0.08464 valid_loss: 0.09618 test_loss: 0.10736 \n",
      "[127/300] train_loss: 0.08150 valid_loss: 0.09343 test_loss: 0.10598 \n",
      "[128/300] train_loss: 0.08058 valid_loss: 0.09408 test_loss: 0.10414 \n",
      "[129/300] train_loss: 0.08318 valid_loss: 0.09632 test_loss: 0.10398 \n",
      "[130/300] train_loss: 0.08150 valid_loss: 0.09514 test_loss: 0.10492 \n",
      "[131/300] train_loss: 0.08009 valid_loss: 0.09708 test_loss: 0.10699 \n",
      "[132/300] train_loss: 0.08302 valid_loss: 0.09325 test_loss: 0.10460 \n",
      "[133/300] train_loss: 0.08236 valid_loss: 0.09269 test_loss: 0.10434 \n",
      "[134/300] train_loss: 0.07916 valid_loss: 0.09160 test_loss: 0.10505 \n",
      "Validation loss decreased (0.092689 --> 0.091597).  Saving model ...\n",
      "[135/300] train_loss: 0.08051 valid_loss: 0.09086 test_loss: 0.10434 \n",
      "Validation loss decreased (0.091597 --> 0.090864).  Saving model ...\n",
      "[136/300] train_loss: 0.08236 valid_loss: 0.09498 test_loss: 0.10478 \n",
      "[137/300] train_loss: 0.08007 valid_loss: 0.09127 test_loss: 0.10465 \n",
      "[138/300] train_loss: 0.08168 valid_loss: 0.09352 test_loss: 0.10278 \n",
      "[139/300] train_loss: 0.08279 valid_loss: 0.09432 test_loss: 0.10359 \n",
      "[140/300] train_loss: 0.08156 valid_loss: 0.09730 test_loss: 0.10360 \n",
      "[141/300] train_loss: 0.08331 valid_loss: 0.09451 test_loss: 0.10426 \n",
      "[142/300] train_loss: 0.08137 valid_loss: 0.09177 test_loss: 0.10264 \n",
      "[143/300] train_loss: 0.08085 valid_loss: 0.09153 test_loss: 0.10364 \n",
      "[144/300] train_loss: 0.07974 valid_loss: 0.09464 test_loss: 0.10372 \n",
      "[145/300] train_loss: 0.08327 valid_loss: 0.09410 test_loss: 0.10283 \n",
      "[146/300] train_loss: 0.08133 valid_loss: 0.09265 test_loss: 0.10389 \n",
      "[147/300] train_loss: 0.08228 valid_loss: 0.09778 test_loss: 0.10409 \n",
      "[148/300] train_loss: 0.07941 valid_loss: 0.09147 test_loss: 0.10300 \n",
      "[149/300] train_loss: 0.08033 valid_loss: 0.09459 test_loss: 0.10324 \n",
      "[150/300] train_loss: 0.08113 valid_loss: 0.08962 test_loss: 0.10304 \n",
      "Validation loss decreased (0.090864 --> 0.089617).  Saving model ...\n",
      "[151/300] train_loss: 0.07862 valid_loss: 0.09104 test_loss: 0.10279 \n",
      "[152/300] train_loss: 0.07886 valid_loss: 0.09217 test_loss: 0.10269 \n",
      "[153/300] train_loss: 0.07924 valid_loss: 0.08941 test_loss: 0.10230 \n",
      "Validation loss decreased (0.089617 --> 0.089409).  Saving model ...\n",
      "[154/300] train_loss: 0.08008 valid_loss: 0.09278 test_loss: 0.10318 \n",
      "[155/300] train_loss: 0.07891 valid_loss: 0.09145 test_loss: 0.10258 \n",
      "[156/300] train_loss: 0.07852 valid_loss: 0.09281 test_loss: 0.10422 \n",
      "[157/300] train_loss: 0.07974 valid_loss: 0.08933 test_loss: 0.10186 \n",
      "Validation loss decreased (0.089409 --> 0.089330).  Saving model ...\n",
      "[158/300] train_loss: 0.07854 valid_loss: 0.09196 test_loss: 0.10151 \n",
      "[159/300] train_loss: 0.07883 valid_loss: 0.09352 test_loss: 0.10320 \n",
      "[160/300] train_loss: 0.07793 valid_loss: 0.09446 test_loss: 0.10051 \n",
      "[161/300] train_loss: 0.07949 valid_loss: 0.09024 test_loss: 0.10033 \n",
      "[162/300] train_loss: 0.07573 valid_loss: 0.09301 test_loss: 0.10240 \n",
      "[163/300] train_loss: 0.07837 valid_loss: 0.08916 test_loss: 0.10139 \n",
      "Validation loss decreased (0.089330 --> 0.089156).  Saving model ...\n",
      "[164/300] train_loss: 0.07777 valid_loss: 0.09034 test_loss: 0.10147 \n",
      "[165/300] train_loss: 0.07755 valid_loss: 0.09108 test_loss: 0.10162 \n",
      "[166/300] train_loss: 0.07885 valid_loss: 0.09015 test_loss: 0.10313 \n",
      "[167/300] train_loss: 0.07794 valid_loss: 0.08837 test_loss: 0.10097 \n",
      "Validation loss decreased (0.089156 --> 0.088367).  Saving model ...\n",
      "[168/300] train_loss: 0.07833 valid_loss: 0.08998 test_loss: 0.10191 \n",
      "[169/300] train_loss: 0.07761 valid_loss: 0.09380 test_loss: 0.10124 \n",
      "[170/300] train_loss: 0.07618 valid_loss: 0.09004 test_loss: 0.10147 \n",
      "[171/300] train_loss: 0.07706 valid_loss: 0.08938 test_loss: 0.10148 \n",
      "[172/300] train_loss: 0.07709 valid_loss: 0.09007 test_loss: 0.10061 \n",
      "[173/300] train_loss: 0.07509 valid_loss: 0.09327 test_loss: 0.10088 \n",
      "[174/300] train_loss: 0.07625 valid_loss: 0.09343 test_loss: 0.10066 \n",
      "[175/300] train_loss: 0.07609 valid_loss: 0.09595 test_loss: 0.09913 \n",
      "[176/300] train_loss: 0.07673 valid_loss: 0.09113 test_loss: 0.10085 \n",
      "[177/300] train_loss: 0.07784 valid_loss: 0.08880 test_loss: 0.10129 \n",
      "[178/300] train_loss: 0.07617 valid_loss: 0.08912 test_loss: 0.09906 \n",
      "[179/300] train_loss: 0.07696 valid_loss: 0.08743 test_loss: 0.09924 \n",
      "Validation loss decreased (0.088367 --> 0.087427).  Saving model ...\n",
      "[180/300] train_loss: 0.07506 valid_loss: 0.09814 test_loss: 0.10022 \n",
      "[181/300] train_loss: 0.07646 valid_loss: 0.09297 test_loss: 0.10004 \n",
      "[182/300] train_loss: 0.07845 valid_loss: 0.10411 test_loss: 0.10014 \n",
      "[183/300] train_loss: 0.07744 valid_loss: 0.09586 test_loss: 0.09931 \n",
      "[184/300] train_loss: 0.07602 valid_loss: 0.10017 test_loss: 0.10062 \n",
      "[185/300] train_loss: 0.07811 valid_loss: 0.08945 test_loss: 0.09850 \n",
      "[186/300] train_loss: 0.07618 valid_loss: 0.10073 test_loss: 0.09934 \n",
      "[187/300] train_loss: 0.07564 valid_loss: 0.09653 test_loss: 0.09938 \n",
      "[188/300] train_loss: 0.07503 valid_loss: 0.09638 test_loss: 0.10062 \n",
      "[189/300] train_loss: 0.07465 valid_loss: 0.08871 test_loss: 0.09951 \n",
      "[190/300] train_loss: 0.07241 valid_loss: 0.09427 test_loss: 0.09916 \n",
      "[191/300] train_loss: 0.07618 valid_loss: 0.09763 test_loss: 0.10016 \n",
      "[192/300] train_loss: 0.07358 valid_loss: 0.08692 test_loss: 0.10024 \n",
      "Validation loss decreased (0.087427 --> 0.086916).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193/300] train_loss: 0.07805 valid_loss: 0.08978 test_loss: 0.09876 \n",
      "[194/300] train_loss: 0.07504 valid_loss: 0.09001 test_loss: 0.09916 \n",
      "[195/300] train_loss: 0.07408 valid_loss: 0.08885 test_loss: 0.09873 \n",
      "[196/300] train_loss: 0.07624 valid_loss: 0.08900 test_loss: 0.09881 \n",
      "[197/300] train_loss: 0.07463 valid_loss: 0.08650 test_loss: 0.09860 \n",
      "Validation loss decreased (0.086916 --> 0.086500).  Saving model ...\n",
      "[198/300] train_loss: 0.07578 valid_loss: 0.08851 test_loss: 0.09892 \n",
      "[199/300] train_loss: 0.07568 valid_loss: 0.08910 test_loss: 0.09946 \n",
      "[200/300] train_loss: 0.07502 valid_loss: 0.08748 test_loss: 0.09934 \n",
      "[201/300] train_loss: 0.07662 valid_loss: 0.08925 test_loss: 0.09847 \n",
      "[202/300] train_loss: 0.07288 valid_loss: 0.08703 test_loss: 0.09882 \n",
      "[203/300] train_loss: 0.07123 valid_loss: 0.08721 test_loss: 0.09811 \n",
      "[204/300] train_loss: 0.07403 valid_loss: 0.09302 test_loss: 0.09871 \n",
      "[205/300] train_loss: 0.07535 valid_loss: 0.08818 test_loss: 0.09802 \n",
      "[206/300] train_loss: 0.07693 valid_loss: 0.08631 test_loss: 0.09793 \n",
      "Validation loss decreased (0.086500 --> 0.086312).  Saving model ...\n",
      "[207/300] train_loss: 0.07605 valid_loss: 0.08577 test_loss: 0.09775 \n",
      "Validation loss decreased (0.086312 --> 0.085767).  Saving model ...\n",
      "[208/300] train_loss: 0.07497 valid_loss: 0.08524 test_loss: 0.09740 \n",
      "Validation loss decreased (0.085767 --> 0.085242).  Saving model ...\n",
      "[209/300] train_loss: 0.07496 valid_loss: 0.08686 test_loss: 0.09894 \n",
      "[210/300] train_loss: 0.07485 valid_loss: 0.08749 test_loss: 0.09883 \n",
      "[211/300] train_loss: 0.07436 valid_loss: 0.08985 test_loss: 0.09793 \n",
      "[212/300] train_loss: 0.07335 valid_loss: 0.08770 test_loss: 0.09798 \n",
      "[213/300] train_loss: 0.07704 valid_loss: 0.08605 test_loss: 0.09770 \n",
      "[214/300] train_loss: 0.07203 valid_loss: 0.08760 test_loss: 0.09822 \n",
      "[215/300] train_loss: 0.07613 valid_loss: 0.08842 test_loss: 0.09757 \n",
      "[216/300] train_loss: 0.07586 valid_loss: 0.08567 test_loss: 0.09736 \n",
      "[217/300] train_loss: 0.07541 valid_loss: 0.09585 test_loss: 0.09771 \n",
      "[218/300] train_loss: 0.07250 valid_loss: 0.08746 test_loss: 0.09833 \n",
      "[219/300] train_loss: 0.07238 valid_loss: 0.08700 test_loss: 0.09758 \n",
      "[220/300] train_loss: 0.07643 valid_loss: 0.08748 test_loss: 0.09696 \n",
      "[221/300] train_loss: 0.07352 valid_loss: 0.08624 test_loss: 0.09759 \n",
      "[222/300] train_loss: 0.07348 valid_loss: 0.08427 test_loss: 0.09621 \n",
      "Validation loss decreased (0.085242 --> 0.084274).  Saving model ...\n",
      "[223/300] train_loss: 0.07270 valid_loss: 0.08638 test_loss: 0.09669 \n",
      "[224/300] train_loss: 0.07359 valid_loss: 0.08608 test_loss: 0.09697 \n",
      "[225/300] train_loss: 0.07355 valid_loss: 0.08708 test_loss: 0.09807 \n",
      "[226/300] train_loss: 0.07424 valid_loss: 0.08670 test_loss: 0.09820 \n",
      "[227/300] train_loss: 0.07375 valid_loss: 0.08707 test_loss: 0.09635 \n",
      "[228/300] train_loss: 0.07357 valid_loss: 0.08838 test_loss: 0.09802 \n",
      "[229/300] train_loss: 0.07207 valid_loss: 0.08525 test_loss: 0.09809 \n",
      "[230/300] train_loss: 0.07191 valid_loss: 0.08572 test_loss: 0.09612 \n",
      "[231/300] train_loss: 0.07198 valid_loss: 0.08469 test_loss: 0.09649 \n",
      "[232/300] train_loss: 0.06974 valid_loss: 0.08483 test_loss: 0.09669 \n",
      "[233/300] train_loss: 0.07084 valid_loss: 0.08854 test_loss: 0.09606 \n",
      "[234/300] train_loss: 0.06891 valid_loss: 0.08662 test_loss: 0.09676 \n",
      "[235/300] train_loss: 0.07168 valid_loss: 0.08579 test_loss: 0.09613 \n",
      "[236/300] train_loss: 0.07172 valid_loss: 0.08707 test_loss: 0.09685 \n",
      "[237/300] train_loss: 0.06986 valid_loss: 0.08363 test_loss: 0.09567 \n",
      "Validation loss decreased (0.084274 --> 0.083633).  Saving model ...\n",
      "[238/300] train_loss: 0.07183 valid_loss: 0.08644 test_loss: 0.09627 \n",
      "[239/300] train_loss: 0.07197 valid_loss: 0.08461 test_loss: 0.09585 \n",
      "[240/300] train_loss: 0.06949 valid_loss: 0.08477 test_loss: 0.09664 \n",
      "[241/300] train_loss: 0.07036 valid_loss: 0.08616 test_loss: 0.09622 \n",
      "[242/300] train_loss: 0.07221 valid_loss: 0.08482 test_loss: 0.09715 \n",
      "[243/300] train_loss: 0.07223 valid_loss: 0.08667 test_loss: 0.09576 \n",
      "[244/300] train_loss: 0.07268 valid_loss: 0.08354 test_loss: 0.09578 \n",
      "Validation loss decreased (0.083633 --> 0.083542).  Saving model ...\n",
      "[245/300] train_loss: 0.06939 valid_loss: 0.08428 test_loss: 0.09601 \n",
      "[246/300] train_loss: 0.07036 valid_loss: 0.08518 test_loss: 0.09666 \n",
      "[247/300] train_loss: 0.07135 valid_loss: 0.08459 test_loss: 0.09552 \n",
      "[248/300] train_loss: 0.07315 valid_loss: 0.08546 test_loss: 0.09580 \n",
      "[249/300] train_loss: 0.06916 valid_loss: 0.08498 test_loss: 0.09561 \n",
      "[250/300] train_loss: 0.07127 valid_loss: 0.08798 test_loss: 0.09671 \n",
      "[251/300] train_loss: 0.07039 valid_loss: 0.08490 test_loss: 0.09509 \n",
      "[252/300] train_loss: 0.07105 valid_loss: 0.09025 test_loss: 0.09563 \n",
      "[253/300] train_loss: 0.07108 valid_loss: 0.08474 test_loss: 0.09498 \n",
      "[254/300] train_loss: 0.07134 valid_loss: 0.08365 test_loss: 0.09562 \n",
      "[255/300] train_loss: 0.07063 valid_loss: 0.08456 test_loss: 0.09576 \n",
      "[256/300] train_loss: 0.07185 valid_loss: 0.08283 test_loss: 0.09639 \n",
      "Validation loss decreased (0.083542 --> 0.082829).  Saving model ...\n",
      "[257/300] train_loss: 0.07300 valid_loss: 0.08426 test_loss: 0.09604 \n",
      "[258/300] train_loss: 0.07059 valid_loss: 0.09012 test_loss: 0.09558 \n",
      "[259/300] train_loss: 0.07023 valid_loss: 0.08633 test_loss: 0.09549 \n",
      "[260/300] train_loss: 0.06946 valid_loss: 0.08210 test_loss: 0.09661 \n",
      "Validation loss decreased (0.082829 --> 0.082099).  Saving model ...\n",
      "[261/300] train_loss: 0.06881 valid_loss: 0.08313 test_loss: 0.09582 \n",
      "[262/300] train_loss: 0.07054 valid_loss: 0.08337 test_loss: 0.09609 \n",
      "[263/300] train_loss: 0.06900 valid_loss: 0.08234 test_loss: 0.09402 \n",
      "[264/300] train_loss: 0.07042 valid_loss: 0.08513 test_loss: 0.09600 \n",
      "[265/300] train_loss: 0.06952 valid_loss: 0.08574 test_loss: 0.09516 \n",
      "[266/300] train_loss: 0.06954 valid_loss: 0.08390 test_loss: 0.09499 \n",
      "[267/300] train_loss: 0.06850 valid_loss: 0.08572 test_loss: 0.09593 \n",
      "[268/300] train_loss: 0.06853 valid_loss: 0.08395 test_loss: 0.09426 \n",
      "[269/300] train_loss: 0.07087 valid_loss: 0.08433 test_loss: 0.09519 \n",
      "[270/300] train_loss: 0.06919 valid_loss: 0.08302 test_loss: 0.09536 \n",
      "[271/300] train_loss: 0.06903 valid_loss: 0.08298 test_loss: 0.09461 \n",
      "[272/300] train_loss: 0.06852 valid_loss: 0.08204 test_loss: 0.09512 \n",
      "Validation loss decreased (0.082099 --> 0.082036).  Saving model ...\n",
      "[273/300] train_loss: 0.07140 valid_loss: 0.08669 test_loss: 0.09498 \n",
      "[274/300] train_loss: 0.06943 valid_loss: 0.08410 test_loss: 0.09426 \n",
      "[275/300] train_loss: 0.07016 valid_loss: 0.08509 test_loss: 0.09364 \n",
      "[276/300] train_loss: 0.06924 valid_loss: 0.08727 test_loss: 0.09406 \n",
      "[277/300] train_loss: 0.06628 valid_loss: 0.08435 test_loss: 0.09418 \n",
      "[278/300] train_loss: 0.06841 valid_loss: 0.08467 test_loss: 0.09471 \n",
      "[279/300] train_loss: 0.06694 valid_loss: 0.08784 test_loss: 0.09391 \n",
      "[280/300] train_loss: 0.06893 valid_loss: 0.08403 test_loss: 0.09555 \n",
      "[281/300] train_loss: 0.07062 valid_loss: 0.08703 test_loss: 0.09411 \n",
      "[282/300] train_loss: 0.06940 valid_loss: 0.08292 test_loss: 0.09371 \n",
      "[283/300] train_loss: 0.06795 valid_loss: 0.08452 test_loss: 0.09519 \n",
      "[284/300] train_loss: 0.06897 valid_loss: 0.08792 test_loss: 0.09313 \n",
      "[285/300] train_loss: 0.06955 valid_loss: 0.08451 test_loss: 0.09313 \n",
      "[286/300] train_loss: 0.06787 valid_loss: 0.08505 test_loss: 0.09400 \n",
      "[287/300] train_loss: 0.06912 valid_loss: 0.08756 test_loss: 0.09491 \n",
      "[288/300] train_loss: 0.07038 valid_loss: 0.08601 test_loss: 0.09356 \n",
      "[289/300] train_loss: 0.06768 valid_loss: 0.08628 test_loss: 0.09481 \n",
      "[290/300] train_loss: 0.06759 valid_loss: 0.08367 test_loss: 0.09431 \n",
      "[291/300] train_loss: 0.06854 valid_loss: 0.08379 test_loss: 0.09377 \n",
      "[292/300] train_loss: 0.06508 valid_loss: 0.08393 test_loss: 0.09454 \n",
      "[293/300] train_loss: 0.06931 valid_loss: 0.08428 test_loss: 0.09455 \n",
      "[294/300] train_loss: 0.06746 valid_loss: 0.08521 test_loss: 0.09429 \n",
      "[295/300] train_loss: 0.06758 valid_loss: 0.08140 test_loss: 0.09358 \n",
      "Validation loss decreased (0.082036 --> 0.081405).  Saving model ...\n",
      "[296/300] train_loss: 0.06791 valid_loss: 0.08186 test_loss: 0.09332 \n",
      "[297/300] train_loss: 0.06675 valid_loss: 0.08528 test_loss: 0.09391 \n",
      "[298/300] train_loss: 0.06647 valid_loss: 0.08285 test_loss: 0.09398 \n",
      "[299/300] train_loss: 0.06627 valid_loss: 0.08393 test_loss: 0.09402 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300/300] train_loss: 0.06679 valid_loss: 0.08166 test_loss: 0.09384 \n",
      "TRAINING MODEL 10\n",
      "[  1/300] train_loss: 0.63514 valid_loss: 0.55292 test_loss: 0.55760 \n",
      "Validation loss decreased (inf --> 0.552923).  Saving model ...\n",
      "[  2/300] train_loss: 0.46196 valid_loss: 0.40505 test_loss: 0.40688 \n",
      "Validation loss decreased (0.552923 --> 0.405054).  Saving model ...\n",
      "[  3/300] train_loss: 0.35481 valid_loss: 0.33669 test_loss: 0.34077 \n",
      "Validation loss decreased (0.405054 --> 0.336694).  Saving model ...\n",
      "[  4/300] train_loss: 0.29658 valid_loss: 0.29027 test_loss: 0.30046 \n",
      "Validation loss decreased (0.336694 --> 0.290273).  Saving model ...\n",
      "[  5/300] train_loss: 0.25867 valid_loss: 0.25282 test_loss: 0.26728 \n",
      "Validation loss decreased (0.290273 --> 0.252822).  Saving model ...\n",
      "[  6/300] train_loss: 0.23007 valid_loss: 0.22365 test_loss: 0.23898 \n",
      "Validation loss decreased (0.252822 --> 0.223653).  Saving model ...\n",
      "[  7/300] train_loss: 0.20716 valid_loss: 0.20718 test_loss: 0.22080 \n",
      "Validation loss decreased (0.223653 --> 0.207183).  Saving model ...\n",
      "[  8/300] train_loss: 0.19091 valid_loss: 0.19178 test_loss: 0.20471 \n",
      "Validation loss decreased (0.207183 --> 0.191784).  Saving model ...\n",
      "[  9/300] train_loss: 0.17803 valid_loss: 0.18075 test_loss: 0.19502 \n",
      "Validation loss decreased (0.191784 --> 0.180750).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17124 valid_loss: 0.17326 test_loss: 0.18455 \n",
      "Validation loss decreased (0.180750 --> 0.173259).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16309 valid_loss: 0.16547 test_loss: 0.17726 \n",
      "Validation loss decreased (0.173259 --> 0.165467).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15553 valid_loss: 0.16199 test_loss: 0.17137 \n",
      "Validation loss decreased (0.165467 --> 0.161993).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15013 valid_loss: 0.15788 test_loss: 0.16893 \n",
      "Validation loss decreased (0.161993 --> 0.157883).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14464 valid_loss: 0.15347 test_loss: 0.16431 \n",
      "Validation loss decreased (0.157883 --> 0.153469).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14508 valid_loss: 0.14617 test_loss: 0.15960 \n",
      "Validation loss decreased (0.153469 --> 0.146167).  Saving model ...\n",
      "[ 16/300] train_loss: 0.14189 valid_loss: 0.14786 test_loss: 0.15958 \n",
      "[ 17/300] train_loss: 0.13627 valid_loss: 0.14279 test_loss: 0.15522 \n",
      "Validation loss decreased (0.146167 --> 0.142786).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13399 valid_loss: 0.14148 test_loss: 0.15371 \n",
      "Validation loss decreased (0.142786 --> 0.141479).  Saving model ...\n",
      "[ 19/300] train_loss: 0.12831 valid_loss: 0.14190 test_loss: 0.15321 \n",
      "[ 20/300] train_loss: 0.13202 valid_loss: 0.13896 test_loss: 0.15186 \n",
      "Validation loss decreased (0.141479 --> 0.138955).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12794 valid_loss: 0.13660 test_loss: 0.15113 \n",
      "Validation loss decreased (0.138955 --> 0.136604).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12737 valid_loss: 0.13229 test_loss: 0.14535 \n",
      "Validation loss decreased (0.136604 --> 0.132285).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12351 valid_loss: 0.13060 test_loss: 0.14517 \n",
      "Validation loss decreased (0.132285 --> 0.130598).  Saving model ...\n",
      "[ 24/300] train_loss: 0.12090 valid_loss: 0.13056 test_loss: 0.14638 \n",
      "Validation loss decreased (0.130598 --> 0.130559).  Saving model ...\n",
      "[ 25/300] train_loss: 0.12135 valid_loss: 0.13085 test_loss: 0.14378 \n",
      "[ 26/300] train_loss: 0.11984 valid_loss: 0.12837 test_loss: 0.14262 \n",
      "Validation loss decreased (0.130559 --> 0.128374).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11883 valid_loss: 0.12696 test_loss: 0.14015 \n",
      "Validation loss decreased (0.128374 --> 0.126960).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12183 valid_loss: 0.12468 test_loss: 0.13978 \n",
      "Validation loss decreased (0.126960 --> 0.124685).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11862 valid_loss: 0.12436 test_loss: 0.13959 \n",
      "Validation loss decreased (0.124685 --> 0.124355).  Saving model ...\n",
      "[ 30/300] train_loss: 0.11571 valid_loss: 0.12343 test_loss: 0.13821 \n",
      "Validation loss decreased (0.124355 --> 0.123429).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11517 valid_loss: 0.12079 test_loss: 0.13541 \n",
      "Validation loss decreased (0.123429 --> 0.120787).  Saving model ...\n",
      "[ 32/300] train_loss: 0.11176 valid_loss: 0.12160 test_loss: 0.13785 \n",
      "[ 33/300] train_loss: 0.11333 valid_loss: 0.12024 test_loss: 0.13494 \n",
      "Validation loss decreased (0.120787 --> 0.120236).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11278 valid_loss: 0.11860 test_loss: 0.13616 \n",
      "Validation loss decreased (0.120236 --> 0.118603).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11329 valid_loss: 0.11988 test_loss: 0.13350 \n",
      "[ 36/300] train_loss: 0.11071 valid_loss: 0.11965 test_loss: 0.13312 \n",
      "[ 37/300] train_loss: 0.11118 valid_loss: 0.11859 test_loss: 0.13161 \n",
      "Validation loss decreased (0.118603 --> 0.118588).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11058 valid_loss: 0.11466 test_loss: 0.13104 \n",
      "Validation loss decreased (0.118588 --> 0.114656).  Saving model ...\n",
      "[ 39/300] train_loss: 0.10898 valid_loss: 0.11583 test_loss: 0.13209 \n",
      "[ 40/300] train_loss: 0.10991 valid_loss: 0.11429 test_loss: 0.12952 \n",
      "Validation loss decreased (0.114656 --> 0.114286).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10930 valid_loss: 0.11598 test_loss: 0.13482 \n",
      "[ 42/300] train_loss: 0.10648 valid_loss: 0.11640 test_loss: 0.12939 \n",
      "[ 43/300] train_loss: 0.10655 valid_loss: 0.11603 test_loss: 0.13026 \n",
      "[ 44/300] train_loss: 0.10675 valid_loss: 0.11490 test_loss: 0.12782 \n",
      "[ 45/300] train_loss: 0.10327 valid_loss: 0.11160 test_loss: 0.12682 \n",
      "Validation loss decreased (0.114286 --> 0.111596).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10292 valid_loss: 0.11134 test_loss: 0.12575 \n",
      "Validation loss decreased (0.111596 --> 0.111341).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10434 valid_loss: 0.11004 test_loss: 0.12551 \n",
      "Validation loss decreased (0.111341 --> 0.110039).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10264 valid_loss: 0.10987 test_loss: 0.12539 \n",
      "Validation loss decreased (0.110039 --> 0.109868).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10392 valid_loss: 0.10950 test_loss: 0.12381 \n",
      "Validation loss decreased (0.109868 --> 0.109499).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10566 valid_loss: 0.10699 test_loss: 0.12547 \n",
      "Validation loss decreased (0.109499 --> 0.106994).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10175 valid_loss: 0.10828 test_loss: 0.12449 \n",
      "[ 52/300] train_loss: 0.10149 valid_loss: 0.10683 test_loss: 0.12301 \n",
      "Validation loss decreased (0.106994 --> 0.106834).  Saving model ...\n",
      "[ 53/300] train_loss: 0.10133 valid_loss: 0.11134 test_loss: 0.12352 \n",
      "[ 54/300] train_loss: 0.09847 valid_loss: 0.10741 test_loss: 0.12255 \n",
      "[ 55/300] train_loss: 0.10149 valid_loss: 0.10485 test_loss: 0.12135 \n",
      "Validation loss decreased (0.106834 --> 0.104849).  Saving model ...\n",
      "[ 56/300] train_loss: 0.09698 valid_loss: 0.10968 test_loss: 0.12204 \n",
      "[ 57/300] train_loss: 0.09929 valid_loss: 0.10629 test_loss: 0.12008 \n",
      "[ 58/300] train_loss: 0.09727 valid_loss: 0.10845 test_loss: 0.12210 \n",
      "[ 59/300] train_loss: 0.09680 valid_loss: 0.10596 test_loss: 0.12119 \n",
      "[ 60/300] train_loss: 0.09811 valid_loss: 0.10416 test_loss: 0.11864 \n",
      "Validation loss decreased (0.104849 --> 0.104164).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09801 valid_loss: 0.10329 test_loss: 0.11942 \n",
      "Validation loss decreased (0.104164 --> 0.103293).  Saving model ...\n",
      "[ 62/300] train_loss: 0.09625 valid_loss: 0.10465 test_loss: 0.11871 \n",
      "[ 63/300] train_loss: 0.09586 valid_loss: 0.10408 test_loss: 0.12051 \n",
      "[ 64/300] train_loss: 0.09495 valid_loss: 0.10415 test_loss: 0.12028 \n",
      "[ 65/300] train_loss: 0.09340 valid_loss: 0.10183 test_loss: 0.11827 \n",
      "Validation loss decreased (0.103293 --> 0.101831).  Saving model ...\n",
      "[ 66/300] train_loss: 0.09511 valid_loss: 0.10341 test_loss: 0.11654 \n",
      "[ 67/300] train_loss: 0.09341 valid_loss: 0.10093 test_loss: 0.11647 \n",
      "Validation loss decreased (0.101831 --> 0.100934).  Saving model ...\n",
      "[ 68/300] train_loss: 0.09350 valid_loss: 0.10211 test_loss: 0.11637 \n",
      "[ 69/300] train_loss: 0.09486 valid_loss: 0.10287 test_loss: 0.11607 \n",
      "[ 70/300] train_loss: 0.09548 valid_loss: 0.10097 test_loss: 0.11602 \n",
      "[ 71/300] train_loss: 0.09454 valid_loss: 0.10025 test_loss: 0.11537 \n",
      "Validation loss decreased (0.100934 --> 0.100248).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 72/300] train_loss: 0.09350 valid_loss: 0.10311 test_loss: 0.11467 \n",
      "[ 73/300] train_loss: 0.09403 valid_loss: 0.10155 test_loss: 0.11386 \n",
      "[ 74/300] train_loss: 0.09386 valid_loss: 0.09918 test_loss: 0.11300 \n",
      "Validation loss decreased (0.100248 --> 0.099183).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09174 valid_loss: 0.09798 test_loss: 0.11283 \n",
      "Validation loss decreased (0.099183 --> 0.097984).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09260 valid_loss: 0.09944 test_loss: 0.11304 \n",
      "[ 77/300] train_loss: 0.09382 valid_loss: 0.09682 test_loss: 0.11207 \n",
      "Validation loss decreased (0.097984 --> 0.096821).  Saving model ...\n",
      "[ 78/300] train_loss: 0.09186 valid_loss: 0.09884 test_loss: 0.11305 \n",
      "[ 79/300] train_loss: 0.09203 valid_loss: 0.09746 test_loss: 0.11293 \n",
      "[ 80/300] train_loss: 0.09123 valid_loss: 0.10469 test_loss: 0.11369 \n",
      "[ 81/300] train_loss: 0.09094 valid_loss: 0.10269 test_loss: 0.11238 \n",
      "[ 82/300] train_loss: 0.08829 valid_loss: 0.09823 test_loss: 0.11196 \n",
      "[ 83/300] train_loss: 0.08964 valid_loss: 0.09995 test_loss: 0.11228 \n",
      "[ 84/300] train_loss: 0.09060 valid_loss: 0.09759 test_loss: 0.11200 \n",
      "[ 85/300] train_loss: 0.08872 valid_loss: 0.09858 test_loss: 0.11238 \n",
      "[ 86/300] train_loss: 0.08834 valid_loss: 0.09821 test_loss: 0.11078 \n",
      "[ 87/300] train_loss: 0.08697 valid_loss: 0.09927 test_loss: 0.11329 \n",
      "[ 88/300] train_loss: 0.08878 valid_loss: 0.09541 test_loss: 0.11085 \n",
      "Validation loss decreased (0.096821 --> 0.095410).  Saving model ...\n",
      "[ 89/300] train_loss: 0.08918 valid_loss: 0.09827 test_loss: 0.11276 \n",
      "[ 90/300] train_loss: 0.09052 valid_loss: 0.09692 test_loss: 0.11137 \n",
      "[ 91/300] train_loss: 0.08951 valid_loss: 0.09596 test_loss: 0.11042 \n",
      "[ 92/300] train_loss: 0.09170 valid_loss: 0.09716 test_loss: 0.10942 \n",
      "[ 93/300] train_loss: 0.08822 valid_loss: 0.09493 test_loss: 0.10902 \n",
      "Validation loss decreased (0.095410 --> 0.094931).  Saving model ...\n",
      "[ 94/300] train_loss: 0.08716 valid_loss: 0.09565 test_loss: 0.10862 \n",
      "[ 95/300] train_loss: 0.08708 valid_loss: 0.09380 test_loss: 0.10885 \n",
      "Validation loss decreased (0.094931 --> 0.093797).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08685 valid_loss: 0.09568 test_loss: 0.10989 \n",
      "[ 97/300] train_loss: 0.08967 valid_loss: 0.09470 test_loss: 0.10752 \n",
      "[ 98/300] train_loss: 0.08791 valid_loss: 0.09431 test_loss: 0.10839 \n",
      "[ 99/300] train_loss: 0.08693 valid_loss: 0.09789 test_loss: 0.10845 \n",
      "[100/300] train_loss: 0.08769 valid_loss: 0.09529 test_loss: 0.10855 \n",
      "[101/300] train_loss: 0.08516 valid_loss: 0.09379 test_loss: 0.10732 \n",
      "Validation loss decreased (0.093797 --> 0.093785).  Saving model ...\n",
      "[102/300] train_loss: 0.08963 valid_loss: 0.09581 test_loss: 0.10924 \n",
      "[103/300] train_loss: 0.08592 valid_loss: 0.09468 test_loss: 0.10904 \n",
      "[104/300] train_loss: 0.08574 valid_loss: 0.09484 test_loss: 0.10776 \n",
      "[105/300] train_loss: 0.08600 valid_loss: 0.09478 test_loss: 0.10757 \n",
      "[106/300] train_loss: 0.08798 valid_loss: 0.09337 test_loss: 0.10769 \n",
      "Validation loss decreased (0.093785 --> 0.093373).  Saving model ...\n",
      "[107/300] train_loss: 0.08720 valid_loss: 0.09255 test_loss: 0.10696 \n",
      "Validation loss decreased (0.093373 --> 0.092551).  Saving model ...\n",
      "[108/300] train_loss: 0.08689 valid_loss: 0.09367 test_loss: 0.10780 \n",
      "[109/300] train_loss: 0.08489 valid_loss: 0.09515 test_loss: 0.10785 \n",
      "[110/300] train_loss: 0.08763 valid_loss: 0.09289 test_loss: 0.10797 \n",
      "[111/300] train_loss: 0.08593 valid_loss: 0.09353 test_loss: 0.10716 \n",
      "[112/300] train_loss: 0.08293 valid_loss: 0.09349 test_loss: 0.10647 \n",
      "[113/300] train_loss: 0.08642 valid_loss: 0.09432 test_loss: 0.10611 \n",
      "[114/300] train_loss: 0.08741 valid_loss: 0.09451 test_loss: 0.10595 \n",
      "[115/300] train_loss: 0.08347 valid_loss: 0.09435 test_loss: 0.10497 \n",
      "[116/300] train_loss: 0.08217 valid_loss: 0.09381 test_loss: 0.10691 \n",
      "[117/300] train_loss: 0.08424 valid_loss: 0.09261 test_loss: 0.10512 \n",
      "[118/300] train_loss: 0.08197 valid_loss: 0.09208 test_loss: 0.10505 \n",
      "Validation loss decreased (0.092551 --> 0.092084).  Saving model ...\n",
      "[119/300] train_loss: 0.08424 valid_loss: 0.09143 test_loss: 0.10371 \n",
      "Validation loss decreased (0.092084 --> 0.091435).  Saving model ...\n",
      "[120/300] train_loss: 0.08225 valid_loss: 0.09082 test_loss: 0.10453 \n",
      "Validation loss decreased (0.091435 --> 0.090820).  Saving model ...\n",
      "[121/300] train_loss: 0.08175 valid_loss: 0.09120 test_loss: 0.10383 \n",
      "[122/300] train_loss: 0.08325 valid_loss: 0.09353 test_loss: 0.10435 \n",
      "[123/300] train_loss: 0.08375 valid_loss: 0.09366 test_loss: 0.10503 \n",
      "[124/300] train_loss: 0.08032 valid_loss: 0.09111 test_loss: 0.10411 \n",
      "[125/300] train_loss: 0.08136 valid_loss: 0.09262 test_loss: 0.10409 \n",
      "[126/300] train_loss: 0.08272 valid_loss: 0.09778 test_loss: 0.10515 \n",
      "[127/300] train_loss: 0.07992 valid_loss: 0.09761 test_loss: 0.10509 \n",
      "[128/300] train_loss: 0.08120 valid_loss: 0.09730 test_loss: 0.10506 \n",
      "[129/300] train_loss: 0.08419 valid_loss: 0.09302 test_loss: 0.10369 \n",
      "[130/300] train_loss: 0.08275 valid_loss: 0.09308 test_loss: 0.10368 \n",
      "[131/300] train_loss: 0.07927 valid_loss: 0.08925 test_loss: 0.10389 \n",
      "Validation loss decreased (0.090820 --> 0.089245).  Saving model ...\n",
      "[132/300] train_loss: 0.08300 valid_loss: 0.09915 test_loss: 0.10357 \n",
      "[133/300] train_loss: 0.08059 valid_loss: 0.09286 test_loss: 0.10309 \n",
      "[134/300] train_loss: 0.07863 valid_loss: 0.09389 test_loss: 0.10325 \n",
      "[135/300] train_loss: 0.08224 valid_loss: 0.09395 test_loss: 0.10359 \n",
      "[136/300] train_loss: 0.08047 valid_loss: 0.09035 test_loss: 0.10309 \n",
      "[137/300] train_loss: 0.07988 valid_loss: 0.09454 test_loss: 0.10279 \n",
      "[138/300] train_loss: 0.07980 valid_loss: 0.09168 test_loss: 0.10235 \n",
      "[139/300] train_loss: 0.08088 valid_loss: 0.09942 test_loss: 0.10172 \n",
      "[140/300] train_loss: 0.08025 valid_loss: 0.09573 test_loss: 0.10181 \n",
      "[141/300] train_loss: 0.07908 valid_loss: 0.09537 test_loss: 0.10377 \n",
      "[142/300] train_loss: 0.08041 valid_loss: 0.09149 test_loss: 0.10170 \n",
      "[143/300] train_loss: 0.07835 valid_loss: 0.09407 test_loss: 0.10111 \n",
      "[144/300] train_loss: 0.07968 valid_loss: 0.09327 test_loss: 0.10169 \n",
      "[145/300] train_loss: 0.07838 valid_loss: 0.09281 test_loss: 0.10157 \n",
      "[146/300] train_loss: 0.08014 valid_loss: 0.08871 test_loss: 0.10099 \n",
      "Validation loss decreased (0.089245 --> 0.088713).  Saving model ...\n",
      "[147/300] train_loss: 0.08141 valid_loss: 0.08939 test_loss: 0.10039 \n",
      "[148/300] train_loss: 0.08210 valid_loss: 0.09255 test_loss: 0.10107 \n",
      "[149/300] train_loss: 0.07670 valid_loss: 0.09286 test_loss: 0.10113 \n",
      "[150/300] train_loss: 0.07981 valid_loss: 0.09117 test_loss: 0.10174 \n",
      "[151/300] train_loss: 0.07871 valid_loss: 0.09169 test_loss: 0.10036 \n",
      "[152/300] train_loss: 0.08080 valid_loss: 0.09069 test_loss: 0.10227 \n",
      "[153/300] train_loss: 0.07688 valid_loss: 0.09015 test_loss: 0.10071 \n",
      "[154/300] train_loss: 0.08025 valid_loss: 0.08998 test_loss: 0.10046 \n",
      "[155/300] train_loss: 0.08047 valid_loss: 0.08858 test_loss: 0.09957 \n",
      "Validation loss decreased (0.088713 --> 0.088581).  Saving model ...\n",
      "[156/300] train_loss: 0.07975 valid_loss: 0.09367 test_loss: 0.09991 \n",
      "[157/300] train_loss: 0.07857 valid_loss: 0.08853 test_loss: 0.09916 \n",
      "Validation loss decreased (0.088581 --> 0.088533).  Saving model ...\n",
      "[158/300] train_loss: 0.07755 valid_loss: 0.09023 test_loss: 0.09953 \n",
      "[159/300] train_loss: 0.07625 valid_loss: 0.09365 test_loss: 0.10007 \n",
      "[160/300] train_loss: 0.07792 valid_loss: 0.09651 test_loss: 0.09966 \n",
      "[161/300] train_loss: 0.07731 valid_loss: 0.09729 test_loss: 0.09978 \n",
      "[162/300] train_loss: 0.07805 valid_loss: 0.09075 test_loss: 0.10044 \n",
      "[163/300] train_loss: 0.07966 valid_loss: 0.10343 test_loss: 0.10087 \n",
      "[164/300] train_loss: 0.07886 valid_loss: 0.09119 test_loss: 0.09936 \n",
      "[165/300] train_loss: 0.07785 valid_loss: 0.08709 test_loss: 0.09901 \n",
      "Validation loss decreased (0.088533 --> 0.087092).  Saving model ...\n",
      "[166/300] train_loss: 0.07834 valid_loss: 0.09023 test_loss: 0.09938 \n",
      "[167/300] train_loss: 0.07725 valid_loss: 0.08786 test_loss: 0.09949 \n",
      "[168/300] train_loss: 0.07925 valid_loss: 0.08764 test_loss: 0.09968 \n",
      "[169/300] train_loss: 0.07775 valid_loss: 0.08756 test_loss: 0.09912 \n",
      "[170/300] train_loss: 0.07612 valid_loss: 0.09094 test_loss: 0.09949 \n",
      "[171/300] train_loss: 0.07403 valid_loss: 0.08963 test_loss: 0.09888 \n",
      "[172/300] train_loss: 0.07621 valid_loss: 0.09078 test_loss: 0.09887 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173/300] train_loss: 0.07529 valid_loss: 0.08865 test_loss: 0.09834 \n",
      "[174/300] train_loss: 0.07687 valid_loss: 0.09082 test_loss: 0.10080 \n",
      "[175/300] train_loss: 0.07615 valid_loss: 0.08999 test_loss: 0.09923 \n",
      "[176/300] train_loss: 0.07574 valid_loss: 0.08768 test_loss: 0.09849 \n",
      "[177/300] train_loss: 0.07747 valid_loss: 0.08605 test_loss: 0.09803 \n",
      "Validation loss decreased (0.087092 --> 0.086054).  Saving model ...\n",
      "[178/300] train_loss: 0.07623 valid_loss: 0.08743 test_loss: 0.09827 \n",
      "[179/300] train_loss: 0.07473 valid_loss: 0.09174 test_loss: 0.09744 \n",
      "[180/300] train_loss: 0.07752 valid_loss: 0.08722 test_loss: 0.09772 \n",
      "[181/300] train_loss: 0.07714 valid_loss: 0.08644 test_loss: 0.09795 \n",
      "[182/300] train_loss: 0.07690 valid_loss: 0.08593 test_loss: 0.09787 \n",
      "Validation loss decreased (0.086054 --> 0.085927).  Saving model ...\n",
      "[183/300] train_loss: 0.07593 valid_loss: 0.08736 test_loss: 0.09751 \n",
      "[184/300] train_loss: 0.07594 valid_loss: 0.08784 test_loss: 0.09937 \n",
      "[185/300] train_loss: 0.07579 valid_loss: 0.08662 test_loss: 0.09739 \n",
      "[186/300] train_loss: 0.07506 valid_loss: 0.08620 test_loss: 0.09681 \n",
      "[187/300] train_loss: 0.07620 valid_loss: 0.08693 test_loss: 0.09751 \n",
      "[188/300] train_loss: 0.07617 valid_loss: 0.08522 test_loss: 0.09670 \n",
      "Validation loss decreased (0.085927 --> 0.085221).  Saving model ...\n",
      "[189/300] train_loss: 0.07297 valid_loss: 0.08550 test_loss: 0.09733 \n",
      "[190/300] train_loss: 0.07687 valid_loss: 0.08538 test_loss: 0.09746 \n",
      "[191/300] train_loss: 0.07640 valid_loss: 0.08529 test_loss: 0.09708 \n",
      "[192/300] train_loss: 0.07582 valid_loss: 0.08538 test_loss: 0.09736 \n",
      "[193/300] train_loss: 0.07692 valid_loss: 0.08618 test_loss: 0.09665 \n",
      "[194/300] train_loss: 0.07252 valid_loss: 0.08587 test_loss: 0.09711 \n",
      "[195/300] train_loss: 0.07695 valid_loss: 0.08552 test_loss: 0.09599 \n",
      "[196/300] train_loss: 0.07545 valid_loss: 0.08495 test_loss: 0.09583 \n",
      "Validation loss decreased (0.085221 --> 0.084950).  Saving model ...\n",
      "[197/300] train_loss: 0.07248 valid_loss: 0.08653 test_loss: 0.09628 \n",
      "[198/300] train_loss: 0.07300 valid_loss: 0.08484 test_loss: 0.09618 \n",
      "Validation loss decreased (0.084950 --> 0.084840).  Saving model ...\n",
      "[199/300] train_loss: 0.07290 valid_loss: 0.08420 test_loss: 0.09489 \n",
      "Validation loss decreased (0.084840 --> 0.084199).  Saving model ...\n",
      "[200/300] train_loss: 0.07359 valid_loss: 0.08855 test_loss: 0.09503 \n",
      "[201/300] train_loss: 0.07238 valid_loss: 0.09043 test_loss: 0.09619 \n",
      "[202/300] train_loss: 0.07233 valid_loss: 0.08376 test_loss: 0.09491 \n",
      "Validation loss decreased (0.084199 --> 0.083763).  Saving model ...\n",
      "[203/300] train_loss: 0.07371 valid_loss: 0.08341 test_loss: 0.09492 \n",
      "Validation loss decreased (0.083763 --> 0.083413).  Saving model ...\n",
      "[204/300] train_loss: 0.07279 valid_loss: 0.08595 test_loss: 0.09560 \n",
      "[205/300] train_loss: 0.07395 valid_loss: 0.08498 test_loss: 0.09556 \n",
      "[206/300] train_loss: 0.07481 valid_loss: 0.08487 test_loss: 0.09455 \n",
      "[207/300] train_loss: 0.07317 valid_loss: 0.08524 test_loss: 0.09572 \n",
      "[208/300] train_loss: 0.07519 valid_loss: 0.08666 test_loss: 0.09481 \n",
      "[209/300] train_loss: 0.07407 valid_loss: 0.08479 test_loss: 0.09617 \n",
      "[210/300] train_loss: 0.07590 valid_loss: 0.08418 test_loss: 0.09445 \n",
      "[211/300] train_loss: 0.07317 valid_loss: 0.08652 test_loss: 0.09644 \n",
      "[212/300] train_loss: 0.07331 valid_loss: 0.08594 test_loss: 0.09514 \n",
      "[213/300] train_loss: 0.07306 valid_loss: 0.08451 test_loss: 0.09434 \n",
      "[214/300] train_loss: 0.07443 valid_loss: 0.08578 test_loss: 0.09479 \n",
      "[215/300] train_loss: 0.07293 valid_loss: 0.08402 test_loss: 0.09517 \n",
      "[216/300] train_loss: 0.07406 valid_loss: 0.08426 test_loss: 0.09417 \n",
      "[217/300] train_loss: 0.07222 valid_loss: 0.08378 test_loss: 0.09415 \n",
      "[218/300] train_loss: 0.07295 valid_loss: 0.08270 test_loss: 0.09412 \n",
      "Validation loss decreased (0.083413 --> 0.082697).  Saving model ...\n",
      "[219/300] train_loss: 0.07163 valid_loss: 0.08264 test_loss: 0.09305 \n",
      "Validation loss decreased (0.082697 --> 0.082639).  Saving model ...\n",
      "[220/300] train_loss: 0.07336 valid_loss: 0.08530 test_loss: 0.09523 \n",
      "[221/300] train_loss: 0.07026 valid_loss: 0.08436 test_loss: 0.09428 \n",
      "[222/300] train_loss: 0.07426 valid_loss: 0.08210 test_loss: 0.09396 \n",
      "Validation loss decreased (0.082639 --> 0.082097).  Saving model ...\n",
      "[223/300] train_loss: 0.07354 valid_loss: 0.08322 test_loss: 0.09393 \n",
      "[224/300] train_loss: 0.07314 valid_loss: 0.08445 test_loss: 0.09371 \n",
      "[225/300] train_loss: 0.07191 valid_loss: 0.08200 test_loss: 0.09295 \n",
      "Validation loss decreased (0.082097 --> 0.082004).  Saving model ...\n",
      "[226/300] train_loss: 0.07215 valid_loss: 0.08367 test_loss: 0.09360 \n",
      "[227/300] train_loss: 0.07206 valid_loss: 0.08487 test_loss: 0.09317 \n",
      "[228/300] train_loss: 0.07395 valid_loss: 0.08448 test_loss: 0.09307 \n",
      "[229/300] train_loss: 0.07068 valid_loss: 0.08374 test_loss: 0.09378 \n",
      "[230/300] train_loss: 0.07071 valid_loss: 0.08575 test_loss: 0.09334 \n",
      "[231/300] train_loss: 0.07014 valid_loss: 0.08618 test_loss: 0.09338 \n",
      "[232/300] train_loss: 0.07066 valid_loss: 0.08521 test_loss: 0.09316 \n",
      "[233/300] train_loss: 0.07294 valid_loss: 0.08456 test_loss: 0.09348 \n",
      "[234/300] train_loss: 0.07233 valid_loss: 0.08129 test_loss: 0.09268 \n",
      "Validation loss decreased (0.082004 --> 0.081291).  Saving model ...\n",
      "[235/300] train_loss: 0.07046 valid_loss: 0.08271 test_loss: 0.09294 \n",
      "[236/300] train_loss: 0.07040 valid_loss: 0.08280 test_loss: 0.09347 \n",
      "[237/300] train_loss: 0.07157 valid_loss: 0.08270 test_loss: 0.09306 \n",
      "[238/300] train_loss: 0.06945 valid_loss: 0.08307 test_loss: 0.09364 \n",
      "[239/300] train_loss: 0.07277 valid_loss: 0.08180 test_loss: 0.09280 \n",
      "[240/300] train_loss: 0.07238 valid_loss: 0.08041 test_loss: 0.09321 \n",
      "Validation loss decreased (0.081291 --> 0.080406).  Saving model ...\n",
      "[241/300] train_loss: 0.06949 valid_loss: 0.08192 test_loss: 0.09360 \n",
      "[242/300] train_loss: 0.06974 valid_loss: 0.08192 test_loss: 0.09319 \n",
      "[243/300] train_loss: 0.07114 valid_loss: 0.08224 test_loss: 0.09356 \n",
      "[244/300] train_loss: 0.06900 valid_loss: 0.08245 test_loss: 0.09283 \n",
      "[245/300] train_loss: 0.07133 valid_loss: 0.08601 test_loss: 0.09250 \n",
      "[246/300] train_loss: 0.06978 valid_loss: 0.08370 test_loss: 0.09278 \n",
      "[247/300] train_loss: 0.06961 valid_loss: 0.08206 test_loss: 0.09154 \n",
      "[248/300] train_loss: 0.07355 valid_loss: 0.08150 test_loss: 0.09211 \n",
      "[249/300] train_loss: 0.07084 valid_loss: 0.08289 test_loss: 0.09317 \n",
      "[250/300] train_loss: 0.07170 valid_loss: 0.08060 test_loss: 0.09202 \n",
      "[251/300] train_loss: 0.07034 valid_loss: 0.08189 test_loss: 0.09322 \n",
      "[252/300] train_loss: 0.06800 valid_loss: 0.08211 test_loss: 0.09253 \n",
      "[253/300] train_loss: 0.07016 valid_loss: 0.08250 test_loss: 0.09309 \n",
      "[254/300] train_loss: 0.07082 valid_loss: 0.08541 test_loss: 0.09276 \n",
      "[255/300] train_loss: 0.07191 valid_loss: 0.08578 test_loss: 0.09431 \n",
      "[256/300] train_loss: 0.06841 valid_loss: 0.08590 test_loss: 0.09182 \n",
      "[257/300] train_loss: 0.06916 valid_loss: 0.08288 test_loss: 0.09216 \n",
      "[258/300] train_loss: 0.07002 valid_loss: 0.08705 test_loss: 0.09378 \n",
      "[259/300] train_loss: 0.06997 valid_loss: 0.08948 test_loss: 0.09210 \n",
      "[260/300] train_loss: 0.07118 valid_loss: 0.08396 test_loss: 0.09262 \n",
      "[261/300] train_loss: 0.07247 valid_loss: 0.08356 test_loss: 0.09156 \n",
      "[262/300] train_loss: 0.06732 valid_loss: 0.09046 test_loss: 0.09296 \n",
      "[263/300] train_loss: 0.06972 valid_loss: 0.08827 test_loss: 0.09201 \n",
      "[264/300] train_loss: 0.06940 valid_loss: 0.08616 test_loss: 0.09235 \n",
      "[265/300] train_loss: 0.06944 valid_loss: 0.08834 test_loss: 0.09201 \n",
      "[266/300] train_loss: 0.06948 valid_loss: 0.09175 test_loss: 0.09207 \n",
      "[267/300] train_loss: 0.07093 valid_loss: 0.08513 test_loss: 0.09343 \n",
      "[268/300] train_loss: 0.06893 valid_loss: 0.08367 test_loss: 0.09139 \n",
      "[269/300] train_loss: 0.06832 valid_loss: 0.08517 test_loss: 0.09198 \n",
      "[270/300] train_loss: 0.06747 valid_loss: 0.08358 test_loss: 0.09205 \n",
      "[271/300] train_loss: 0.06948 valid_loss: 0.08663 test_loss: 0.09229 \n",
      "[272/300] train_loss: 0.06812 valid_loss: 0.09095 test_loss: 0.09215 \n",
      "[273/300] train_loss: 0.07039 valid_loss: 0.08246 test_loss: 0.09103 \n",
      "[274/300] train_loss: 0.07058 valid_loss: 0.09010 test_loss: 0.09136 \n",
      "[275/300] train_loss: 0.06735 valid_loss: 0.08141 test_loss: 0.09076 \n",
      "[276/300] train_loss: 0.06690 valid_loss: 0.08161 test_loss: 0.09214 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277/300] train_loss: 0.06927 valid_loss: 0.08067 test_loss: 0.09153 \n",
      "[278/300] train_loss: 0.07039 valid_loss: 0.08095 test_loss: 0.09112 \n",
      "[279/300] train_loss: 0.06961 valid_loss: 0.08298 test_loss: 0.09162 \n",
      "[280/300] train_loss: 0.06865 valid_loss: 0.08626 test_loss: 0.09147 \n",
      "[281/300] train_loss: 0.06904 valid_loss: 0.08526 test_loss: 0.09053 \n",
      "[282/300] train_loss: 0.06859 valid_loss: 0.09339 test_loss: 0.09083 \n",
      "[283/300] train_loss: 0.06737 valid_loss: 0.09080 test_loss: 0.09171 \n",
      "[284/300] train_loss: 0.06824 valid_loss: 0.08911 test_loss: 0.09118 \n",
      "[285/300] train_loss: 0.06910 valid_loss: 0.08170 test_loss: 0.09154 \n",
      "[286/300] train_loss: 0.06993 valid_loss: 0.08215 test_loss: 0.09115 \n",
      "[287/300] train_loss: 0.06896 valid_loss: 0.08140 test_loss: 0.09166 \n",
      "[288/300] train_loss: 0.06842 valid_loss: 0.08699 test_loss: 0.08978 \n",
      "[289/300] train_loss: 0.06774 valid_loss: 0.08374 test_loss: 0.09020 \n",
      "[290/300] train_loss: 0.06582 valid_loss: 0.09000 test_loss: 0.09078 \n",
      "[291/300] train_loss: 0.06646 valid_loss: 0.08519 test_loss: 0.08985 \n",
      "[292/300] train_loss: 0.06628 valid_loss: 0.08262 test_loss: 0.08950 \n",
      "[293/300] train_loss: 0.06728 valid_loss: 0.08513 test_loss: 0.09106 \n",
      "[294/300] train_loss: 0.06666 valid_loss: 0.09166 test_loss: 0.08962 \n",
      "[295/300] train_loss: 0.06813 valid_loss: 0.08543 test_loss: 0.08963 \n",
      "[296/300] train_loss: 0.06860 valid_loss: 0.08172 test_loss: 0.09062 \n",
      "[297/300] train_loss: 0.06692 valid_loss: 0.08479 test_loss: 0.08980 \n",
      "[298/300] train_loss: 0.06918 valid_loss: 0.08251 test_loss: 0.08914 \n",
      "[299/300] train_loss: 0.06725 valid_loss: 0.08328 test_loss: 0.08937 \n",
      "[300/300] train_loss: 0.06754 valid_loss: 0.08627 test_loss: 0.08982 \n",
      "TRAINING MODEL 11\n",
      "[  1/300] train_loss: 0.58881 valid_loss: 0.50145 test_loss: 0.50104 \n",
      "Validation loss decreased (inf --> 0.501447).  Saving model ...\n",
      "[  2/300] train_loss: 0.41617 valid_loss: 0.37084 test_loss: 0.37522 \n",
      "Validation loss decreased (0.501447 --> 0.370841).  Saving model ...\n",
      "[  3/300] train_loss: 0.32595 valid_loss: 0.31470 test_loss: 0.32658 \n",
      "Validation loss decreased (0.370841 --> 0.314696).  Saving model ...\n",
      "[  4/300] train_loss: 0.28114 valid_loss: 0.28050 test_loss: 0.29463 \n",
      "Validation loss decreased (0.314696 --> 0.280498).  Saving model ...\n",
      "[  5/300] train_loss: 0.24673 valid_loss: 0.24684 test_loss: 0.26200 \n",
      "Validation loss decreased (0.280498 --> 0.246839).  Saving model ...\n",
      "[  6/300] train_loss: 0.22191 valid_loss: 0.22406 test_loss: 0.23828 \n",
      "Validation loss decreased (0.246839 --> 0.224060).  Saving model ...\n",
      "[  7/300] train_loss: 0.20200 valid_loss: 0.20665 test_loss: 0.21756 \n",
      "Validation loss decreased (0.224060 --> 0.206648).  Saving model ...\n",
      "[  8/300] train_loss: 0.18730 valid_loss: 0.19162 test_loss: 0.20392 \n",
      "Validation loss decreased (0.206648 --> 0.191620).  Saving model ...\n",
      "[  9/300] train_loss: 0.17508 valid_loss: 0.18229 test_loss: 0.19223 \n",
      "Validation loss decreased (0.191620 --> 0.182286).  Saving model ...\n",
      "[ 10/300] train_loss: 0.16440 valid_loss: 0.17202 test_loss: 0.18335 \n",
      "Validation loss decreased (0.182286 --> 0.172019).  Saving model ...\n",
      "[ 11/300] train_loss: 0.15946 valid_loss: 0.16714 test_loss: 0.17657 \n",
      "Validation loss decreased (0.172019 --> 0.167144).  Saving model ...\n",
      "[ 12/300] train_loss: 0.15287 valid_loss: 0.16097 test_loss: 0.17035 \n",
      "Validation loss decreased (0.167144 --> 0.160971).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15007 valid_loss: 0.15863 test_loss: 0.16633 \n",
      "Validation loss decreased (0.160971 --> 0.158634).  Saving model ...\n",
      "[ 14/300] train_loss: 0.14457 valid_loss: 0.15431 test_loss: 0.16309 \n",
      "Validation loss decreased (0.158634 --> 0.154313).  Saving model ...\n",
      "[ 15/300] train_loss: 0.14107 valid_loss: 0.15135 test_loss: 0.15925 \n",
      "Validation loss decreased (0.154313 --> 0.151345).  Saving model ...\n",
      "[ 16/300] train_loss: 0.13372 valid_loss: 0.15105 test_loss: 0.15848 \n",
      "Validation loss decreased (0.151345 --> 0.151054).  Saving model ...\n",
      "[ 17/300] train_loss: 0.13505 valid_loss: 0.14460 test_loss: 0.15432 \n",
      "Validation loss decreased (0.151054 --> 0.144604).  Saving model ...\n",
      "[ 18/300] train_loss: 0.13069 valid_loss: 0.14418 test_loss: 0.15436 \n",
      "Validation loss decreased (0.144604 --> 0.144177).  Saving model ...\n",
      "[ 19/300] train_loss: 0.13127 valid_loss: 0.14444 test_loss: 0.15301 \n",
      "[ 20/300] train_loss: 0.13165 valid_loss: 0.13810 test_loss: 0.14952 \n",
      "Validation loss decreased (0.144177 --> 0.138098).  Saving model ...\n",
      "[ 21/300] train_loss: 0.12874 valid_loss: 0.13661 test_loss: 0.14851 \n",
      "Validation loss decreased (0.138098 --> 0.136611).  Saving model ...\n",
      "[ 22/300] train_loss: 0.12422 valid_loss: 0.13545 test_loss: 0.14586 \n",
      "Validation loss decreased (0.136611 --> 0.135449).  Saving model ...\n",
      "[ 23/300] train_loss: 0.12164 valid_loss: 0.13309 test_loss: 0.14529 \n",
      "Validation loss decreased (0.135449 --> 0.133094).  Saving model ...\n",
      "[ 24/300] train_loss: 0.11984 valid_loss: 0.13417 test_loss: 0.14375 \n",
      "[ 25/300] train_loss: 0.12227 valid_loss: 0.13227 test_loss: 0.14333 \n",
      "Validation loss decreased (0.133094 --> 0.132266).  Saving model ...\n",
      "[ 26/300] train_loss: 0.11712 valid_loss: 0.13006 test_loss: 0.14219 \n",
      "Validation loss decreased (0.132266 --> 0.130058).  Saving model ...\n",
      "[ 27/300] train_loss: 0.11881 valid_loss: 0.12741 test_loss: 0.14001 \n",
      "Validation loss decreased (0.130058 --> 0.127410).  Saving model ...\n",
      "[ 28/300] train_loss: 0.11881 valid_loss: 0.12781 test_loss: 0.13977 \n",
      "[ 29/300] train_loss: 0.11882 valid_loss: 0.12750 test_loss: 0.13973 \n",
      "[ 30/300] train_loss: 0.11710 valid_loss: 0.12249 test_loss: 0.13757 \n",
      "Validation loss decreased (0.127410 --> 0.122489).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11476 valid_loss: 0.12495 test_loss: 0.13875 \n",
      "[ 32/300] train_loss: 0.11757 valid_loss: 0.12521 test_loss: 0.13626 \n",
      "[ 33/300] train_loss: 0.11267 valid_loss: 0.12621 test_loss: 0.13516 \n",
      "[ 34/300] train_loss: 0.11578 valid_loss: 0.12700 test_loss: 0.13619 \n",
      "[ 35/300] train_loss: 0.11224 valid_loss: 0.12145 test_loss: 0.13388 \n",
      "Validation loss decreased (0.122489 --> 0.121452).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11043 valid_loss: 0.11978 test_loss: 0.13335 \n",
      "Validation loss decreased (0.121452 --> 0.119778).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11223 valid_loss: 0.12202 test_loss: 0.13499 \n",
      "[ 38/300] train_loss: 0.10786 valid_loss: 0.11988 test_loss: 0.13260 \n",
      "[ 39/300] train_loss: 0.10885 valid_loss: 0.11837 test_loss: 0.13185 \n",
      "Validation loss decreased (0.119778 --> 0.118369).  Saving model ...\n",
      "[ 40/300] train_loss: 0.10860 valid_loss: 0.11712 test_loss: 0.13119 \n",
      "Validation loss decreased (0.118369 --> 0.117116).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10752 valid_loss: 0.11762 test_loss: 0.13020 \n",
      "[ 42/300] train_loss: 0.10440 valid_loss: 0.11931 test_loss: 0.13096 \n",
      "[ 43/300] train_loss: 0.10821 valid_loss: 0.11545 test_loss: 0.12918 \n",
      "Validation loss decreased (0.117116 --> 0.115448).  Saving model ...\n",
      "[ 44/300] train_loss: 0.10433 valid_loss: 0.11788 test_loss: 0.13112 \n",
      "[ 45/300] train_loss: 0.10562 valid_loss: 0.11484 test_loss: 0.12824 \n",
      "Validation loss decreased (0.115448 --> 0.114840).  Saving model ...\n",
      "[ 46/300] train_loss: 0.10296 valid_loss: 0.11526 test_loss: 0.12705 \n",
      "[ 47/300] train_loss: 0.10381 valid_loss: 0.11438 test_loss: 0.12737 \n",
      "Validation loss decreased (0.114840 --> 0.114378).  Saving model ...\n",
      "[ 48/300] train_loss: 0.10310 valid_loss: 0.11120 test_loss: 0.12621 \n",
      "Validation loss decreased (0.114378 --> 0.111197).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10321 valid_loss: 0.11297 test_loss: 0.12806 \n",
      "[ 50/300] train_loss: 0.10128 valid_loss: 0.11056 test_loss: 0.12469 \n",
      "Validation loss decreased (0.111197 --> 0.110558).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10027 valid_loss: 0.10943 test_loss: 0.12405 \n",
      "Validation loss decreased (0.110558 --> 0.109432).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10067 valid_loss: 0.11330 test_loss: 0.12438 \n",
      "[ 53/300] train_loss: 0.10416 valid_loss: 0.11441 test_loss: 0.12724 \n",
      "[ 54/300] train_loss: 0.09988 valid_loss: 0.11027 test_loss: 0.12471 \n",
      "[ 55/300] train_loss: 0.10026 valid_loss: 0.11156 test_loss: 0.12248 \n",
      "[ 56/300] train_loss: 0.09752 valid_loss: 0.11153 test_loss: 0.12220 \n",
      "[ 57/300] train_loss: 0.09843 valid_loss: 0.11372 test_loss: 0.12287 \n",
      "[ 58/300] train_loss: 0.09402 valid_loss: 0.11021 test_loss: 0.12290 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 59/300] train_loss: 0.10114 valid_loss: 0.10864 test_loss: 0.12243 \n",
      "Validation loss decreased (0.109432 --> 0.108640).  Saving model ...\n",
      "[ 60/300] train_loss: 0.09719 valid_loss: 0.10626 test_loss: 0.12086 \n",
      "Validation loss decreased (0.108640 --> 0.106259).  Saving model ...\n",
      "[ 61/300] train_loss: 0.09459 valid_loss: 0.10802 test_loss: 0.11990 \n",
      "[ 62/300] train_loss: 0.09839 valid_loss: 0.10797 test_loss: 0.11908 \n",
      "[ 63/300] train_loss: 0.10014 valid_loss: 0.10613 test_loss: 0.11895 \n",
      "Validation loss decreased (0.106259 --> 0.106129).  Saving model ...\n",
      "[ 64/300] train_loss: 0.09725 valid_loss: 0.10357 test_loss: 0.11947 \n",
      "Validation loss decreased (0.106129 --> 0.103565).  Saving model ...\n",
      "[ 65/300] train_loss: 0.09325 valid_loss: 0.10872 test_loss: 0.12050 \n",
      "[ 66/300] train_loss: 0.09486 valid_loss: 0.11212 test_loss: 0.12022 \n",
      "[ 67/300] train_loss: 0.09308 valid_loss: 0.10822 test_loss: 0.11915 \n",
      "[ 68/300] train_loss: 0.09494 valid_loss: 0.10386 test_loss: 0.11718 \n",
      "[ 69/300] train_loss: 0.09398 valid_loss: 0.10536 test_loss: 0.11729 \n",
      "[ 70/300] train_loss: 0.09258 valid_loss: 0.10395 test_loss: 0.11822 \n",
      "[ 71/300] train_loss: 0.09364 valid_loss: 0.10971 test_loss: 0.11753 \n",
      "[ 72/300] train_loss: 0.09349 valid_loss: 0.10257 test_loss: 0.11672 \n",
      "Validation loss decreased (0.103565 --> 0.102575).  Saving model ...\n",
      "[ 73/300] train_loss: 0.09300 valid_loss: 0.10253 test_loss: 0.11517 \n",
      "Validation loss decreased (0.102575 --> 0.102529).  Saving model ...\n",
      "[ 74/300] train_loss: 0.09279 valid_loss: 0.10309 test_loss: 0.11636 \n",
      "[ 75/300] train_loss: 0.09389 valid_loss: 0.10280 test_loss: 0.11491 \n",
      "[ 76/300] train_loss: 0.09320 valid_loss: 0.10027 test_loss: 0.11290 \n",
      "Validation loss decreased (0.102529 --> 0.100266).  Saving model ...\n",
      "[ 77/300] train_loss: 0.09215 valid_loss: 0.10056 test_loss: 0.11516 \n",
      "[ 78/300] train_loss: 0.09241 valid_loss: 0.10093 test_loss: 0.11356 \n",
      "[ 79/300] train_loss: 0.09098 valid_loss: 0.10360 test_loss: 0.11437 \n",
      "[ 80/300] train_loss: 0.09228 valid_loss: 0.10221 test_loss: 0.11479 \n",
      "[ 81/300] train_loss: 0.09013 valid_loss: 0.10060 test_loss: 0.11346 \n",
      "[ 82/300] train_loss: 0.08942 valid_loss: 0.10097 test_loss: 0.11357 \n",
      "[ 83/300] train_loss: 0.08948 valid_loss: 0.10526 test_loss: 0.11311 \n",
      "[ 84/300] train_loss: 0.09032 valid_loss: 0.10296 test_loss: 0.11283 \n",
      "[ 85/300] train_loss: 0.08950 valid_loss: 0.10278 test_loss: 0.11250 \n",
      "[ 86/300] train_loss: 0.09068 valid_loss: 0.10054 test_loss: 0.11410 \n",
      "[ 87/300] train_loss: 0.08653 valid_loss: 0.09776 test_loss: 0.11176 \n",
      "Validation loss decreased (0.100266 --> 0.097762).  Saving model ...\n",
      "[ 88/300] train_loss: 0.09098 valid_loss: 0.09835 test_loss: 0.11107 \n",
      "[ 89/300] train_loss: 0.08983 valid_loss: 0.09991 test_loss: 0.11151 \n",
      "[ 90/300] train_loss: 0.08952 valid_loss: 0.09756 test_loss: 0.11089 \n",
      "Validation loss decreased (0.097762 --> 0.097564).  Saving model ...\n",
      "[ 91/300] train_loss: 0.09087 valid_loss: 0.09996 test_loss: 0.11017 \n",
      "[ 92/300] train_loss: 0.08701 valid_loss: 0.10527 test_loss: 0.11198 \n",
      "[ 93/300] train_loss: 0.08851 valid_loss: 0.10338 test_loss: 0.11027 \n",
      "[ 94/300] train_loss: 0.08745 valid_loss: 0.10105 test_loss: 0.11038 \n",
      "[ 95/300] train_loss: 0.08786 valid_loss: 0.09651 test_loss: 0.11001 \n",
      "Validation loss decreased (0.097564 --> 0.096510).  Saving model ...\n",
      "[ 96/300] train_loss: 0.08793 valid_loss: 0.10125 test_loss: 0.10883 \n",
      "[ 97/300] train_loss: 0.08544 valid_loss: 0.09826 test_loss: 0.10986 \n",
      "[ 98/300] train_loss: 0.08751 valid_loss: 0.10073 test_loss: 0.10957 \n",
      "[ 99/300] train_loss: 0.08560 valid_loss: 0.09775 test_loss: 0.10827 \n",
      "[100/300] train_loss: 0.08706 valid_loss: 0.09732 test_loss: 0.10937 \n",
      "[101/300] train_loss: 0.08524 valid_loss: 0.09615 test_loss: 0.10933 \n",
      "Validation loss decreased (0.096510 --> 0.096146).  Saving model ...\n",
      "[102/300] train_loss: 0.08469 valid_loss: 0.09843 test_loss: 0.10884 \n",
      "[103/300] train_loss: 0.08605 valid_loss: 0.09745 test_loss: 0.10859 \n",
      "[104/300] train_loss: 0.08533 valid_loss: 0.10126 test_loss: 0.10859 \n",
      "[105/300] train_loss: 0.08593 valid_loss: 0.10842 test_loss: 0.10865 \n",
      "[106/300] train_loss: 0.08658 valid_loss: 0.09750 test_loss: 0.10709 \n",
      "[107/300] train_loss: 0.08558 valid_loss: 0.09486 test_loss: 0.10742 \n",
      "Validation loss decreased (0.096146 --> 0.094856).  Saving model ...\n",
      "[108/300] train_loss: 0.08444 valid_loss: 0.09495 test_loss: 0.10657 \n",
      "[109/300] train_loss: 0.08231 valid_loss: 0.09330 test_loss: 0.10659 \n",
      "Validation loss decreased (0.094856 --> 0.093299).  Saving model ...\n",
      "[110/300] train_loss: 0.08801 valid_loss: 0.09474 test_loss: 0.10910 \n",
      "[111/300] train_loss: 0.08576 valid_loss: 0.09603 test_loss: 0.10800 \n",
      "[112/300] train_loss: 0.08654 valid_loss: 0.09604 test_loss: 0.10867 \n",
      "[113/300] train_loss: 0.08694 valid_loss: 0.09225 test_loss: 0.10554 \n",
      "Validation loss decreased (0.093299 --> 0.092248).  Saving model ...\n",
      "[114/300] train_loss: 0.08318 valid_loss: 0.09930 test_loss: 0.10653 \n",
      "[115/300] train_loss: 0.08638 valid_loss: 0.09652 test_loss: 0.10617 \n",
      "[116/300] train_loss: 0.08277 valid_loss: 0.09592 test_loss: 0.10628 \n",
      "[117/300] train_loss: 0.08271 valid_loss: 0.09508 test_loss: 0.10669 \n",
      "[118/300] train_loss: 0.08249 valid_loss: 0.09464 test_loss: 0.10681 \n",
      "[119/300] train_loss: 0.08312 valid_loss: 0.09416 test_loss: 0.10493 \n",
      "[120/300] train_loss: 0.08218 valid_loss: 0.09336 test_loss: 0.10451 \n",
      "[121/300] train_loss: 0.08204 valid_loss: 0.09580 test_loss: 0.10511 \n",
      "[122/300] train_loss: 0.08296 valid_loss: 0.09587 test_loss: 0.10448 \n",
      "[123/300] train_loss: 0.08217 valid_loss: 0.09228 test_loss: 0.10393 \n",
      "[124/300] train_loss: 0.08352 valid_loss: 0.09169 test_loss: 0.10446 \n",
      "Validation loss decreased (0.092248 --> 0.091687).  Saving model ...\n",
      "[125/300] train_loss: 0.08212 valid_loss: 0.09440 test_loss: 0.10531 \n",
      "[126/300] train_loss: 0.08384 valid_loss: 0.09160 test_loss: 0.10425 \n",
      "Validation loss decreased (0.091687 --> 0.091603).  Saving model ...\n",
      "[127/300] train_loss: 0.08182 valid_loss: 0.09436 test_loss: 0.10408 \n",
      "[128/300] train_loss: 0.08311 valid_loss: 0.09071 test_loss: 0.10362 \n",
      "Validation loss decreased (0.091603 --> 0.090709).  Saving model ...\n",
      "[129/300] train_loss: 0.08273 valid_loss: 0.09336 test_loss: 0.10346 \n",
      "[130/300] train_loss: 0.08356 valid_loss: 0.09258 test_loss: 0.10440 \n",
      "[131/300] train_loss: 0.08253 valid_loss: 0.09266 test_loss: 0.10378 \n",
      "[132/300] train_loss: 0.08219 valid_loss: 0.09354 test_loss: 0.10530 \n",
      "[133/300] train_loss: 0.08096 valid_loss: 0.09094 test_loss: 0.10336 \n",
      "[134/300] train_loss: 0.08255 valid_loss: 0.09137 test_loss: 0.10351 \n",
      "[135/300] train_loss: 0.08020 valid_loss: 0.09525 test_loss: 0.10309 \n",
      "[136/300] train_loss: 0.08348 valid_loss: 0.09537 test_loss: 0.10451 \n",
      "[137/300] train_loss: 0.08043 valid_loss: 0.09872 test_loss: 0.10286 \n",
      "[138/300] train_loss: 0.07915 valid_loss: 0.09145 test_loss: 0.10263 \n",
      "[139/300] train_loss: 0.07803 valid_loss: 0.09643 test_loss: 0.10322 \n",
      "[140/300] train_loss: 0.08240 valid_loss: 0.09257 test_loss: 0.10233 \n",
      "[141/300] train_loss: 0.08218 valid_loss: 0.09549 test_loss: 0.10371 \n",
      "[142/300] train_loss: 0.07856 valid_loss: 0.09213 test_loss: 0.10174 \n",
      "[143/300] train_loss: 0.07937 valid_loss: 0.09317 test_loss: 0.10264 \n",
      "[144/300] train_loss: 0.07921 valid_loss: 0.09602 test_loss: 0.10368 \n",
      "[145/300] train_loss: 0.08006 valid_loss: 0.09159 test_loss: 0.10151 \n",
      "[146/300] train_loss: 0.07972 valid_loss: 0.08875 test_loss: 0.10261 \n",
      "Validation loss decreased (0.090709 --> 0.088748).  Saving model ...\n",
      "[147/300] train_loss: 0.08092 valid_loss: 0.09285 test_loss: 0.10241 \n",
      "[148/300] train_loss: 0.08093 valid_loss: 0.09883 test_loss: 0.10247 \n",
      "[149/300] train_loss: 0.07839 valid_loss: 0.09265 test_loss: 0.10318 \n",
      "[150/300] train_loss: 0.08033 valid_loss: 0.09123 test_loss: 0.10181 \n",
      "[151/300] train_loss: 0.07989 valid_loss: 0.09460 test_loss: 0.10082 \n",
      "[152/300] train_loss: 0.08092 valid_loss: 0.09052 test_loss: 0.10299 \n",
      "[153/300] train_loss: 0.07903 valid_loss: 0.08995 test_loss: 0.10117 \n",
      "[154/300] train_loss: 0.07991 valid_loss: 0.09418 test_loss: 0.10106 \n",
      "[155/300] train_loss: 0.07845 valid_loss: 0.09116 test_loss: 0.10182 \n",
      "[156/300] train_loss: 0.08283 valid_loss: 0.09055 test_loss: 0.10093 \n",
      "[157/300] train_loss: 0.07741 valid_loss: 0.09901 test_loss: 0.10209 \n",
      "[158/300] train_loss: 0.07736 valid_loss: 0.09605 test_loss: 0.10215 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159/300] train_loss: 0.07844 valid_loss: 0.09041 test_loss: 0.10035 \n",
      "[160/300] train_loss: 0.07744 valid_loss: 0.08966 test_loss: 0.10125 \n",
      "[161/300] train_loss: 0.07919 valid_loss: 0.08712 test_loss: 0.10139 \n",
      "Validation loss decreased (0.088748 --> 0.087122).  Saving model ...\n",
      "[162/300] train_loss: 0.07927 valid_loss: 0.09671 test_loss: 0.10189 \n",
      "[163/300] train_loss: 0.07904 valid_loss: 0.09053 test_loss: 0.10062 \n",
      "[164/300] train_loss: 0.07836 valid_loss: 0.08791 test_loss: 0.10055 \n",
      "[165/300] train_loss: 0.07725 valid_loss: 0.08937 test_loss: 0.10095 \n",
      "[166/300] train_loss: 0.07687 valid_loss: 0.08867 test_loss: 0.10148 \n",
      "[167/300] train_loss: 0.07895 valid_loss: 0.09793 test_loss: 0.10012 \n",
      "[168/300] train_loss: 0.07539 valid_loss: 0.09065 test_loss: 0.09956 \n",
      "[169/300] train_loss: 0.07609 valid_loss: 0.09174 test_loss: 0.09967 \n",
      "[170/300] train_loss: 0.07589 valid_loss: 0.08761 test_loss: 0.09917 \n",
      "[171/300] train_loss: 0.07677 valid_loss: 0.09063 test_loss: 0.09913 \n",
      "[172/300] train_loss: 0.07718 valid_loss: 0.09659 test_loss: 0.09977 \n",
      "[173/300] train_loss: 0.07649 valid_loss: 0.09347 test_loss: 0.09996 \n",
      "[174/300] train_loss: 0.07533 valid_loss: 0.08696 test_loss: 0.09869 \n",
      "Validation loss decreased (0.087122 --> 0.086961).  Saving model ...\n",
      "[175/300] train_loss: 0.07766 valid_loss: 0.08645 test_loss: 0.09941 \n",
      "Validation loss decreased (0.086961 --> 0.086448).  Saving model ...\n",
      "[176/300] train_loss: 0.07629 valid_loss: 0.08752 test_loss: 0.09992 \n",
      "[177/300] train_loss: 0.07686 valid_loss: 0.08783 test_loss: 0.09943 \n",
      "[178/300] train_loss: 0.07661 valid_loss: 0.08691 test_loss: 0.09827 \n",
      "[179/300] train_loss: 0.07511 valid_loss: 0.08691 test_loss: 0.09913 \n",
      "[180/300] train_loss: 0.07728 valid_loss: 0.08827 test_loss: 0.09806 \n",
      "[181/300] train_loss: 0.07679 valid_loss: 0.09079 test_loss: 0.09881 \n",
      "[182/300] train_loss: 0.07729 valid_loss: 0.09063 test_loss: 0.09854 \n",
      "[183/300] train_loss: 0.07529 valid_loss: 0.08481 test_loss: 0.09734 \n",
      "Validation loss decreased (0.086448 --> 0.084811).  Saving model ...\n",
      "[184/300] train_loss: 0.07645 valid_loss: 0.08660 test_loss: 0.09820 \n",
      "[185/300] train_loss: 0.07766 valid_loss: 0.08737 test_loss: 0.09757 \n",
      "[186/300] train_loss: 0.07385 valid_loss: 0.09290 test_loss: 0.09807 \n",
      "[187/300] train_loss: 0.07795 valid_loss: 0.09226 test_loss: 0.09744 \n",
      "[188/300] train_loss: 0.07670 valid_loss: 0.09202 test_loss: 0.09871 \n",
      "[189/300] train_loss: 0.07489 valid_loss: 0.09077 test_loss: 0.09888 \n",
      "[190/300] train_loss: 0.07623 valid_loss: 0.08663 test_loss: 0.09832 \n",
      "[191/300] train_loss: 0.07572 valid_loss: 0.09817 test_loss: 0.09895 \n",
      "[192/300] train_loss: 0.07551 valid_loss: 0.08627 test_loss: 0.09824 \n",
      "[193/300] train_loss: 0.07644 valid_loss: 0.09068 test_loss: 0.09745 \n",
      "[194/300] train_loss: 0.07545 valid_loss: 0.09124 test_loss: 0.09806 \n",
      "[195/300] train_loss: 0.07700 valid_loss: 0.08841 test_loss: 0.09655 \n",
      "[196/300] train_loss: 0.07467 valid_loss: 0.08628 test_loss: 0.09779 \n",
      "[197/300] train_loss: 0.07523 valid_loss: 0.08688 test_loss: 0.09660 \n",
      "[198/300] train_loss: 0.07650 valid_loss: 0.08922 test_loss: 0.09914 \n",
      "[199/300] train_loss: 0.07499 valid_loss: 0.09731 test_loss: 0.09764 \n",
      "[200/300] train_loss: 0.07297 valid_loss: 0.08674 test_loss: 0.09684 \n",
      "[201/300] train_loss: 0.07410 valid_loss: 0.08494 test_loss: 0.09879 \n",
      "[202/300] train_loss: 0.07612 valid_loss: 0.08588 test_loss: 0.09603 \n",
      "[203/300] train_loss: 0.07543 valid_loss: 0.08763 test_loss: 0.09685 \n",
      "[204/300] train_loss: 0.07668 valid_loss: 0.08558 test_loss: 0.09599 \n",
      "[205/300] train_loss: 0.07500 valid_loss: 0.09169 test_loss: 0.09660 \n",
      "[206/300] train_loss: 0.07236 valid_loss: 0.08985 test_loss: 0.09838 \n",
      "[207/300] train_loss: 0.07418 valid_loss: 0.08384 test_loss: 0.09644 \n",
      "Validation loss decreased (0.084811 --> 0.083844).  Saving model ...\n",
      "[208/300] train_loss: 0.07501 valid_loss: 0.08537 test_loss: 0.09627 \n",
      "[209/300] train_loss: 0.07378 valid_loss: 0.08634 test_loss: 0.09727 \n",
      "[210/300] train_loss: 0.07365 valid_loss: 0.08744 test_loss: 0.09695 \n",
      "[211/300] train_loss: 0.07534 valid_loss: 0.09199 test_loss: 0.09601 \n",
      "[212/300] train_loss: 0.07333 valid_loss: 0.09109 test_loss: 0.09650 \n",
      "[213/300] train_loss: 0.07363 valid_loss: 0.08846 test_loss: 0.09608 \n",
      "[214/300] train_loss: 0.07454 valid_loss: 0.08480 test_loss: 0.09635 \n",
      "[215/300] train_loss: 0.07431 valid_loss: 0.08791 test_loss: 0.09750 \n",
      "[216/300] train_loss: 0.07348 valid_loss: 0.08549 test_loss: 0.09814 \n",
      "[217/300] train_loss: 0.07591 valid_loss: 0.08414 test_loss: 0.09606 \n",
      "[218/300] train_loss: 0.07311 valid_loss: 0.08660 test_loss: 0.09557 \n",
      "[219/300] train_loss: 0.07223 valid_loss: 0.08568 test_loss: 0.09606 \n",
      "[220/300] train_loss: 0.07283 valid_loss: 0.08555 test_loss: 0.09722 \n",
      "[221/300] train_loss: 0.07123 valid_loss: 0.08391 test_loss: 0.09579 \n",
      "[222/300] train_loss: 0.07521 valid_loss: 0.08554 test_loss: 0.09496 \n",
      "[223/300] train_loss: 0.07204 valid_loss: 0.08598 test_loss: 0.09472 \n",
      "[224/300] train_loss: 0.07397 valid_loss: 0.08480 test_loss: 0.09436 \n",
      "[225/300] train_loss: 0.07268 valid_loss: 0.08493 test_loss: 0.09617 \n",
      "[226/300] train_loss: 0.07338 valid_loss: 0.08388 test_loss: 0.09641 \n",
      "[227/300] train_loss: 0.07124 valid_loss: 0.08583 test_loss: 0.09510 \n",
      "[228/300] train_loss: 0.07226 valid_loss: 0.09026 test_loss: 0.09471 \n",
      "[229/300] train_loss: 0.07235 valid_loss: 0.08634 test_loss: 0.09543 \n",
      "[230/300] train_loss: 0.07037 valid_loss: 0.08381 test_loss: 0.09578 \n",
      "Validation loss decreased (0.083844 --> 0.083809).  Saving model ...\n",
      "[231/300] train_loss: 0.07248 valid_loss: 0.08193 test_loss: 0.09453 \n",
      "Validation loss decreased (0.083809 --> 0.081926).  Saving model ...\n",
      "[232/300] train_loss: 0.07191 valid_loss: 0.08359 test_loss: 0.09398 \n",
      "[233/300] train_loss: 0.07115 valid_loss: 0.08573 test_loss: 0.09510 \n",
      "[234/300] train_loss: 0.07107 valid_loss: 0.08585 test_loss: 0.09533 \n",
      "[235/300] train_loss: 0.07228 valid_loss: 0.08266 test_loss: 0.09324 \n",
      "[236/300] train_loss: 0.07198 valid_loss: 0.08415 test_loss: 0.09480 \n",
      "[237/300] train_loss: 0.06949 valid_loss: 0.08318 test_loss: 0.09466 \n",
      "[238/300] train_loss: 0.07168 valid_loss: 0.08215 test_loss: 0.09378 \n",
      "[239/300] train_loss: 0.07234 valid_loss: 0.08547 test_loss: 0.09371 \n",
      "[240/300] train_loss: 0.07359 valid_loss: 0.08414 test_loss: 0.09459 \n",
      "[241/300] train_loss: 0.07027 valid_loss: 0.08176 test_loss: 0.09504 \n",
      "Validation loss decreased (0.081926 --> 0.081764).  Saving model ...\n",
      "[242/300] train_loss: 0.06992 valid_loss: 0.08201 test_loss: 0.09426 \n",
      "[243/300] train_loss: 0.07296 valid_loss: 0.08216 test_loss: 0.09438 \n",
      "[244/300] train_loss: 0.07151 valid_loss: 0.08542 test_loss: 0.09413 \n",
      "[245/300] train_loss: 0.07439 valid_loss: 0.08600 test_loss: 0.09320 \n",
      "[246/300] train_loss: 0.07285 valid_loss: 0.08224 test_loss: 0.09456 \n",
      "[247/300] train_loss: 0.07076 valid_loss: 0.08605 test_loss: 0.09423 \n",
      "[248/300] train_loss: 0.07120 valid_loss: 0.09561 test_loss: 0.09512 \n",
      "[249/300] train_loss: 0.06968 valid_loss: 0.09239 test_loss: 0.09351 \n",
      "[250/300] train_loss: 0.07006 valid_loss: 0.08473 test_loss: 0.09353 \n",
      "[251/300] train_loss: 0.07108 valid_loss: 0.09105 test_loss: 0.09399 \n",
      "[252/300] train_loss: 0.06843 valid_loss: 0.08884 test_loss: 0.09388 \n",
      "[253/300] train_loss: 0.07053 valid_loss: 0.08848 test_loss: 0.09288 \n",
      "[254/300] train_loss: 0.07056 valid_loss: 0.08588 test_loss: 0.09301 \n",
      "[255/300] train_loss: 0.06792 valid_loss: 0.08267 test_loss: 0.09219 \n",
      "[256/300] train_loss: 0.07011 valid_loss: 0.08212 test_loss: 0.09320 \n",
      "[257/300] train_loss: 0.07114 valid_loss: 0.08076 test_loss: 0.09240 \n",
      "Validation loss decreased (0.081764 --> 0.080759).  Saving model ...\n",
      "[258/300] train_loss: 0.06951 valid_loss: 0.08209 test_loss: 0.09233 \n",
      "[259/300] train_loss: 0.06990 valid_loss: 0.08190 test_loss: 0.09156 \n",
      "[260/300] train_loss: 0.07026 valid_loss: 0.08116 test_loss: 0.09253 \n",
      "[261/300] train_loss: 0.07069 valid_loss: 0.08220 test_loss: 0.09203 \n",
      "[262/300] train_loss: 0.06908 valid_loss: 0.08322 test_loss: 0.09252 \n",
      "[263/300] train_loss: 0.07029 valid_loss: 0.08046 test_loss: 0.09253 \n",
      "Validation loss decreased (0.080759 --> 0.080461).  Saving model ...\n",
      "[264/300] train_loss: 0.06947 valid_loss: 0.08167 test_loss: 0.09265 \n",
      "[265/300] train_loss: 0.06576 valid_loss: 0.08214 test_loss: 0.09341 \n",
      "[266/300] train_loss: 0.06828 valid_loss: 0.07992 test_loss: 0.09200 \n",
      "Validation loss decreased (0.080461 --> 0.079923).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[267/300] train_loss: 0.06539 valid_loss: 0.08374 test_loss: 0.09251 \n",
      "[268/300] train_loss: 0.07137 valid_loss: 0.08264 test_loss: 0.09356 \n",
      "[269/300] train_loss: 0.06815 valid_loss: 0.09015 test_loss: 0.09355 \n",
      "[270/300] train_loss: 0.06707 valid_loss: 0.08481 test_loss: 0.09306 \n",
      "[271/300] train_loss: 0.06915 valid_loss: 0.08415 test_loss: 0.09203 \n",
      "[272/300] train_loss: 0.06909 valid_loss: 0.08409 test_loss: 0.09219 \n",
      "[273/300] train_loss: 0.06987 valid_loss: 0.08550 test_loss: 0.09142 \n",
      "[274/300] train_loss: 0.06930 valid_loss: 0.08528 test_loss: 0.09122 \n",
      "[275/300] train_loss: 0.06755 valid_loss: 0.07986 test_loss: 0.09145 \n",
      "Validation loss decreased (0.079923 --> 0.079861).  Saving model ...\n",
      "[276/300] train_loss: 0.06901 valid_loss: 0.08326 test_loss: 0.09253 \n",
      "[277/300] train_loss: 0.06947 valid_loss: 0.07983 test_loss: 0.09136 \n",
      "Validation loss decreased (0.079861 --> 0.079831).  Saving model ...\n",
      "[278/300] train_loss: 0.06775 valid_loss: 0.08125 test_loss: 0.09195 \n",
      "[279/300] train_loss: 0.06664 valid_loss: 0.08103 test_loss: 0.09191 \n",
      "[280/300] train_loss: 0.06859 valid_loss: 0.08134 test_loss: 0.09096 \n",
      "[281/300] train_loss: 0.07204 valid_loss: 0.08081 test_loss: 0.09112 \n",
      "[282/300] train_loss: 0.06957 valid_loss: 0.08236 test_loss: 0.09120 \n",
      "[283/300] train_loss: 0.07006 valid_loss: 0.09959 test_loss: 0.09105 \n",
      "[284/300] train_loss: 0.07084 valid_loss: 0.08724 test_loss: 0.09208 \n",
      "[285/300] train_loss: 0.06990 valid_loss: 0.08046 test_loss: 0.09190 \n",
      "[286/300] train_loss: 0.06868 valid_loss: 0.08190 test_loss: 0.09241 \n",
      "[287/300] train_loss: 0.06590 valid_loss: 0.08187 test_loss: 0.09139 \n",
      "[288/300] train_loss: 0.06779 valid_loss: 0.07993 test_loss: 0.09146 \n",
      "[289/300] train_loss: 0.06683 valid_loss: 0.07948 test_loss: 0.09153 \n",
      "Validation loss decreased (0.079831 --> 0.079476).  Saving model ...\n",
      "[290/300] train_loss: 0.06853 valid_loss: 0.07896 test_loss: 0.09120 \n",
      "Validation loss decreased (0.079476 --> 0.078965).  Saving model ...\n",
      "[291/300] train_loss: 0.06776 valid_loss: 0.08049 test_loss: 0.09166 \n",
      "[292/300] train_loss: 0.06661 valid_loss: 0.08541 test_loss: 0.09233 \n",
      "[293/300] train_loss: 0.06666 valid_loss: 0.08374 test_loss: 0.09157 \n",
      "[294/300] train_loss: 0.06603 valid_loss: 0.08432 test_loss: 0.09185 \n",
      "[295/300] train_loss: 0.06858 valid_loss: 0.08292 test_loss: 0.09129 \n",
      "[296/300] train_loss: 0.06863 valid_loss: 0.08750 test_loss: 0.09177 \n",
      "[297/300] train_loss: 0.06862 valid_loss: 0.08667 test_loss: 0.09161 \n",
      "[298/300] train_loss: 0.06675 valid_loss: 0.08318 test_loss: 0.09237 \n",
      "[299/300] train_loss: 0.06703 valid_loss: 0.08387 test_loss: 0.09122 \n",
      "[300/300] train_loss: 0.06644 valid_loss: 0.08962 test_loss: 0.09139 \n",
      "TRAINING MODEL 12\n",
      "[  1/300] train_loss: 0.70329 valid_loss: 0.64348 test_loss: 0.63219 \n",
      "Validation loss decreased (inf --> 0.643477).  Saving model ...\n",
      "[  2/300] train_loss: 0.56177 valid_loss: 0.49183 test_loss: 0.47915 \n",
      "Validation loss decreased (0.643477 --> 0.491832).  Saving model ...\n",
      "[  3/300] train_loss: 0.42977 valid_loss: 0.39958 test_loss: 0.39419 \n",
      "Validation loss decreased (0.491832 --> 0.399578).  Saving model ...\n",
      "[  4/300] train_loss: 0.35256 valid_loss: 0.34644 test_loss: 0.34573 \n",
      "Validation loss decreased (0.399578 --> 0.346437).  Saving model ...\n",
      "[  5/300] train_loss: 0.30704 valid_loss: 0.30444 test_loss: 0.31083 \n",
      "Validation loss decreased (0.346437 --> 0.304443).  Saving model ...\n",
      "[  6/300] train_loss: 0.27463 valid_loss: 0.27152 test_loss: 0.28416 \n",
      "Validation loss decreased (0.304443 --> 0.271519).  Saving model ...\n",
      "[  7/300] train_loss: 0.24826 valid_loss: 0.25158 test_loss: 0.26544 \n",
      "Validation loss decreased (0.271519 --> 0.251579).  Saving model ...\n",
      "[  8/300] train_loss: 0.22315 valid_loss: 0.22823 test_loss: 0.24284 \n",
      "Validation loss decreased (0.251579 --> 0.228232).  Saving model ...\n",
      "[  9/300] train_loss: 0.20710 valid_loss: 0.21143 test_loss: 0.22843 \n",
      "Validation loss decreased (0.228232 --> 0.211435).  Saving model ...\n",
      "[ 10/300] train_loss: 0.19470 valid_loss: 0.19573 test_loss: 0.21014 \n",
      "Validation loss decreased (0.211435 --> 0.195731).  Saving model ...\n",
      "[ 11/300] train_loss: 0.18123 valid_loss: 0.18634 test_loss: 0.19766 \n",
      "Validation loss decreased (0.195731 --> 0.186336).  Saving model ...\n",
      "[ 12/300] train_loss: 0.17065 valid_loss: 0.17556 test_loss: 0.18778 \n",
      "Validation loss decreased (0.186336 --> 0.175558).  Saving model ...\n",
      "[ 13/300] train_loss: 0.16623 valid_loss: 0.16801 test_loss: 0.18226 \n",
      "Validation loss decreased (0.175558 --> 0.168012).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15812 valid_loss: 0.16174 test_loss: 0.17478 \n",
      "Validation loss decreased (0.168012 --> 0.161736).  Saving model ...\n",
      "[ 15/300] train_loss: 0.15185 valid_loss: 0.16259 test_loss: 0.17348 \n",
      "[ 16/300] train_loss: 0.15105 valid_loss: 0.15761 test_loss: 0.16842 \n",
      "Validation loss decreased (0.161736 --> 0.157610).  Saving model ...\n",
      "[ 17/300] train_loss: 0.14981 valid_loss: 0.15528 test_loss: 0.16737 \n",
      "Validation loss decreased (0.157610 --> 0.155280).  Saving model ...\n",
      "[ 18/300] train_loss: 0.14594 valid_loss: 0.15625 test_loss: 0.16431 \n",
      "[ 19/300] train_loss: 0.14278 valid_loss: 0.15086 test_loss: 0.16210 \n",
      "Validation loss decreased (0.155280 --> 0.150857).  Saving model ...\n",
      "[ 20/300] train_loss: 0.14047 valid_loss: 0.14739 test_loss: 0.15974 \n",
      "Validation loss decreased (0.150857 --> 0.147394).  Saving model ...\n",
      "[ 21/300] train_loss: 0.13824 valid_loss: 0.14507 test_loss: 0.15661 \n",
      "Validation loss decreased (0.147394 --> 0.145073).  Saving model ...\n",
      "[ 22/300] train_loss: 0.13390 valid_loss: 0.14270 test_loss: 0.15326 \n",
      "Validation loss decreased (0.145073 --> 0.142700).  Saving model ...\n",
      "[ 23/300] train_loss: 0.13028 valid_loss: 0.14042 test_loss: 0.15332 \n",
      "Validation loss decreased (0.142700 --> 0.140422).  Saving model ...\n",
      "[ 24/300] train_loss: 0.13008 valid_loss: 0.13709 test_loss: 0.15060 \n",
      "Validation loss decreased (0.140422 --> 0.137088).  Saving model ...\n",
      "[ 25/300] train_loss: 0.13292 valid_loss: 0.13620 test_loss: 0.14714 \n",
      "Validation loss decreased (0.137088 --> 0.136202).  Saving model ...\n",
      "[ 26/300] train_loss: 0.12717 valid_loss: 0.13710 test_loss: 0.14914 \n",
      "[ 27/300] train_loss: 0.12764 valid_loss: 0.13610 test_loss: 0.14838 \n",
      "Validation loss decreased (0.136202 --> 0.136100).  Saving model ...\n",
      "[ 28/300] train_loss: 0.12289 valid_loss: 0.13642 test_loss: 0.14690 \n",
      "[ 29/300] train_loss: 0.12346 valid_loss: 0.12962 test_loss: 0.14246 \n",
      "Validation loss decreased (0.136100 --> 0.129622).  Saving model ...\n",
      "[ 30/300] train_loss: 0.12046 valid_loss: 0.13245 test_loss: 0.14424 \n",
      "[ 31/300] train_loss: 0.11923 valid_loss: 0.13240 test_loss: 0.14381 \n",
      "[ 32/300] train_loss: 0.12067 valid_loss: 0.13022 test_loss: 0.14306 \n",
      "[ 33/300] train_loss: 0.11933 valid_loss: 0.12931 test_loss: 0.14012 \n",
      "Validation loss decreased (0.129622 --> 0.129310).  Saving model ...\n",
      "[ 34/300] train_loss: 0.11634 valid_loss: 0.12809 test_loss: 0.13989 \n",
      "Validation loss decreased (0.129310 --> 0.128087).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11748 valid_loss: 0.12471 test_loss: 0.13890 \n",
      "Validation loss decreased (0.128087 --> 0.124709).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11622 valid_loss: 0.12411 test_loss: 0.13625 \n",
      "Validation loss decreased (0.124709 --> 0.124108).  Saving model ...\n",
      "[ 37/300] train_loss: 0.11275 valid_loss: 0.12251 test_loss: 0.13660 \n",
      "Validation loss decreased (0.124108 --> 0.122508).  Saving model ...\n",
      "[ 38/300] train_loss: 0.11688 valid_loss: 0.12132 test_loss: 0.13516 \n",
      "Validation loss decreased (0.122508 --> 0.121319).  Saving model ...\n",
      "[ 39/300] train_loss: 0.11325 valid_loss: 0.12591 test_loss: 0.13878 \n",
      "[ 40/300] train_loss: 0.11099 valid_loss: 0.12033 test_loss: 0.13382 \n",
      "Validation loss decreased (0.121319 --> 0.120335).  Saving model ...\n",
      "[ 41/300] train_loss: 0.11005 valid_loss: 0.12049 test_loss: 0.13422 \n",
      "[ 42/300] train_loss: 0.11631 valid_loss: 0.11859 test_loss: 0.13295 \n",
      "Validation loss decreased (0.120335 --> 0.118591).  Saving model ...\n",
      "[ 43/300] train_loss: 0.11000 valid_loss: 0.11819 test_loss: 0.13156 \n",
      "Validation loss decreased (0.118591 --> 0.118194).  Saving model ...\n",
      "[ 44/300] train_loss: 0.11140 valid_loss: 0.11769 test_loss: 0.13050 \n",
      "Validation loss decreased (0.118194 --> 0.117687).  Saving model ...\n",
      "[ 45/300] train_loss: 0.11070 valid_loss: 0.11995 test_loss: 0.13105 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 46/300] train_loss: 0.10966 valid_loss: 0.11629 test_loss: 0.12920 \n",
      "Validation loss decreased (0.117687 --> 0.116287).  Saving model ...\n",
      "[ 47/300] train_loss: 0.11107 valid_loss: 0.11964 test_loss: 0.13061 \n",
      "[ 48/300] train_loss: 0.10843 valid_loss: 0.11561 test_loss: 0.12987 \n",
      "Validation loss decreased (0.116287 --> 0.115606).  Saving model ...\n",
      "[ 49/300] train_loss: 0.10580 valid_loss: 0.11520 test_loss: 0.12955 \n",
      "Validation loss decreased (0.115606 --> 0.115200).  Saving model ...\n",
      "[ 50/300] train_loss: 0.10274 valid_loss: 0.11396 test_loss: 0.12725 \n",
      "Validation loss decreased (0.115200 --> 0.113960).  Saving model ...\n",
      "[ 51/300] train_loss: 0.10401 valid_loss: 0.11227 test_loss: 0.12586 \n",
      "Validation loss decreased (0.113960 --> 0.112271).  Saving model ...\n",
      "[ 52/300] train_loss: 0.10567 valid_loss: 0.11248 test_loss: 0.12647 \n",
      "[ 53/300] train_loss: 0.10186 valid_loss: 0.11667 test_loss: 0.12971 \n",
      "[ 54/300] train_loss: 0.10267 valid_loss: 0.11573 test_loss: 0.12761 \n",
      "[ 55/300] train_loss: 0.10200 valid_loss: 0.11249 test_loss: 0.12384 \n",
      "[ 56/300] train_loss: 0.10205 valid_loss: 0.11272 test_loss: 0.12563 \n",
      "[ 57/300] train_loss: 0.10265 valid_loss: 0.11228 test_loss: 0.12557 \n",
      "[ 58/300] train_loss: 0.10235 valid_loss: 0.11228 test_loss: 0.12409 \n",
      "[ 59/300] train_loss: 0.10306 valid_loss: 0.10980 test_loss: 0.12275 \n",
      "Validation loss decreased (0.112271 --> 0.109797).  Saving model ...\n",
      "[ 60/300] train_loss: 0.10069 valid_loss: 0.11099 test_loss: 0.12425 \n",
      "[ 61/300] train_loss: 0.10061 valid_loss: 0.11178 test_loss: 0.12431 \n",
      "[ 62/300] train_loss: 0.10500 valid_loss: 0.10592 test_loss: 0.12233 \n",
      "Validation loss decreased (0.109797 --> 0.105915).  Saving model ...\n",
      "[ 63/300] train_loss: 0.09917 valid_loss: 0.11237 test_loss: 0.12038 \n",
      "[ 64/300] train_loss: 0.10036 valid_loss: 0.10917 test_loss: 0.12268 \n",
      "[ 65/300] train_loss: 0.09958 valid_loss: 0.10893 test_loss: 0.12187 \n",
      "[ 66/300] train_loss: 0.10034 valid_loss: 0.10974 test_loss: 0.12238 \n",
      "[ 67/300] train_loss: 0.09868 valid_loss: 0.10648 test_loss: 0.12131 \n",
      "[ 68/300] train_loss: 0.09638 valid_loss: 0.11040 test_loss: 0.12211 \n",
      "[ 69/300] train_loss: 0.09748 valid_loss: 0.10938 test_loss: 0.12018 \n",
      "[ 70/300] train_loss: 0.09842 valid_loss: 0.10832 test_loss: 0.11904 \n",
      "[ 71/300] train_loss: 0.09940 valid_loss: 0.11000 test_loss: 0.11880 \n",
      "[ 72/300] train_loss: 0.09920 valid_loss: 0.10879 test_loss: 0.11985 \n",
      "[ 73/300] train_loss: 0.09708 valid_loss: 0.10782 test_loss: 0.11967 \n",
      "[ 74/300] train_loss: 0.09767 valid_loss: 0.10547 test_loss: 0.11824 \n",
      "Validation loss decreased (0.105915 --> 0.105470).  Saving model ...\n",
      "[ 75/300] train_loss: 0.09392 valid_loss: 0.10415 test_loss: 0.11704 \n",
      "Validation loss decreased (0.105470 --> 0.104154).  Saving model ...\n",
      "[ 76/300] train_loss: 0.09346 valid_loss: 0.10668 test_loss: 0.11655 \n",
      "[ 77/300] train_loss: 0.09351 valid_loss: 0.10563 test_loss: 0.11816 \n",
      "[ 78/300] train_loss: 0.09297 valid_loss: 0.10774 test_loss: 0.11693 \n",
      "[ 79/300] train_loss: 0.09499 valid_loss: 0.10538 test_loss: 0.11788 \n",
      "[ 80/300] train_loss: 0.09346 valid_loss: 0.10799 test_loss: 0.11449 \n",
      "[ 81/300] train_loss: 0.09165 valid_loss: 0.10464 test_loss: 0.11680 \n",
      "[ 82/300] train_loss: 0.09490 valid_loss: 0.10325 test_loss: 0.11599 \n",
      "Validation loss decreased (0.104154 --> 0.103247).  Saving model ...\n",
      "[ 83/300] train_loss: 0.09252 valid_loss: 0.10193 test_loss: 0.11601 \n",
      "Validation loss decreased (0.103247 --> 0.101934).  Saving model ...\n",
      "[ 84/300] train_loss: 0.09171 valid_loss: 0.10286 test_loss: 0.11381 \n",
      "[ 85/300] train_loss: 0.08839 valid_loss: 0.10781 test_loss: 0.11364 \n",
      "[ 86/300] train_loss: 0.09072 valid_loss: 0.10787 test_loss: 0.11426 \n",
      "[ 87/300] train_loss: 0.09157 valid_loss: 0.10297 test_loss: 0.11449 \n",
      "[ 88/300] train_loss: 0.08990 valid_loss: 0.10305 test_loss: 0.11407 \n",
      "[ 89/300] train_loss: 0.09057 valid_loss: 0.10224 test_loss: 0.11335 \n",
      "[ 90/300] train_loss: 0.09288 valid_loss: 0.10098 test_loss: 0.11172 \n",
      "Validation loss decreased (0.101934 --> 0.100980).  Saving model ...\n",
      "[ 91/300] train_loss: 0.09133 valid_loss: 0.10289 test_loss: 0.11338 \n",
      "[ 92/300] train_loss: 0.08886 valid_loss: 0.10009 test_loss: 0.11162 \n",
      "Validation loss decreased (0.100980 --> 0.100092).  Saving model ...\n",
      "[ 93/300] train_loss: 0.09275 valid_loss: 0.10241 test_loss: 0.11265 \n",
      "[ 94/300] train_loss: 0.08920 valid_loss: 0.10165 test_loss: 0.11324 \n",
      "[ 95/300] train_loss: 0.08870 valid_loss: 0.10021 test_loss: 0.11301 \n",
      "[ 96/300] train_loss: 0.08969 valid_loss: 0.09867 test_loss: 0.11217 \n",
      "Validation loss decreased (0.100092 --> 0.098666).  Saving model ...\n",
      "[ 97/300] train_loss: 0.09101 valid_loss: 0.10103 test_loss: 0.11023 \n",
      "[ 98/300] train_loss: 0.08807 valid_loss: 0.09995 test_loss: 0.11047 \n",
      "[ 99/300] train_loss: 0.09076 valid_loss: 0.10052 test_loss: 0.11025 \n",
      "[100/300] train_loss: 0.09016 valid_loss: 0.09759 test_loss: 0.11055 \n",
      "Validation loss decreased (0.098666 --> 0.097587).  Saving model ...\n",
      "[101/300] train_loss: 0.08753 valid_loss: 0.10022 test_loss: 0.11204 \n",
      "[102/300] train_loss: 0.08808 valid_loss: 0.10223 test_loss: 0.11172 \n",
      "[103/300] train_loss: 0.08729 valid_loss: 0.09890 test_loss: 0.11094 \n",
      "[104/300] train_loss: 0.08848 valid_loss: 0.09736 test_loss: 0.10880 \n",
      "Validation loss decreased (0.097587 --> 0.097357).  Saving model ...\n",
      "[105/300] train_loss: 0.08530 valid_loss: 0.09930 test_loss: 0.10855 \n",
      "[106/300] train_loss: 0.08626 valid_loss: 0.10001 test_loss: 0.10940 \n",
      "[107/300] train_loss: 0.08448 valid_loss: 0.09631 test_loss: 0.10842 \n",
      "Validation loss decreased (0.097357 --> 0.096311).  Saving model ...\n",
      "[108/300] train_loss: 0.08648 valid_loss: 0.09621 test_loss: 0.10907 \n",
      "Validation loss decreased (0.096311 --> 0.096211).  Saving model ...\n",
      "[109/300] train_loss: 0.08395 valid_loss: 0.09547 test_loss: 0.10773 \n",
      "Validation loss decreased (0.096211 --> 0.095471).  Saving model ...\n",
      "[110/300] train_loss: 0.08622 valid_loss: 0.09666 test_loss: 0.10884 \n",
      "[111/300] train_loss: 0.08449 valid_loss: 0.09541 test_loss: 0.10829 \n",
      "Validation loss decreased (0.095471 --> 0.095406).  Saving model ...\n",
      "[112/300] train_loss: 0.08635 valid_loss: 0.09537 test_loss: 0.10873 \n",
      "Validation loss decreased (0.095406 --> 0.095375).  Saving model ...\n",
      "[113/300] train_loss: 0.08498 valid_loss: 0.09476 test_loss: 0.10723 \n",
      "Validation loss decreased (0.095375 --> 0.094763).  Saving model ...\n",
      "[114/300] train_loss: 0.08602 valid_loss: 0.09547 test_loss: 0.10791 \n",
      "[115/300] train_loss: 0.08324 valid_loss: 0.10075 test_loss: 0.10824 \n",
      "[116/300] train_loss: 0.08452 valid_loss: 0.09568 test_loss: 0.10681 \n",
      "[117/300] train_loss: 0.08707 valid_loss: 0.09562 test_loss: 0.10791 \n",
      "[118/300] train_loss: 0.08549 valid_loss: 0.09769 test_loss: 0.10842 \n",
      "[119/300] train_loss: 0.08526 valid_loss: 0.09415 test_loss: 0.10555 \n",
      "Validation loss decreased (0.094763 --> 0.094153).  Saving model ...\n",
      "[120/300] train_loss: 0.08400 valid_loss: 0.09842 test_loss: 0.10778 \n",
      "[121/300] train_loss: 0.08439 valid_loss: 0.09436 test_loss: 0.10633 \n",
      "[122/300] train_loss: 0.08332 valid_loss: 0.09476 test_loss: 0.10690 \n",
      "[123/300] train_loss: 0.08555 valid_loss: 0.09462 test_loss: 0.10679 \n",
      "[124/300] train_loss: 0.08307 valid_loss: 0.09381 test_loss: 0.10637 \n",
      "Validation loss decreased (0.094153 --> 0.093808).  Saving model ...\n",
      "[125/300] train_loss: 0.08466 valid_loss: 0.09474 test_loss: 0.10601 \n",
      "[126/300] train_loss: 0.08596 valid_loss: 0.09590 test_loss: 0.10657 \n",
      "[127/300] train_loss: 0.08235 valid_loss: 0.09454 test_loss: 0.10637 \n",
      "[128/300] train_loss: 0.08378 valid_loss: 0.09378 test_loss: 0.10558 \n",
      "Validation loss decreased (0.093808 --> 0.093776).  Saving model ...\n",
      "[129/300] train_loss: 0.08743 valid_loss: 0.09296 test_loss: 0.10608 \n",
      "Validation loss decreased (0.093776 --> 0.092957).  Saving model ...\n",
      "[130/300] train_loss: 0.08285 valid_loss: 0.09314 test_loss: 0.10503 \n",
      "[131/300] train_loss: 0.08308 valid_loss: 0.09735 test_loss: 0.10649 \n",
      "[132/300] train_loss: 0.08516 valid_loss: 0.09496 test_loss: 0.10438 \n",
      "[133/300] train_loss: 0.08006 valid_loss: 0.09368 test_loss: 0.10482 \n",
      "[134/300] train_loss: 0.08277 valid_loss: 0.09225 test_loss: 0.10629 \n",
      "Validation loss decreased (0.092957 --> 0.092254).  Saving model ...\n",
      "[135/300] train_loss: 0.08251 valid_loss: 0.09448 test_loss: 0.10535 \n",
      "[136/300] train_loss: 0.08285 valid_loss: 0.09390 test_loss: 0.10512 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137/300] train_loss: 0.08086 valid_loss: 0.09416 test_loss: 0.10534 \n",
      "[138/300] train_loss: 0.08048 valid_loss: 0.09516 test_loss: 0.10597 \n",
      "[139/300] train_loss: 0.08217 valid_loss: 0.09210 test_loss: 0.10351 \n",
      "Validation loss decreased (0.092254 --> 0.092096).  Saving model ...\n",
      "[140/300] train_loss: 0.08448 valid_loss: 0.09649 test_loss: 0.10493 \n",
      "[141/300] train_loss: 0.08417 valid_loss: 0.09193 test_loss: 0.10339 \n",
      "Validation loss decreased (0.092096 --> 0.091933).  Saving model ...\n",
      "[142/300] train_loss: 0.08151 valid_loss: 0.09064 test_loss: 0.10304 \n",
      "Validation loss decreased (0.091933 --> 0.090637).  Saving model ...\n",
      "[143/300] train_loss: 0.07904 valid_loss: 0.09113 test_loss: 0.10344 \n",
      "[144/300] train_loss: 0.08028 valid_loss: 0.09196 test_loss: 0.10430 \n",
      "[145/300] train_loss: 0.08013 valid_loss: 0.09116 test_loss: 0.10328 \n",
      "[146/300] train_loss: 0.07917 valid_loss: 0.09183 test_loss: 0.10385 \n",
      "[147/300] train_loss: 0.07998 valid_loss: 0.09405 test_loss: 0.10337 \n",
      "[148/300] train_loss: 0.07736 valid_loss: 0.09092 test_loss: 0.10232 \n",
      "[149/300] train_loss: 0.08021 valid_loss: 0.09058 test_loss: 0.10282 \n",
      "Validation loss decreased (0.090637 --> 0.090576).  Saving model ...\n",
      "[150/300] train_loss: 0.08118 valid_loss: 0.09366 test_loss: 0.10469 \n",
      "[151/300] train_loss: 0.07880 valid_loss: 0.09056 test_loss: 0.10250 \n",
      "Validation loss decreased (0.090576 --> 0.090564).  Saving model ...\n",
      "[152/300] train_loss: 0.07734 valid_loss: 0.09015 test_loss: 0.10203 \n",
      "Validation loss decreased (0.090564 --> 0.090152).  Saving model ...\n",
      "[153/300] train_loss: 0.07730 valid_loss: 0.09159 test_loss: 0.10185 \n",
      "[154/300] train_loss: 0.07887 valid_loss: 0.08984 test_loss: 0.10178 \n",
      "Validation loss decreased (0.090152 --> 0.089838).  Saving model ...\n",
      "[155/300] train_loss: 0.08114 valid_loss: 0.08860 test_loss: 0.10163 \n",
      "Validation loss decreased (0.089838 --> 0.088603).  Saving model ...\n",
      "[156/300] train_loss: 0.07820 valid_loss: 0.10524 test_loss: 0.10226 \n",
      "[157/300] train_loss: 0.07922 valid_loss: 0.09455 test_loss: 0.10327 \n",
      "[158/300] train_loss: 0.07931 valid_loss: 0.09042 test_loss: 0.10184 \n",
      "[159/300] train_loss: 0.07839 valid_loss: 0.08979 test_loss: 0.10077 \n",
      "[160/300] train_loss: 0.07938 valid_loss: 0.09293 test_loss: 0.10346 \n",
      "[161/300] train_loss: 0.07842 valid_loss: 0.08928 test_loss: 0.10187 \n",
      "[162/300] train_loss: 0.07653 valid_loss: 0.09203 test_loss: 0.10225 \n",
      "[163/300] train_loss: 0.07833 valid_loss: 0.09032 test_loss: 0.10117 \n",
      "[164/300] train_loss: 0.07722 valid_loss: 0.09000 test_loss: 0.10154 \n",
      "[165/300] train_loss: 0.07789 valid_loss: 0.08836 test_loss: 0.10079 \n",
      "Validation loss decreased (0.088603 --> 0.088360).  Saving model ...\n",
      "[166/300] train_loss: 0.07866 valid_loss: 0.08870 test_loss: 0.10078 \n",
      "[167/300] train_loss: 0.07672 valid_loss: 0.08931 test_loss: 0.10108 \n",
      "[168/300] train_loss: 0.07906 valid_loss: 0.08884 test_loss: 0.10056 \n",
      "[169/300] train_loss: 0.07724 valid_loss: 0.09009 test_loss: 0.10079 \n",
      "[170/300] train_loss: 0.07827 valid_loss: 0.08908 test_loss: 0.10066 \n",
      "[171/300] train_loss: 0.07708 valid_loss: 0.08953 test_loss: 0.10071 \n",
      "[172/300] train_loss: 0.07739 valid_loss: 0.08867 test_loss: 0.10065 \n",
      "[173/300] train_loss: 0.07657 valid_loss: 0.08898 test_loss: 0.10065 \n",
      "[174/300] train_loss: 0.07640 valid_loss: 0.08734 test_loss: 0.10027 \n",
      "Validation loss decreased (0.088360 --> 0.087344).  Saving model ...\n",
      "[175/300] train_loss: 0.07587 valid_loss: 0.08766 test_loss: 0.09988 \n",
      "[176/300] train_loss: 0.07660 valid_loss: 0.08719 test_loss: 0.09900 \n",
      "Validation loss decreased (0.087344 --> 0.087195).  Saving model ...\n",
      "[177/300] train_loss: 0.07370 valid_loss: 0.08663 test_loss: 0.09909 \n",
      "Validation loss decreased (0.087195 --> 0.086626).  Saving model ...\n",
      "[178/300] train_loss: 0.07687 valid_loss: 0.08621 test_loss: 0.09853 \n",
      "Validation loss decreased (0.086626 --> 0.086207).  Saving model ...\n",
      "[179/300] train_loss: 0.07790 valid_loss: 0.08706 test_loss: 0.09974 \n",
      "[180/300] train_loss: 0.07586 valid_loss: 0.08731 test_loss: 0.09941 \n",
      "[181/300] train_loss: 0.07500 valid_loss: 0.08729 test_loss: 0.09972 \n",
      "[182/300] train_loss: 0.07321 valid_loss: 0.08752 test_loss: 0.09993 \n",
      "[183/300] train_loss: 0.07520 valid_loss: 0.08791 test_loss: 0.09961 \n",
      "[184/300] train_loss: 0.07574 valid_loss: 0.08690 test_loss: 0.10071 \n",
      "[185/300] train_loss: 0.07720 valid_loss: 0.08700 test_loss: 0.09867 \n",
      "[186/300] train_loss: 0.07797 valid_loss: 0.08669 test_loss: 0.09848 \n",
      "[187/300] train_loss: 0.07582 valid_loss: 0.08760 test_loss: 0.09799 \n",
      "[188/300] train_loss: 0.07456 valid_loss: 0.08830 test_loss: 0.09913 \n",
      "[189/300] train_loss: 0.07548 valid_loss: 0.08681 test_loss: 0.09789 \n",
      "[190/300] train_loss: 0.07596 valid_loss: 0.08655 test_loss: 0.09824 \n",
      "[191/300] train_loss: 0.07695 valid_loss: 0.08665 test_loss: 0.09865 \n",
      "[192/300] train_loss: 0.07470 valid_loss: 0.08914 test_loss: 0.09993 \n",
      "[193/300] train_loss: 0.07687 valid_loss: 0.08581 test_loss: 0.09792 \n",
      "Validation loss decreased (0.086207 --> 0.085813).  Saving model ...\n",
      "[194/300] train_loss: 0.07513 valid_loss: 0.08653 test_loss: 0.09853 \n",
      "[195/300] train_loss: 0.07604 valid_loss: 0.08607 test_loss: 0.09866 \n",
      "[196/300] train_loss: 0.07404 valid_loss: 0.08644 test_loss: 0.09777 \n",
      "[197/300] train_loss: 0.07503 valid_loss: 0.08578 test_loss: 0.09733 \n",
      "Validation loss decreased (0.085813 --> 0.085778).  Saving model ...\n",
      "[198/300] train_loss: 0.07679 valid_loss: 0.08531 test_loss: 0.09737 \n",
      "Validation loss decreased (0.085778 --> 0.085314).  Saving model ...\n",
      "[199/300] train_loss: 0.07330 valid_loss: 0.08588 test_loss: 0.09825 \n",
      "[200/300] train_loss: 0.07487 valid_loss: 0.08540 test_loss: 0.09793 \n",
      "[201/300] train_loss: 0.07117 valid_loss: 0.08618 test_loss: 0.09770 \n",
      "[202/300] train_loss: 0.07557 valid_loss: 0.08687 test_loss: 0.09903 \n",
      "[203/300] train_loss: 0.07752 valid_loss: 0.08657 test_loss: 0.09819 \n",
      "[204/300] train_loss: 0.07455 valid_loss: 0.08686 test_loss: 0.09808 \n",
      "[205/300] train_loss: 0.07407 valid_loss: 0.08694 test_loss: 0.09772 \n",
      "[206/300] train_loss: 0.07341 valid_loss: 0.08648 test_loss: 0.09732 \n",
      "[207/300] train_loss: 0.07484 valid_loss: 0.08802 test_loss: 0.09810 \n",
      "[208/300] train_loss: 0.07623 valid_loss: 0.08397 test_loss: 0.09664 \n",
      "Validation loss decreased (0.085314 --> 0.083973).  Saving model ...\n",
      "[209/300] train_loss: 0.07450 valid_loss: 0.08563 test_loss: 0.09698 \n",
      "[210/300] train_loss: 0.07158 valid_loss: 0.08522 test_loss: 0.09728 \n",
      "[211/300] train_loss: 0.07491 valid_loss: 0.08653 test_loss: 0.09683 \n",
      "[212/300] train_loss: 0.07236 valid_loss: 0.08587 test_loss: 0.09771 \n",
      "[213/300] train_loss: 0.07298 valid_loss: 0.08456 test_loss: 0.09784 \n",
      "[214/300] train_loss: 0.07304 valid_loss: 0.08515 test_loss: 0.09716 \n",
      "[215/300] train_loss: 0.07261 valid_loss: 0.08571 test_loss: 0.09710 \n",
      "[216/300] train_loss: 0.07218 valid_loss: 0.08513 test_loss: 0.09735 \n",
      "[217/300] train_loss: 0.07457 valid_loss: 0.08492 test_loss: 0.09736 \n",
      "[218/300] train_loss: 0.07490 valid_loss: 0.08326 test_loss: 0.09645 \n",
      "Validation loss decreased (0.083973 --> 0.083257).  Saving model ...\n",
      "[219/300] train_loss: 0.07224 valid_loss: 0.08516 test_loss: 0.09699 \n",
      "[220/300] train_loss: 0.07267 valid_loss: 0.08474 test_loss: 0.09669 \n",
      "[221/300] train_loss: 0.07238 valid_loss: 0.08447 test_loss: 0.09656 \n",
      "[222/300] train_loss: 0.07343 valid_loss: 0.08340 test_loss: 0.09598 \n",
      "[223/300] train_loss: 0.07268 valid_loss: 0.08735 test_loss: 0.09810 \n",
      "[224/300] train_loss: 0.07268 valid_loss: 0.08694 test_loss: 0.09659 \n",
      "[225/300] train_loss: 0.07296 valid_loss: 0.08541 test_loss: 0.09688 \n",
      "[226/300] train_loss: 0.07202 valid_loss: 0.08543 test_loss: 0.09604 \n",
      "[227/300] train_loss: 0.07140 valid_loss: 0.08643 test_loss: 0.09746 \n",
      "[228/300] train_loss: 0.07342 valid_loss: 0.08520 test_loss: 0.09571 \n",
      "[229/300] train_loss: 0.07114 valid_loss: 0.08363 test_loss: 0.09640 \n",
      "[230/300] train_loss: 0.07108 valid_loss: 0.08576 test_loss: 0.09650 \n",
      "[231/300] train_loss: 0.07324 valid_loss: 0.08529 test_loss: 0.09710 \n",
      "[232/300] train_loss: 0.07240 valid_loss: 0.08455 test_loss: 0.09521 \n",
      "[233/300] train_loss: 0.07168 valid_loss: 0.08232 test_loss: 0.09542 \n",
      "Validation loss decreased (0.083257 --> 0.082323).  Saving model ...\n",
      "[234/300] train_loss: 0.07254 valid_loss: 0.08769 test_loss: 0.09533 \n",
      "[235/300] train_loss: 0.07413 valid_loss: 0.08465 test_loss: 0.09551 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[236/300] train_loss: 0.07167 valid_loss: 0.08437 test_loss: 0.09478 \n",
      "[237/300] train_loss: 0.07269 valid_loss: 0.08483 test_loss: 0.09559 \n",
      "[238/300] train_loss: 0.07211 valid_loss: 0.08408 test_loss: 0.09555 \n",
      "[239/300] train_loss: 0.06986 valid_loss: 0.08383 test_loss: 0.09505 \n",
      "[240/300] train_loss: 0.07163 valid_loss: 0.08352 test_loss: 0.09507 \n",
      "[241/300] train_loss: 0.07102 valid_loss: 0.08488 test_loss: 0.09675 \n",
      "[242/300] train_loss: 0.07015 valid_loss: 0.08461 test_loss: 0.09597 \n",
      "[243/300] train_loss: 0.06971 valid_loss: 0.08319 test_loss: 0.09607 \n",
      "[244/300] train_loss: 0.07009 valid_loss: 0.08492 test_loss: 0.09571 \n",
      "[245/300] train_loss: 0.06987 valid_loss: 0.08439 test_loss: 0.09560 \n",
      "[246/300] train_loss: 0.07030 valid_loss: 0.08460 test_loss: 0.09591 \n",
      "[247/300] train_loss: 0.06925 valid_loss: 0.08499 test_loss: 0.09656 \n",
      "[248/300] train_loss: 0.07099 valid_loss: 0.08449 test_loss: 0.09540 \n",
      "[249/300] train_loss: 0.07001 valid_loss: 0.08274 test_loss: 0.09465 \n",
      "[250/300] train_loss: 0.07239 valid_loss: 0.08324 test_loss: 0.09510 \n",
      "[251/300] train_loss: 0.07207 valid_loss: 0.08399 test_loss: 0.09541 \n",
      "[252/300] train_loss: 0.06928 valid_loss: 0.08422 test_loss: 0.09529 \n",
      "[253/300] train_loss: 0.07043 valid_loss: 0.08350 test_loss: 0.09500 \n",
      "[254/300] train_loss: 0.06753 valid_loss: 0.08379 test_loss: 0.09460 \n",
      "[255/300] train_loss: 0.06872 valid_loss: 0.08294 test_loss: 0.09505 \n",
      "[256/300] train_loss: 0.06951 valid_loss: 0.08384 test_loss: 0.09539 \n",
      "[257/300] train_loss: 0.06844 valid_loss: 0.08439 test_loss: 0.09474 \n",
      "[258/300] train_loss: 0.06952 valid_loss: 0.08271 test_loss: 0.09474 \n",
      "[259/300] train_loss: 0.06904 valid_loss: 0.08233 test_loss: 0.09436 \n",
      "[260/300] train_loss: 0.06796 valid_loss: 0.08306 test_loss: 0.09419 \n",
      "[261/300] train_loss: 0.06812 valid_loss: 0.08207 test_loss: 0.09351 \n",
      "Validation loss decreased (0.082323 --> 0.082070).  Saving model ...\n",
      "[262/300] train_loss: 0.06996 valid_loss: 0.08314 test_loss: 0.09400 \n",
      "[263/300] train_loss: 0.06916 valid_loss: 0.08266 test_loss: 0.09339 \n",
      "[264/300] train_loss: 0.06761 valid_loss: 0.08306 test_loss: 0.09440 \n",
      "[265/300] train_loss: 0.06932 valid_loss: 0.08538 test_loss: 0.09369 \n",
      "[266/300] train_loss: 0.07091 valid_loss: 0.08494 test_loss: 0.09534 \n",
      "[267/300] train_loss: 0.06971 valid_loss: 0.08412 test_loss: 0.09441 \n",
      "[268/300] train_loss: 0.06648 valid_loss: 0.08314 test_loss: 0.09411 \n",
      "[269/300] train_loss: 0.06839 valid_loss: 0.08211 test_loss: 0.09358 \n",
      "[270/300] train_loss: 0.06758 valid_loss: 0.08498 test_loss: 0.09424 \n",
      "[271/300] train_loss: 0.06963 valid_loss: 0.08467 test_loss: 0.09402 \n",
      "[272/300] train_loss: 0.06874 valid_loss: 0.08166 test_loss: 0.09341 \n",
      "Validation loss decreased (0.082070 --> 0.081656).  Saving model ...\n",
      "[273/300] train_loss: 0.06943 valid_loss: 0.08289 test_loss: 0.09341 \n",
      "[274/300] train_loss: 0.06965 valid_loss: 0.08245 test_loss: 0.09358 \n",
      "[275/300] train_loss: 0.06839 valid_loss: 0.08280 test_loss: 0.09397 \n",
      "[276/300] train_loss: 0.06870 valid_loss: 0.08327 test_loss: 0.09334 \n",
      "[277/300] train_loss: 0.06937 valid_loss: 0.08093 test_loss: 0.09312 \n",
      "Validation loss decreased (0.081656 --> 0.080927).  Saving model ...\n",
      "[278/300] train_loss: 0.06761 valid_loss: 0.08178 test_loss: 0.09323 \n",
      "[279/300] train_loss: 0.06969 valid_loss: 0.08220 test_loss: 0.09290 \n",
      "[280/300] train_loss: 0.07045 valid_loss: 0.08154 test_loss: 0.09299 \n",
      "[281/300] train_loss: 0.06917 valid_loss: 0.08075 test_loss: 0.09254 \n",
      "Validation loss decreased (0.080927 --> 0.080747).  Saving model ...\n",
      "[282/300] train_loss: 0.06702 valid_loss: 0.08075 test_loss: 0.09298 \n",
      "Validation loss decreased (0.080747 --> 0.080747).  Saving model ...\n",
      "[283/300] train_loss: 0.06859 valid_loss: 0.07992 test_loss: 0.09253 \n",
      "Validation loss decreased (0.080747 --> 0.079924).  Saving model ...\n",
      "[284/300] train_loss: 0.06895 valid_loss: 0.08103 test_loss: 0.09260 \n",
      "[285/300] train_loss: 0.06798 valid_loss: 0.08141 test_loss: 0.09306 \n",
      "[286/300] train_loss: 0.06859 valid_loss: 0.08137 test_loss: 0.09314 \n",
      "[287/300] train_loss: 0.06668 valid_loss: 0.08237 test_loss: 0.09367 \n",
      "[288/300] train_loss: 0.06693 valid_loss: 0.08140 test_loss: 0.09377 \n",
      "[289/300] train_loss: 0.06678 valid_loss: 0.08322 test_loss: 0.09489 \n",
      "[290/300] train_loss: 0.06745 valid_loss: 0.08177 test_loss: 0.09375 \n",
      "[291/300] train_loss: 0.06830 valid_loss: 0.08146 test_loss: 0.09269 \n",
      "[292/300] train_loss: 0.06804 valid_loss: 0.08234 test_loss: 0.09436 \n",
      "[293/300] train_loss: 0.06841 valid_loss: 0.08156 test_loss: 0.09193 \n",
      "[294/300] train_loss: 0.06719 valid_loss: 0.08041 test_loss: 0.09132 \n",
      "[295/300] train_loss: 0.06583 valid_loss: 0.08103 test_loss: 0.09277 \n",
      "[296/300] train_loss: 0.06565 valid_loss: 0.08028 test_loss: 0.09245 \n",
      "[297/300] train_loss: 0.06789 valid_loss: 0.08163 test_loss: 0.09296 \n",
      "[298/300] train_loss: 0.06865 valid_loss: 0.08178 test_loss: 0.09381 \n",
      "[299/300] train_loss: 0.06666 valid_loss: 0.08019 test_loss: 0.09226 \n",
      "[300/300] train_loss: 0.06860 valid_loss: 0.08018 test_loss: 0.09199 \n",
      "TRAINING MODEL 13\n",
      "[  1/300] train_loss: 0.62252 valid_loss: 0.52169 test_loss: 0.51562 \n",
      "Validation loss decreased (inf --> 0.521686).  Saving model ...\n",
      "[  2/300] train_loss: 0.43676 valid_loss: 0.39454 test_loss: 0.39140 \n",
      "Validation loss decreased (0.521686 --> 0.394539).  Saving model ...\n",
      "[  3/300] train_loss: 0.34527 valid_loss: 0.34208 test_loss: 0.34299 \n",
      "Validation loss decreased (0.394539 --> 0.342082).  Saving model ...\n",
      "[  4/300] train_loss: 0.30178 valid_loss: 0.30410 test_loss: 0.31224 \n",
      "Validation loss decreased (0.342082 --> 0.304104).  Saving model ...\n",
      "[  5/300] train_loss: 0.26793 valid_loss: 0.27145 test_loss: 0.28097 \n",
      "Validation loss decreased (0.304104 --> 0.271452).  Saving model ...\n",
      "[  6/300] train_loss: 0.23892 valid_loss: 0.23868 test_loss: 0.25235 \n",
      "Validation loss decreased (0.271452 --> 0.238680).  Saving model ...\n",
      "[  7/300] train_loss: 0.21859 valid_loss: 0.21645 test_loss: 0.22929 \n",
      "Validation loss decreased (0.238680 --> 0.216447).  Saving model ...\n",
      "[  8/300] train_loss: 0.19748 valid_loss: 0.20219 test_loss: 0.21496 \n",
      "Validation loss decreased (0.216447 --> 0.202186).  Saving model ...\n",
      "[  9/300] train_loss: 0.18533 valid_loss: 0.19074 test_loss: 0.20094 \n",
      "Validation loss decreased (0.202186 --> 0.190744).  Saving model ...\n",
      "[ 10/300] train_loss: 0.17516 valid_loss: 0.18288 test_loss: 0.19255 \n",
      "Validation loss decreased (0.190744 --> 0.182876).  Saving model ...\n",
      "[ 11/300] train_loss: 0.16541 valid_loss: 0.17210 test_loss: 0.18219 \n",
      "Validation loss decreased (0.182876 --> 0.172100).  Saving model ...\n",
      "[ 12/300] train_loss: 0.16270 valid_loss: 0.16601 test_loss: 0.17618 \n",
      "Validation loss decreased (0.172100 --> 0.166013).  Saving model ...\n",
      "[ 13/300] train_loss: 0.15505 valid_loss: 0.16220 test_loss: 0.17257 \n",
      "Validation loss decreased (0.166013 --> 0.162203).  Saving model ...\n",
      "[ 14/300] train_loss: 0.15210 valid_loss: 0.15517 test_loss: 0.16751 \n",
      "Validation loss decreased (0.162203 --> 0.155168).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 300\n",
    "\n",
    "train_loader = dl_train_seen\n",
    "valid_loader = dl_valid_seen\n",
    "test_loader = dl_test_seen\n",
    "\n",
    "#i = 0\n",
    "for i in range(20):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    model = PTPNet(1,3,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    fn = 'UKDALE_seen_%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sh-_NRsnPkdC"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.plot(test_loss)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzBMEXOuEDrj"
   },
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 100\n",
    "\n",
    "train_loader = dl_train_unseen\n",
    "valid_loader = dl_valid_unseen\n",
    "test_loader = dl_test_unseen\n",
    "\n",
    "#i = 0\n",
    "for i in range(20):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    model = PTPNet(1,3,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    fn = 'UKDALE_unseen_%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYrzbsFJEThv"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.plot(test_loss)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "MD1z2vUwyZIH",
    "outputId": "42c5af30-c246-4805-8ce7-2a7fbe79176c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTPNet(\n",
       "  (encoder1): Encoder(\n",
       "    (conv): Conv1d(1, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder2): Encoder(\n",
       "    (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder3): Encoder(\n",
       "    (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder4): Encoder(\n",
       "    (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool1): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool2): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(10,), stride=(10,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool3): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(20,), stride=(20,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool4): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv): ConvTranspose1d(512, 32, kernel_size=(8,), stride=(8,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (activation): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PTPNet(1,3,32).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "HuXotkW1yd-b",
    "outputId": "49ef336e-dab6-470f-ab7c-7ed83a4c4162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/NILM/UKDALE_seen_0.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_1.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_2.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_3.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_4.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_5.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_6.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_7.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_8.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_9.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_10.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_11.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_12.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_13.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_14.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_15.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_16.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_17.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_18.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_seen_19.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.867 (0.864, 0.870)\n",
      "Precision : 0.875 (0.867, 0.885)\n",
      "Recall    : 0.859 (0.846, 0.871)\n",
      "Accuracy  : 0.880 (0.878, 0.882)\n",
      "MCC       : 0.759 (0.755, 0.762)\n",
      "MAE       : 15.25 (15.08, 15.47)\n",
      "SAE       : -0.020 (-0.046, 0.002)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.930 (0.905, 0.946)\n",
      "Precision : 0.942 (0.904, 0.966)\n",
      "Recall    : 0.919 (0.890, 0.942)\n",
      "Accuracy  : 0.997 (0.995, 0.997)\n",
      "MCC       : 0.928 (0.903, 0.945)\n",
      "MAE       : 20.41 (19.99, 21.00)\n",
      "SAE       : -0.042 (-0.082, -0.005)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.978 (0.976, 0.980)\n",
      "Precision : 0.975 (0.968, 0.979)\n",
      "Recall    : 0.982 (0.977, 0.987)\n",
      "Accuracy  : 0.997 (0.996, 0.997)\n",
      "MCC       : 0.977 (0.974, 0.979)\n",
      "MAE       : 41.97 (41.80, 42.21)\n",
      "SAE       : -0.077 (-0.085, -0.066)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.5\n",
    "for i in range(20):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_seen_%d.pth' %i\n",
    "    filename = './UKDALE_seen_%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[1], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[1], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[1], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[1], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[1], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[1], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[1], sorted(scores[i]['SAE'])[18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "oYjnWxnQyURS",
    "outputId": "1f4f491b-4271-426e-ee56-c739248a2a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_0.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_1.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_2.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_3.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_4.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_5.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_6.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_7.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_8.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_9.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_10.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_11.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_12.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_13.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_14.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_15.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_16.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_17.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_18.pth\n",
      "/content/gdrive/My Drive/NILM/UKDALE_unseen_19.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.871 (0.863, 0.876)\n",
      "Precision : 0.892 (0.883, 0.898)\n",
      "Recall    : 0.851 (0.841, 0.861)\n",
      "Accuracy  : 0.905 (0.900, 0.908)\n",
      "MCC       : 0.796 (0.786, 0.803)\n",
      "MAE       : 17.03 (16.82, 17.24)\n",
      "SAE       : -0.046 (-0.066, -0.025)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.809 (0.790, 0.822)\n",
      "Precision : 0.788 (0.738, 0.826)\n",
      "Recall    : 0.835 (0.768, 0.897)\n",
      "Accuracy  : 0.989 (0.987, 0.990)\n",
      "MCC       : 0.805 (0.784, 0.817)\n",
      "MAE       : 33.07 (31.19, 35.68)\n",
      "SAE       : 0.063 (-0.054, 0.219)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.863 (0.835, 0.900)\n",
      "Precision : 0.858 (0.811, 0.893)\n",
      "Recall    : 0.869 (0.827, 0.918)\n",
      "Accuracy  : 0.997 (0.996, 0.998)\n",
      "MCC       : 0.862 (0.834, 0.899)\n",
      "MAE       : 8.31 (7.88, 8.70)\n",
      "SAE       : 0.014 (-0.059, 0.115)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "for i in range(20):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_unseen_%d.pth' %i\n",
    "    filename = './UKDALE_unseen_%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        pm = ds_appliance[1][APPLIANCE[a]].sum() / ds_status[1][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[1], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[1], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[1], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[1], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[1], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[1], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[1], sorted(scores[i]['SAE'])[18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqLXkeMG3kVJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TPNILM_UKDALE_run.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TPNILM",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
